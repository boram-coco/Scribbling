[
  {
    "objectID": "posts/VAE.html",
    "href": "posts/VAE.html",
    "title": "VAE(-ing)",
    "section": "",
    "text": "https://arxiv.org/pdf/1312.6114.pdf"
  },
  {
    "objectID": "posts/VAE.html#오토인코더",
    "href": "posts/VAE.html#오토인코더",
    "title": "VAE(-ing)",
    "section": "오토인코더",
    "text": "오토인코더\n\nEncoder, Decoder 네트워크로 구성된 모델\n학습 데이터-> encoder에 입력값"
  },
  {
    "objectID": "posts/회귀진단 실습.html",
    "href": "posts/회귀진단 실습.html",
    "title": "6. 회귀진단 실습",
    "section": "",
    "text": "해당 자료는 전북대학교 이영미 교수님 2023응용통계학 자료임"
  },
  {
    "objectID": "posts/회귀진단 실습.html#leverage-vs.-outlier-vs.-influence",
    "href": "posts/회귀진단 실습.html#leverage-vs.-outlier-vs.-influence",
    "title": "6. 회귀진단 실습",
    "section": "Leverage vs. Outlier vs. Influence",
    "text": "Leverage vs. Outlier vs. Influence\n\nlibrary(lmtest)\n\nLoading required package: zoo\n\n\nAttaching package: ‘zoo’\n\n\nThe following objects are masked from ‘package:base’:\n\n    as.Date, as.Date.numeric\n\n\n\n\n\ndt <- data.frame(x = c(15,26,10,9,15,20,18,11,\n 8,20,7,9,10,11,11,10,12,42,17,11,10),\n y = c(95,71,83,91,102,87,93,100,\n 104,94,113,96,83,84,102,100,\n 105,57,121,86,100))\n\n\n######## 산점도\nplot(y~x, dt,pch = 20,cex = 2,col = \"darkorange\")"
  },
  {
    "objectID": "posts/회귀진단 실습.html#회귀모형-적합-ybeta_0-beta_1x-epsilon",
    "href": "posts/회귀진단 실습.html#회귀모형-적합-ybeta_0-beta_1x-epsilon",
    "title": "6. 회귀진단 실습",
    "section": "회귀모형 적합: \\(y=\\beta_0+ \\beta_1x + \\epsilon\\)",
    "text": "회귀모형 적합: \\(y=\\beta_0+ \\beta_1x + \\epsilon\\)\n\n######## 회귀적합\nmodel_reg <- lm(y~x, dt)\nsummary(model_reg)\n\n\nCall:\nlm(formula = y ~ x, data = dt)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-15.604  -8.731   1.396   4.523  30.285 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 109.8738     5.0678  21.681 7.31e-15 ***\nx            -1.1270     0.3102  -3.633  0.00177 ** \n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\nResidual standard error: 11.02 on 19 degrees of freedom\nMultiple R-squared:   0.41, Adjusted R-squared:  0.3789 \nF-statistic:  13.2 on 1 and 19 DF,  p-value: 0.001769\n\n\n\nplot(y~x, dt,pch = 20,cex = 2,col = \"darkorange\")\nabline(model_reg, col='steelblue', lwd=2)"
  },
  {
    "objectID": "posts/회귀진단 실습.html#hat-matrix",
    "href": "posts/회귀진단 실습.html#hat-matrix",
    "title": "6. 회귀진단 실습",
    "section": "Hat Matrix",
    "text": "Hat Matrix\n\nX = cbind(rep(1, nrow(dt)), dt$x)\nH = X %*% solve(t(X) %*% X) %*% t(X)\ndiag(H)\n\n\n0.04792247945102180.1545132342960560.06281577558253530.07054520775205490.04792247945102180.07261895784631620.05798959354498150.05666993439408790.07985823090264690.07261895784631620.09075484503431120.07054520775205490.06281577558253530.05666993439408790.05666993439408790.06281577558253530.05210768418671290.651609984164090.05305029786592260.05666993439408790.0628157755825353\n\n\n\nsum(diag(H))\n\n2\n\n\n\nhatvalues(model_reg)\n\n10.047922479451021820.15451323429605630.062815775582535340.070545207752054950.047922479451021860.072618957846316370.057989593544981580.056669934394087990.0798582309026469100.0726189578463163110.0907548450343111120.0705452077520549130.0628157755825353140.0566699343940879150.0566699343940879160.0628157755825353170.0521076841867129180.65160998416409190.0530502978659226200.0566699343940879210.0628157755825353\n\n\n\nwhich.max(hatvalues(model_reg))\nhatvalues(model_reg)[which.max(hatvalues(model_reg))] ##h_{18,18}\n\n18: 18\n\n\n18: 0.65160998416409\n\n\n\n2*(1+1)/nrow(dt)\n\n0.19047619047619\n\n\n\n\\(h_{18,18} > 2 \\bar h\\)이므로 18번째 관측값이 leverage point 로 고려 가능\n\n\nplot(y~x, dt,pch = 20,cex = 2,col = \"darkorange\")\ntext(dt[18,],\"18\", pos=2)\nabline(model_reg, col='steelblue', lwd=2)"
  },
  {
    "objectID": "posts/회귀진단 실습.html#잔차-e_i-y_i---hat-y_i",
    "href": "posts/회귀진단 실습.html#잔차-e_i-y_i---hat-y_i",
    "title": "6. 회귀진단 실습",
    "section": "잔차: \\(e_i = y_i - \\hat y_i\\)",
    "text": "잔차: \\(e_i = y_i - \\hat y_i\\)\n\nresidual <- model_reg$residuals\nhead(residual)\n\n12.030993137772482-9.572128798733133-15.60395143654324-8.7309403514063859.030993137772416-0.334062287911925\n\n\n\nhist(residual)"
  },
  {
    "objectID": "posts/회귀진단 실습.html#내적으로-표준화된-잔차-internally-standardized-residual",
    "href": "posts/회귀진단 실습.html#내적으로-표준화된-잔차-internally-standardized-residual",
    "title": "6. 회귀진단 실습",
    "section": "내적으로 표준화된 잔차 ((internally) standardized residual)",
    "text": "내적으로 표준화된 잔차 ((internally) standardized residual)\n\\[r_i = \\dfrac{e_i}{\\hat \\sigma \\sqrt{1-h_{ii}}}\\]\n\ns_residual <- rstandard(model_reg)\nhead(s_residual)\n\n10.1888322174200252-0.9444063949896653-1.462264369447094-0.82158155071330550.8396593902729546-0.0314703908632008\n\n\n\n# 또는\ns_xx <- sum((dt$x-mean(dt$x))^2) #S_xx\nh_ii <- 1/21 + (dt$x- mean(dt$x))^2/s_xx\n### h_ii <- hatvalues(model_reg)\n### h_ii <- influence(model_reg)$hat\nhat_sigma <- summary(model_reg)$sigma #hat sigma\ns_residual <- resid(model_reg)/(hat_sigma*sqrt(1-h_ii)) ## 내적\n\n\nhist(s_residual)"
  },
  {
    "objectID": "posts/회귀진단 실습.html#외적으로-표준화된-잔차-externally-standardized-residual",
    "href": "posts/회귀진단 실습.html#외적으로-표준화된-잔차-externally-standardized-residual",
    "title": "6. 회귀진단 실습",
    "section": "외적으로 표준화된 잔차 ((externally) standardized residual)",
    "text": "외적으로 표준화된 잔차 ((externally) standardized residual)\n\\[r_i^* = \\dfrac{e_i}{\\hat \\sigma_i \\sqrt{1-h_{ii}}}\\]\n\\(\\hat \\sigma_i : i\\)번째 측정값 \\(y_i\\)를 제외하고 얻어진 \\(\\hat \\sigma\\)\n\\(\\hat \\sigma_i^2 = \\left[ (n-p-1) \\hat \\sigma^2 - \\dfrac{e_i^2}{1-h_{ii}} \\right] / (n-p-2)\\)\n\ns_residual_i <- rstudent(model_reg)\nhead(s_residual_i)\n\n10.1839684933793942-0.9415833513782013-1.510811922917994-0.81426336315943850.8328629175207956-0.030631827537088\n\n\n\n# 또는\nhat_sigma_i <- sqrt(((21-1-1)*hat_sigma^2 - residual^2/(1-h_ii) )/(21-1-2))\n## hat_sigma_i <- influence(model_reg)$sigma\ns_residual_i <- residual/(hat_sigma_i*sqrt(1-h_ii)) ## 외적\n\n\nhist(s_residual_i)\n\n\n\n\n\nwhich.max(s_residual_i)\ns_residual_i[which.max(s_residual_i)]\n\n19: 19\n\n\n19: 3.60697972130439\n\n\n\nqt(0.975, 21-1-2)\n\n2.10092204024104\n\n\n\\(|r_i^*| \\geq t_{\\alpha/2}(n-p-2)\\)이면, 유의수준 \\(\\alpha\\)에서, \\(i\\)번째 관측값이 이상점이라고 할 수 있다.\n따라서 19번째 관측값은 유의수준 0.05에서 이상점이라고 할 수 있다.\n\nplot(y~x, dt,pch = 20,cex = 2,col = \"darkorange\")\ntext(dt[19,],\"19\", pos=2)\nabline(model_reg, col='steelblue', lwd=2)\n\n\n\n\n\ns_residual_i[which(abs(s_residual_i)>qt(0.975,21-2))]\n\n19: 3.60697972130439\n\n\n\n## 잔차그림\npar(mfrow = c(2, 2))\nplot(fitted(model_reg), residual,\n pch=20,cex = 2,col = \"darkorange\",\n xlab = \"Fitted\", ylab = \"Residuals\",\n main = \"residual plot\")\nabline(h=0, lty=2)\nplot(fitted(model_reg), s_residual,\n pch=20,cex = 2,col = \"darkorange\",\n xlab = \"Fitted\", ylab = \"S_Residuals\",\n ylim=c(min(-3, min(s_residual)),\n max(3,max(s_residual))),\n main = \"standardized residual plot\")\nabline(h=c(-2,0,2), lty=2)\nplot(fitted(model_reg), s_residual_i,\n pch=20,cex = 2,col = \"darkorange\",\n xlab = \"Fitted\", ylab = \"S_Residuals_(i)\",\n ylim=c(min(-3, min(s_residual_i)),\n max(3,max(s_residual_i))),\n main = \"studentized residual plot\")\nabline(h=c(-3,-2,0,2,3), lty=2)\nplot(fitted(model_reg), s_residual_i,\n pch=20,cex = 2,col = \"darkorange\",\n xlab = \"Fitted\", ylab = \"S_Residuals_(i)\",\n ylim=c(min(-3, min(s_residual_i)),\n max(3,max(s_residual_i))),\n main = \"studentized residual plot\")\nabline(h=c(-qt(0.975,21-2),0,qt(0.975,21-2)), lty=2)\ntext (fitted(model_reg)[which(abs(s_residual_i)>qt(0.975,21-2))],\n s_residual_i[which(abs(s_residual_i)>qt(0.975,21-2))],\n which(abs(s_residual_i)>qt(0.975,21-2)),adj = c(0,1))"
  },
  {
    "objectID": "posts/회귀진단 실습.html#정규성-검정",
    "href": "posts/회귀진단 실습.html#정규성-검정",
    "title": "6. 회귀진단 실습",
    "section": "정규성 검정",
    "text": "정규성 검정\n\n## 정규성 검정\npar(mfrow=c(1,2))\nhist(resid(model_reg),\n xlab = \"Residuals\",\n main = \"Histogram of Residuals\",\n col = \"darkorange\",\n border = \"dodgerblue\",\n breaks = 20)\nqqnorm(resid(model_reg),\n main = \"Normal Q-Q Plot\",\n col = \"darkgrey\",\n pch=16)\nqqline(resid(model_reg), col = \"dodgerblue\", lwd = 2)\n\n\n\n\n\n## Shapiro-Wilk Test\n## H0 : normal distribution vs. H1 : not H0\nshapiro.test(resid(model_reg))\n\n\n    Shapiro-Wilk normality test\n\ndata:  resid(model_reg)\nW = 0.92578, p-value = 0.1133\n\n\n\n## 독립성 검정\nlmtest::dwtest(model_reg)\n\n\n    Durbin-Watson test\n\ndata:  model_reg\nDW = 2.0844, p-value = 0.5716\nalternative hypothesis: true autocorrelation is greater than 0\n\n\n\n### 등분산성\n## H0 : 등분산 vs. H1 : 이분산 (Heteroscedasticity)\nbptest(model_reg)\n\n\n    studentized Breusch-Pagan test\n\ndata:  model_reg\nBP = 0.0014282, df = 1, p-value = 0.9699"
  },
  {
    "objectID": "posts/회귀진단 실습.html#영향점",
    "href": "posts/회귀진단 실습.html#영향점",
    "title": "6. 회귀진단 실습",
    "section": "영향점",
    "text": "영향점\n\nplot(y~x, dt,pch = 20,cex = 2,col = \"darkorange\")\nabline(model_reg, col='steelblue', lwd=2)\n\n\n\n\n\ninfluence(model_reg)\n\n\n\n    $hat\n        10.047922479451021820.15451323429605630.062815775582535340.070545207752054950.047922479451021860.072618957846316370.057989593544981580.056669934394087990.0798582309026469100.0726189578463163110.0907548450343111120.0705452077520549130.0628157755825353140.0566699343940879150.0566699343940879160.0628157755825353170.0521076841867129180.65160998416409190.0530502978659226200.0566699343940879210.0628157755825353\n\n    $coefficients\n        \nA matrix: 21 × 2 of type dbl\n\n    (Intercept)x\n\n\n    1 0.086545033 0.001045618\n    2 0.958749611-0.104156236\n    3-1.623423657 0.057755211\n    4-1.022877601 0.040022565\n    5 0.384830249 0.004649436\n    6 0.005894578-0.001602673\n    7 0.023216186 0.010379001\n    8 0.230329653-0.007159985\n    9 0.410719785-0.017252806\n    10-0.117621441 0.031980023\n    11 1.595051450-0.070799821\n    12-0.437100147 0.017102603\n    13-1.623423657 0.057755211\n    14-1.230320253 0.038245507\n    15 0.412910892-0.012835671\n    16 0.145243868-0.005167222\n    17 0.681955144-0.017203712\n    18 4.243970455-0.347768136\n    19 0.569162106 0.066321856\n    20-1.047739014 0.032569820\n    21 0.145243868-0.005167222\n\n\n\n    $sigma\n        111.3143301900592211.0559573770349310.6687048798294411.1219769595247511.1128596831455611.3246668862836711.2946094952564811.3083981658447911.29861430792641011.20682225901661110.99278346332161211.28816820706241310.66870487982941410.84242194398621511.27164317318071611.31986011811681711.12966417084631811.1067560007425198.628196059920932010.97712792942652111.3198601181168\n\n    $wt.res\n        12.030993137772482-9.572128798733133-15.60395143654324-8.7309403514063859.030993137772416-0.33406228791192573.4119598823618182.5230374783198893.14207073373048106.665937712088081111.015081818867412-3.7309403514063813-15.603951436543214-13.4769625216801154.52303747831988161.39604856345675178.6500263931830218-5.540306160923011930.284970967498720-11.4769625216801211.39604856345675\n\n\n\n\n\ninfluence.measures(model_reg)\n\nInfluence measures of\n     lm(formula = y ~ x, data = dt) :\n\n     dfb.1_    dfb.x    dffit cov.r   cook.d    hat inf\n1   0.01664  0.00328  0.04127 1.166 8.97e-04 0.0479    \n2   0.18862 -0.33480 -0.40252 1.197 8.15e-02 0.1545    \n3  -0.33098  0.19239 -0.39114 0.936 7.17e-02 0.0628    \n4  -0.20004  0.12788 -0.22433 1.115 2.56e-02 0.0705    \n5   0.07532  0.01487  0.18686 1.085 1.77e-02 0.0479    \n6   0.00113 -0.00503 -0.00857 1.201 3.88e-05 0.0726    \n7   0.00447  0.03266  0.07722 1.170 3.13e-03 0.0580    \n8   0.04430 -0.02250  0.05630 1.174 1.67e-03 0.0567    \n9   0.07907 -0.05427  0.08541 1.200 3.83e-03 0.0799    \n10 -0.02283  0.10141  0.17284 1.152 1.54e-02 0.0726    \n11  0.31560 -0.22889  0.33200 1.088 5.48e-02 0.0908    \n12 -0.08422  0.05384 -0.09445 1.183 4.68e-03 0.0705    \n13 -0.33098  0.19239 -0.39114 0.936 7.17e-02 0.0628    \n14 -0.24681  0.12536 -0.31367 0.992 4.76e-02 0.0567    \n15  0.07968 -0.04047  0.10126 1.159 5.36e-03 0.0567    \n16  0.02791 -0.01622  0.03298 1.187 5.74e-04 0.0628    \n17  0.13328 -0.05493  0.18717 1.096 1.79e-02 0.0521    \n18  0.83112 -1.11275 -1.15578 2.959 6.78e-01 0.6516   *\n19  0.14348  0.27317  0.85374 0.396 2.23e-01 0.0531   *\n20 -0.20761  0.10544 -0.26385 1.043 3.45e-02 0.0567    \n21  0.02791 -0.01622  0.03298 1.187 5.74e-04 0.0628    \n\n\n\nDFFITS: \\(DFFITS(i) = \\dfrac{\\hat y_i - \\tilde y_i(i)}{\\hat \\sigma_{(i)} \\sqrt{h_{ii}}}\\)\n\\(|DFFITS(i)| \\geq 2 \\sqrt{\\dfrac{p+1}{n-p-1}}\\)이면 영향점\n\n\ndffits(model_reg) \n\n10.04127403575140562-0.4025206873025253-0.3911400454742154-0.22432853366080450.1868559838824216-0.0085717364067812270.077223952838937980.056303486522047690.085407472693718100.172840518129759110.33199685399425312-0.094449643042361813-0.39114004547421514-0.313673908094842150.101264129345836160.0329813827461469170.18716612805440518-1.15577873097521190.85373710713076620-0.263846244162542210.0329813827461469\n\n\n\nwhich(abs(dffits(model_reg)) > 2*sqrt(2/(21-2)))\n\n18181919\n\n\n\nCook’s Distance\n\\(D(i) = \\dfrac{\\sum_{i=1}^n (\\hat y_j - \\hat y_j(i))^2}{(p+1)\\hat \\sigma^2}=\\dfrac{(\\hat \\beta - \\hat \\beta(i))^T X^T X (\\hat \\beta - \\hat \\beta(i))}{(p+1) \\hat \\sigma^2}\\)\n\\(\\hat \\beta(i): i\\)번째 관측치를 제외하고 \\(n-1\\)개의 관측값에서 구한 \\(\\hat \\beta\\)의 최소제곱추저량\n\n\ncooks.distance(model_reg)\n\n10.00089740639287069120.081497955150763530.071658144221383340.025615958245264150.017743662633501363.87762740910137e-0570.003130574802994980.0016682085781346990.00383194880672965100.0154395158127621110.0548101351203612120.00467762256482442130.0716581442213833140.0475978118328145150.00536121617564154160.000573584529113046170.017856495213809180.678112028575845190.223288273631179200.0345188940892692210.000573584529113046\n\n\n\n\\(D(i) \\geq F_{0.5}(p+1, n-p-1)\\)이면 영향점으로 의심\n\n\nqf(0.5,2,21-2)\n\n0.719060569091733\n\n\n\nwhich(cooks.distance(model_reg) >qf(0.5,2,21-2))\n\n\n\n\n\nCOVRATIO\n\n\\(COVRATIO(i) = \\dfrac{1}{\\left[1+\\dfrac{(r_i^*)^2-1}{n-p-1}\\right]^{p+1}(1-h_{ii})}\\)\n\\(|COVRATIO(i)-1| \\geq 3(p+1)/n\\)이면 \\(i\\)번째 관측치를 영향을 크게 주는 측정값으로 볼 수 있음\n\ncovratio(model_reg)\n\n11.1658918168321921.1969989767629630.93634739734183941.1151026899392951.0850410825772861.2013199827549771.1701575789867381.1742372676080391.19966823450598101.15209128858604111.08783960928084121.18326164825873130.936347397341839140.992331347870996151.15904532932769161.18673688685713171.09643883044992182.95868271380702190.396431612340971201.04257281407241211.18673688685713\n\n\n\nwhich(abs(covratio(model_reg)-1) > 3*(1+1)/21)\n\n18181919\n\n\n\n영향점\n\n\nsummary(influence.measures(model_reg))\n\nPotentially influential observations of\n     lm(formula = y ~ x, data = dt) :\n\n   dfb.1_ dfb.x   dffit   cov.r   cook.d hat    \n18  0.83  -1.11_* -1.16_*  2.96_*  0.68   0.65_*\n19  0.14   0.27    0.85    0.40_*  0.22   0.05  \n\n\n\n## 18제거 전후\nplot(y~x, dt,pch = 20,\n cex = 2,col = \"darkorange\",\n main = \"18번 제거\")\nabline(model_reg, col='steelblue', lwd=2)\nabline(lm(y~x, dt[-18,]), col='red', lwd=2)\ntext(dt[18,], pos=2, \"18\")\nlegend('topright', legend=c(\"full\", \"del(18)\"),\n col=c('steelblue', 'red'), lty=1, lwd=2)\n# high leverage and high influence, not outlier\n\n\n\n\n\n## 19제거 전후\nplot(y~x, dt,pch = 20,\n cex = 2,col = \"darkorange\",\n main = \"19번 제거\")\nabline(model_reg, col='steelblue', lwd=2)\nabline(lm(y~x, dt[-19,]), col='red', lwd=2)\ntext(dt[19,], pos=2, \"19\")\nlegend('topright', legend=c(\"full\", \"del(19)\"),\n col=c('steelblue', 'red'), lty=1, lwd=2)\n# not leverage and high influence, outlier\n\n\n\n\n\n## 18, 19제거 전후\nplot(y~x, dt,pch = 20,\n cex = 2,col = \"darkorange\",\n main = \"18,19번 제거\")\nabline(model_reg, col='steelblue', lwd=2)\nabline(lm(y~x, dt[-c(18,19),]), col='red', lwd=2)\ntext(dt[c(18,19),], pos=2, c(\"18\",\"19\"))\nlegend('topright', legend=c(\"full\", \"del(18,19)\"),\n col=c('steelblue', 'red'), lty=1, lwd=2)\n\n\n\n\n\n## 회귀진단 그림\npar(mfrow = c(2, 2))\nplot(model_reg, pch=16)"
  },
  {
    "objectID": "posts/graph4-3.html",
    "href": "posts/graph4-3.html",
    "title": "CH4. 지도 그래프 학습(그래프 정규화 방법)",
    "section": "",
    "text": "그래프 머신러닝\ngithub"
  },
  {
    "objectID": "posts/graph4-3.html#load-dataset",
    "href": "posts/graph4-3.html#load-dataset",
    "title": "CH4. 지도 그래프 학습(그래프 정규화 방법)",
    "section": "Load Dataset",
    "text": "Load Dataset\n- 데이터셋: Cora\n\n7개의 클래스로 라벨링돼 있는 2,708개의 컴퓨터 사이언스 논문\n각 논문은 인용을 기반으로 다른 노드와 연결된 노드\n총 5,429개의 간선\n\n\nfrom stellargraph import datasets\n\n2023-04-06 21:44:50.486139: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n\n\n\ndataset = datasets.Cora()\n\n\n%config Completer.use_jedi = False\n\n\ndataset.download()\n\n\nlabel_index = {\n      'Case_Based': 0,\n      'Genetic_Algorithms': 1,\n      'Neural_Networks': 2,\n      'Probabilistic_Methods': 3,\n      'Reinforcement_Learning': 4,\n      'Rule_Learning': 5,\n      'Theory': 6,\n  }\n\n\nG, labels = dataset.load()\n\n\nG: 네트워크 노드, 간선, BOW표현 설명\nlabea : 논문id와 클래스 중 하나 사이의 매핑\n훈련 샘플: 이웃과 관련된 정보가 포함 -> 훈련을 정규화 하는데 사용\n검증 샘플: 이웃과 관련된 정보 불포함 , 예측된 라벨은 노드 특증, bow표현에만 의존\n\n\nimport numpy as np\nfrom sklearn import preprocessing, feature_extraction, model_selection\n\n\nimport tensorflow as tf\nfrom tensorflow.train import Example, Features, Feature, Int64List, BytesList, FloatList\n\n\nGRAPH_PREFIX=\"NL_nbr\"\n\n\ndef _int64_feature(*value):\n    \"\"\"Returns int64 tf.train.Feature from a bool / enum / int / uint.\"\"\"\n    return Feature(int64_list=Int64List(value=list(value)))\n\ndef _bytes_feature(value):\n    \"\"\"Returns bytes tf.train.Feature from a string.\"\"\"\n    return Feature(\n        bytes_list=BytesList(value=[value.encode('utf-8')])\n    )\n\ndef _float_feature(*value):\n    return Feature(float_list=FloatList(value=list(value)))\n\n\n_int64_feature 함수는 bool, enum, int, uint 데이터 타입을 입력 받아 int64_list 타입의 tf.train.Feature 객체를 반환\n_bytes_feature 함수는 문자열 값을 입력 받아 utf-8로 인코딩하여 bytes_list 타입의 tf.train.Feature 객체를 반환\n_float_feature 함수는 float 데이터 타입을 입력 받아 float_list 타입의 tf.train.Feature 객체를 반환\n\n- 반지도 학습 데이터 셋 만드는 함수 정의\n\nfrom functools import reduce\nfrom typing import List, Tuple\nimport pandas as pd\nimport six\n\ndef addFeatures(x, y):\n    res = Features()\n    res.CopyFrom(x)\n    res.MergeFrom(y)\n    return res\n\ndef neighborFeatures(features: Features, weight: float, prefix: str):  # 객체, 가중치, 접두어 입력으로 받음\n    data = {f\"{prefix}_weight\": _float_feature(weight)}\n    for name, feature in six.iteritems(features.feature):\n        data[f\"{prefix}_{name}\"] = feature \n    return Features(feature=data)\n\ndef neighborsFeatures(neighbors: List[Tuple[Features, float]]):\n    return reduce(\n        addFeatures, \n        [neighborFeatures(sample, weight, f\"{GRAPH_PREFIX}_{ith}\") for ith, (sample, weight) in enumerate(neighbors)],\n        Features()\n    )\n\ndef getNeighbors(idx, adjMatrix, topn=5): #인덱스와 인접행렬 이용하여 이웃 데이터셋 추출 \n    weights = adjMatrix.loc[idx]\n    return weights[weights>0].sort_values(ascending=False).head(topn).to_dict()\n    \n\ndef semisupervisedDataset(G, labels, ratio=0.2, topn=5):  #라벨이 있는 데이터와 없는 데이터 추출\n     #ratio:라벨 유무 비율 설정\n     #topn: 함수에서 추출할 이웃 데이터셋 크기 설정\n    n = int(np.round(len(labels)*ratio)) \n    \n    labelled, unlabelled = model_selection.train_test_split(\n        labels, train_size=n, test_size=None, stratify=labels\n    )\n\n\n1. 노드 특징 df로 구성하고 그래프 인접행렬로 저장\n\nadjMatrix = pd.DataFrame.sparse.from_spmatrix(G.to_adjacency_matrix(), index=G.nodes(), columns=G.nodes())\n    \nfeatures = pd.DataFrame(G.node_features(), index=G.nodes())\n\n\n\n2. adjMatrix사용해 노드ID와 간선 가중치 반환하여 노드의 가장 가까운 TOPN이웃 검색하는 도우미 함수 구현\n\ndef getNeighbors(idx, adjMatrix, topn=5): #인덱스와 인접행렬 이용하여 이웃 데이터셋 추출 \n    weights = adjMatrix.loc[idx]\n    neighbors = weights[weights>0]\\\n        .sort_values(ascending=False)\\\n        .head(topn)\n    return [(k,v) for k, v in neighbors.iteritems()]\n    \n\n\n3. 정보를 단일 df로 병합\n\ndataset = {\n        index: Features(feature = {\n            #\"id\": _bytes_feature(str(index)), \n            \"id\": _int64_feature(index),\n            \"words\": _float_feature(*[float(x) for x in features.loc[index].values]), \n            \"label\": _int64_feature(label_index[label])\n        })\n        for index, label in pd.concat([labelled, unlabelled]).items()\n    }\n\nNameError: name 'labelled' is not defined\n\n\n\nfrom functools import reduce\nfrom typing import List, Tuple\nimport pandas as pd\nimport six\n\ndef addFeatures(x, y):\n    res = Features()\n    res.CopyFrom(x)\n    res.MergeFrom(y)\n    return res\n\ndef neighborFeatures(features: Features, weight: float, prefix: str):\n    data = {f\"{prefix}_weight\": _float_feature(weight)}\n    for name, feature in six.iteritems(features.feature):\n        data[f\"{prefix}_{name}\"] = feature \n    return Features(feature=data)\n\ndef neighborsFeatures(neighbors: List[Tuple[Features, float]]):\n    return reduce(\n        addFeatures, \n        [neighborFeatures(sample, weight, f\"{GRAPH_PREFIX}_{ith}\") for ith, (sample, weight) in enumerate(neighbors)],\n        Features()\n    )\n\ndef getNeighbors(idx, adjMatrix, topn=5):\n    weights = adjMatrix.loc[idx]\n    return weights[weights>0].sort_values(ascending=False).head(topn).to_dict()\n    \n\ndef semisupervisedDataset(G, labels, ratio=0.2, topn=5):\n    n = int(np.round(len(labels)*ratio))\n    \n    labelled, unlabelled = model_selection.train_test_split(\n        labels, train_size=n, test_size=None, stratify=labels\n    )\n    \n    adjMatrix = pd.DataFrame.sparse.from_spmatrix(G.to_adjacency_matrix(), index=G.nodes(), columns=G.nodes())\n    \n    features = pd.DataFrame(G.node_features(), index=G.nodes())\n    \n    dataset = {\n        index: Features(feature = {\n            #\"id\": _bytes_feature(str(index)), \n            \"id\": _int64_feature(index),\n            \"words\": _float_feature(*[float(x) for x in features.loc[index].values]), \n            \"label\": _int64_feature(label_index[label])\n        })\n        for index, label in pd.concat([labelled, unlabelled]).items()\n    }\n    \n    trainingSet = [\n        Example(features=addFeatures(\n            dataset[exampleId], \n            neighborsFeatures(\n                [(dataset[nodeId], weight) for nodeId, weight in getNeighbors(exampleId, adjMatrix, topn).items()]\n            )\n        ))\n        for exampleId in labelled.index\n    ]\n    \n    testSet = [Example(features=dataset[exampleId]) for exampleId in unlabelled.index]\n\n    serializer = lambda _list: [e.SerializeToString() for e in _list]\n    \n    return serializer(trainingSet), serializer(testSet)"
  },
  {
    "objectID": "posts/학회.html",
    "href": "posts/학회.html",
    "title": "Scribbling",
    "section": "",
    "text": "일정:23. 6.29(목) ~ 7.1 (토)\n장소: 부경대학교(부산)\n발표신청 및 초록제출: 3.20.(월) ~ 4.20.(목)\n발표요약본제출(석사과정) : ~4.20.(목)\n포스터파일제출: ~ 5.19.(금)\n\n\n\n\n\n하계: 2023. 7.6.(목) ~ 7.7.(금)\n고려대학교\n발표신청 및 사전등록: 23.5.29.(월)\n초록 또는 논문제출: ~5.31.(수)\n\n\n\n\n\n일정: 6.23.(금) ~ 6.24.(토)\n장소: 강릉원주대학교\n발표논문 초록 제출: ~5.5.(금)\n발표 논문 제출: ~6.16.(금)\n학생논문: ~6.9.(금)"
  },
  {
    "objectID": "posts/변수선택 실습.html",
    "href": "posts/변수선택 실습.html",
    "title": "7. 변수선택 실습",
    "section": "",
    "text": "dt <- data.frame(\n\n x1 = c(7,1,11,11,7,11,3,1,2,21,1,11,10),\n x2 = c(26,29,56,31,52,55,71,31,54,47,40,66,68),\n x3 = c(6,15,8,8,6,9,17,22,18,4,23,9,8),\n x4 = c(60,52,20,47,33,22,6,44,22,26,34,12,12),\n y = c(78.5,74.3,104.3,87.6,95.9,109.2,102.7,72.5,93.1,115.9,83.8,113.3,109.4)\n)\n\n\npairs(dt, pch=16)\ncor(dt)\n\n\n\nA matrix: 5 × 5 of type dbl\n\n    x1x2x3x4y\n\n\n    x1 1.0000000 0.2285795-0.8241338-0.2454451 0.7307175\n    x2 0.2285795 1.0000000-0.1392424-0.9729550 0.8162526\n    x3-0.8241338-0.1392424 1.0000000 0.0295370-0.5346707\n    x4-0.2454451-0.9729550 0.0295370 1.0000000-0.8213050\n    y 0.7307175 0.8162526-0.5346707-0.8213050 1.0000000\n\n\n\n\n\n\n\n\nFull Model : \\(𝑦 = 𝛽_0 + 𝛽_1𝑥_1 + 𝛽_2𝑥_2 + 𝛽_3𝑥_3 + 𝛽_4𝑥_4 + \\epsilon\\)\n\n\nm <- lm(y~., dt) ##FM\nsummary(m)\n\n\nCall:\nlm(formula = y ~ ., data = dt)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.1750 -1.6709  0.2508  1.3783  3.9254 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)  \n(Intercept)  62.4054    70.0710   0.891   0.3991  \nx1            1.5511     0.7448   2.083   0.0708 .\nx2            0.5102     0.7238   0.705   0.5009  \nx3            0.1019     0.7547   0.135   0.8959  \nx4           -0.1441     0.7091  -0.203   0.8441  \n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\nResidual standard error: 2.446 on 8 degrees of freedom\nMultiple R-squared:  0.9824,    Adjusted R-squared:  0.9736 \nF-statistic: 111.5 on 4 and 8 DF,  p-value: 4.756e-07\n\n\n\n\n\nsummary(m)\n\n\nCall:\nlm(formula = y ~ ., data = dt)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.1750 -1.6709  0.2508  1.3783  3.9254 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)  \n(Intercept)  62.4054    70.0710   0.891   0.3991  \nx1            1.5511     0.7448   2.083   0.0708 .\nx2            0.5102     0.7238   0.705   0.5009  \nx3            0.1019     0.7547   0.135   0.8959  \nx4           -0.1441     0.7091  -0.203   0.8441  \n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\nResidual standard error: 2.446 on 8 degrees of freedom\nMultiple R-squared:  0.9824,    Adjusted R-squared:  0.9736 \nF-statistic: 111.5 on 4 and 8 DF,  p-value: 4.756e-07\n\n\n\ndrop1(m, test = \"F\") #x3 제거\n\n\n\nA anova: 5 × 6\n\n    DfSum of SqRSSAICF valuePr(>F)\n    <dbl><dbl><dbl><dbl><dbl><dbl>\n\n\n    <none>NA        NA47.8636426.94429        NA        NA\n    x1 125.950911473.8145530.575884.337474000.07082169\n    x2 1 2.972478250.8361225.727550.496824440.50090110\n    x3 1 0.109090047.9727324.973880.018233470.89592269\n    x4 1 0.246974748.1106125.011200.041279720.84407147\n\n\n\n\n\nm1 <- update(m, ~ . -x3)\nsummary(m1) #x4 제거\n\n\nCall:\nlm(formula = y ~ x1 + x2 + x4, data = dt)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.0919 -1.8016  0.2562  1.2818  3.8982 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  71.6483    14.1424   5.066 0.000675 ***\nx1            1.4519     0.1170  12.410 5.78e-07 ***\nx2            0.4161     0.1856   2.242 0.051687 .  \nx4           -0.2365     0.1733  -1.365 0.205395    \n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\nResidual standard error: 2.309 on 9 degrees of freedom\nMultiple R-squared:  0.9823,    Adjusted R-squared:  0.9764 \nF-statistic: 166.8 on 3 and 9 DF,  p-value: 3.323e-08\n\n\n\ndrop1(m1, test = \"F\")\n\n\n\nA anova: 4 × 6\n\n    DfSum of SqRSSAICF valuePr(>F)\n    <dbl><dbl><dbl><dbl><dbl><dbl>\n\n\n    <none>NA        NA 47.9727324.97388        NA          NA\n    x1 1820.907402868.8801360.62933154.0076355.780764e-07\n    x2 1 26.789383 74.7621128.74170  5.0258655.168735e-02\n    x4 1  9.931754 57.9044825.41999  1.8632622.053954e-01\n\n\n\n\n\nm2 <- update(m1, ~ . -x4)\nsummary(m2) \n\n\nCall:\nlm(formula = y ~ x1 + x2, data = dt)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-2.893 -1.574 -1.302  1.363  4.048 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 52.57735    2.28617   23.00 5.46e-10 ***\nx1           1.46831    0.12130   12.11 2.69e-07 ***\nx2           0.66225    0.04585   14.44 5.03e-08 ***\n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\nResidual standard error: 2.406 on 10 degrees of freedom\nMultiple R-squared:  0.9787,    Adjusted R-squared:  0.9744 \nF-statistic: 229.5 on 2 and 10 DF,  p-value: 4.407e-09\n\n\n\ndrop1(m2, test = \"F\")\n\n\n\nA anova: 3 × 6\n\n    DfSum of SqRSSAICF valuePr(>F)\n    <dbl><dbl><dbl><dbl><dbl><dbl>\n\n\n    <none>NA       NA  57.9044825.41999      NA          NA\n    x1 1 848.4319 906.3363459.17799146.52272.692212e-07\n    x2 11207.78231265.6867563.51947208.58185.028960e-08\n\n\n\n\n\n\n\n\nStart model : \\(𝑦 = 𝛽_0 + \\epsilon\\)\n\n\nm0 = lm(y ~ 1, data = dt)\n\n\nadd1(m0,\n scope = y ~ x1 + x2 + x3+ x4,\n test = \"F\") ## x4추가\n\n\n\nA anova: 5 × 6\n\n    DfSum of SqRSSAICF valuePr(>F)\n    <dbl><dbl><dbl><dbl><dbl><dbl>\n\n\n    <none>NA       NA2715.763171.44443       NA          NA\n    x1 11450.07631265.686763.5194712.6025180.0045520446\n    x2 11809.4267 906.336359.1779921.9606050.0006648249\n    x3 1 776.36261939.400569.06740 4.4034170.0597623242\n    x4 11831.8962 883.866958.8516422.7985200.0005762318\n\n\n\n\n\nm1 <- update(m0, ~ . +x4)\nsummary(m1)\n\n\nCall:\nlm(formula = y ~ x4, data = dt)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-12.589  -8.228   1.495   4.726  17.524 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 117.5679     5.2622  22.342 1.62e-10 ***\nx4           -0.7382     0.1546  -4.775 0.000576 ***\n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\nResidual standard error: 8.964 on 11 degrees of freedom\nMultiple R-squared:  0.6745,    Adjusted R-squared:  0.645 \nF-statistic:  22.8 on 1 and 11 DF,  p-value: 0.0005762\n\n\n\nadd1(m1,\n scope = y ~ x1 + x2 + x3+ x4,\n test = \"F\") ## x1추가\n\n\n\nA anova: 4 × 6\n\n    DfSum of SqRSSAICF valuePr(>F)\n    <dbl><dbl><dbl><dbl><dbl><dbl>\n\n\n    <none>NA       NA883.8669258.85164         NA          NA\n    x1 1809.10480 74.7621128.74170108.22390931.105281e-06\n    x2 1 14.98679868.8801360.62933  0.17248396.866842e-01\n    x3 1708.12891175.7380039.85258 40.29458028.375467e-05\n\n\n\n\n\nm2 <- update(m1, ~ . +x1)\nsummary(m2)\n\n\nCall:\nlm(formula = y ~ x4 + x1, data = dt)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-5.0234 -1.4737  0.1371  1.7305  3.7701 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 103.09738    2.12398   48.54 3.32e-13 ***\nx4           -0.61395    0.04864  -12.62 1.81e-07 ***\nx1            1.43996    0.13842   10.40 1.11e-06 ***\n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\nResidual standard error: 2.734 on 10 degrees of freedom\nMultiple R-squared:  0.9725,    Adjusted R-squared:  0.967 \nF-statistic: 176.6 on 2 and 10 DF,  p-value: 1.581e-08\n\n\n\nadd1(m2,\n scope = y ~ x1 + x2 + x3+ x4,\n test = \"F\") ## stop\n\n\n\nA anova: 3 × 6\n\n    DfSum of SqRSSAICF valuePr(>F)\n    <dbl><dbl><dbl><dbl><dbl><dbl>\n\n\n    <none>NA      NA74.7621128.74170      NA        NA\n    x2 126.7893847.9727324.973885.0258650.05168735\n    x3 123.9259950.8361225.727554.2358460.06969226\n\n\n\n\n\n\n\n\nm0 = lm(y ~ 1, data = dt)\n\n\nadd1(m0,\n scope = y ~ x1 + x2 + x3+ x4,\n test = \"F\") ## x4추가\n\n\n\nA anova: 5 × 6\n\n    DfSum of SqRSSAICF valuePr(>F)\n    <dbl><dbl><dbl><dbl><dbl><dbl>\n\n\n    <none>NA       NA2715.763171.44443       NA          NA\n    x1 11450.07631265.686763.5194712.6025180.0045520446\n    x2 11809.4267 906.336359.1779921.9606050.0006648249\n    x3 1 776.36261939.400569.06740 4.4034170.0597623242\n    x4 11831.8962 883.866958.8516422.7985200.0005762318\n\n\n\n\n\nm1 <- update(m0, ~ . +x4)\nsummary(m1)\n\n\nCall:\nlm(formula = y ~ x4, data = dt)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-12.589  -8.228   1.495   4.726  17.524 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 117.5679     5.2622  22.342 1.62e-10 ***\nx4           -0.7382     0.1546  -4.775 0.000576 ***\n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\nResidual standard error: 8.964 on 11 degrees of freedom\nMultiple R-squared:  0.6745,    Adjusted R-squared:  0.645 \nF-statistic:  22.8 on 1 and 11 DF,  p-value: 0.0005762\n\n\n\nadd1(m1,\n scope = y ~ x1 + x2 + x3+ x4,\n test = \"F\") ## x1추가\n\n\n\nA anova: 4 × 6\n\n    DfSum of SqRSSAICF valuePr(>F)\n    <dbl><dbl><dbl><dbl><dbl><dbl>\n\n\n    <none>NA       NA883.8669258.85164         NA          NA\n    x1 1809.10480 74.7621128.74170108.22390931.105281e-06\n    x2 1 14.98679868.8801360.62933  0.17248396.866842e-01\n    x3 1708.12891175.7380039.85258 40.29458028.375467e-05\n\n\n\n\n\nm2 <- update(m1, ~ . +x1)\n\n\ndrop1(m2, test = \"F\") #제거 없음\n\n\n\nA anova: 3 × 6\n\n    DfSum of SqRSSAICF valuePr(>F)\n    <dbl><dbl><dbl><dbl><dbl><dbl>\n\n\n    <none>NA       NA  74.7621128.74170      NA          NA\n    x4 11190.92461265.6867563.51947159.29521.814890e-07\n    x1 1 809.1048 883.8669258.85164108.22391.105281e-06\n\n\n\n\n\nadd1(m2,\n scope = y ~ x1 + x2 + x3+ x4,\n test = \"F\") ## x2추가\n\n\n\nA anova: 3 × 6\n\n    DfSum of SqRSSAICF valuePr(>F)\n    <dbl><dbl><dbl><dbl><dbl><dbl>\n\n\n    <none>NA      NA74.7621128.74170      NA        NA\n    x2 126.7893847.9727324.973885.0258650.05168735\n    x3 123.9259950.8361225.727554.2358460.06969226\n\n\n\n\n\nm3 <- update(m2, ~ . +x2)\nsummary(m3) \n\n\nCall:\nlm(formula = y ~ x4 + x1 + x2, data = dt)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.0919 -1.8016  0.2562  1.2818  3.8982 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  71.6483    14.1424   5.066 0.000675 ***\nx4           -0.2365     0.1733  -1.365 0.205395    \nx1            1.4519     0.1170  12.410 5.78e-07 ***\nx2            0.4161     0.1856   2.242 0.051687 .  \n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\nResidual standard error: 2.309 on 9 degrees of freedom\nMultiple R-squared:  0.9823,    Adjusted R-squared:  0.9764 \nF-statistic: 166.8 on 3 and 9 DF,  p-value: 3.323e-08\n\n\n\ndrop1(m3, test=\"F\") #x4 제거\n\n\n\nA anova: 4 × 6\n\n    DfSum of SqRSSAICF valuePr(>F)\n    <dbl><dbl><dbl><dbl><dbl><dbl>\n\n\n    <none>NA        NA 47.9727324.97388        NA          NA\n    x4 1  9.931754 57.9044825.41999  1.8632622.053954e-01\n    x1 1820.907402868.8801360.62933154.0076355.780764e-07\n    x2 1 26.789383 74.7621128.74170  5.0258655.168735e-02\n\n\n\n\n\nm4 <- update(m3, ~ . -x4)\nsummary(m4)\n\n\nCall:\nlm(formula = y ~ x1 + x2, data = dt)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-2.893 -1.574 -1.302  1.363  4.048 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 52.57735    2.28617   23.00 5.46e-10 ***\nx1           1.46831    0.12130   12.11 2.69e-07 ***\nx2           0.66225    0.04585   14.44 5.03e-08 ***\n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\nResidual standard error: 2.406 on 10 degrees of freedom\nMultiple R-squared:  0.9787,    Adjusted R-squared:  0.9744 \nF-statistic: 229.5 on 2 and 10 DF,  p-value: 4.407e-09\n\n\n\nadd1(m4,\n scope = y ~ x1 + x2 + x3+ x4,\n test = \"F\") #stop\n\n\n\nA anova: 3 × 6\n\n    DfSum of SqRSSAICF valuePr(>F)\n    <dbl><dbl><dbl><dbl><dbl><dbl>\n\n\n    <none>NA      NA57.9044825.41999      NA       NA\n    x3 19.79386948.1106125.011201.8321280.2088895\n    x4 19.93175447.9727324.973881.8632620.2053954\n\n\n\n\n\n\n\n\n\n\nmodel_back = step(m, direction = \"backward\")\nsummary(model_back)\n\nStart:  AIC=26.94\ny ~ x1 + x2 + x3 + x4\n\n       Df Sum of Sq    RSS    AIC\n- x3    1    0.1091 47.973 24.974\n- x4    1    0.2470 48.111 25.011\n- x2    1    2.9725 50.836 25.728\n<none>              47.864 26.944\n- x1    1   25.9509 73.815 30.576\n\nStep:  AIC=24.97\ny ~ x1 + x2 + x4\n\n       Df Sum of Sq    RSS    AIC\n<none>               47.97 24.974\n- x4    1      9.93  57.90 25.420\n- x2    1     26.79  74.76 28.742\n- x1    1    820.91 868.88 60.629\n\n\n\nCall:\nlm(formula = y ~ x1 + x2 + x4, data = dt)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.0919 -1.8016  0.2562  1.2818  3.8982 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  71.6483    14.1424   5.066 0.000675 ***\nx1            1.4519     0.1170  12.410 5.78e-07 ***\nx2            0.4161     0.1856   2.242 0.051687 .  \nx4           -0.2365     0.1733  -1.365 0.205395    \n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\nResidual standard error: 2.309 on 9 degrees of freedom\nMultiple R-squared:  0.9823,    Adjusted R-squared:  0.9764 \nF-statistic: 166.8 on 3 and 9 DF,  p-value: 3.323e-08\n\n\n\n\n\n\nmodel_forward = step(\n m0,\n scope = y ~ x1 + x2 + x3+ x4,\n direction = \"forward\")\nsummary(model_forward)\n\nStart:  AIC=71.44\ny ~ 1\n\n       Df Sum of Sq     RSS    AIC\n+ x4    1   1831.90  883.87 58.852\n+ x2    1   1809.43  906.34 59.178\n+ x1    1   1450.08 1265.69 63.519\n+ x3    1    776.36 1939.40 69.067\n<none>              2715.76 71.444\n\nStep:  AIC=58.85\ny ~ x4\n\n       Df Sum of Sq    RSS    AIC\n+ x1    1    809.10  74.76 28.742\n+ x3    1    708.13 175.74 39.853\n<none>              883.87 58.852\n+ x2    1     14.99 868.88 60.629\n\nStep:  AIC=28.74\ny ~ x4 + x1\n\n       Df Sum of Sq    RSS    AIC\n+ x2    1    26.789 47.973 24.974\n+ x3    1    23.926 50.836 25.728\n<none>              74.762 28.742\n\nStep:  AIC=24.97\ny ~ x4 + x1 + x2\n\n       Df Sum of Sq    RSS    AIC\n<none>              47.973 24.974\n+ x3    1   0.10909 47.864 26.944\n\n\n\nCall:\nlm(formula = y ~ x4 + x1 + x2, data = dt)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.0919 -1.8016  0.2562  1.2818  3.8982 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  71.6483    14.1424   5.066 0.000675 ***\nx4           -0.2365     0.1733  -1.365 0.205395    \nx1            1.4519     0.1170  12.410 5.78e-07 ***\nx2            0.4161     0.1856   2.242 0.051687 .  \n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\nResidual standard error: 2.309 on 9 degrees of freedom\nMultiple R-squared:  0.9823,    Adjusted R-squared:  0.9764 \nF-statistic: 166.8 on 3 and 9 DF,  p-value: 3.323e-08\n\n\n\n\n\n\nmodel_step = step(\n m0,\n scope = y ~ x1 + x2 + x3+ x4,\n direction = \"both\")\nsummary(model_step)\n\nStart:  AIC=71.44\ny ~ 1\n\n       Df Sum of Sq     RSS    AIC\n+ x4    1   1831.90  883.87 58.852\n+ x2    1   1809.43  906.34 59.178\n+ x1    1   1450.08 1265.69 63.519\n+ x3    1    776.36 1939.40 69.067\n<none>              2715.76 71.444\n\nStep:  AIC=58.85\ny ~ x4\n\n       Df Sum of Sq     RSS    AIC\n+ x1    1    809.10   74.76 28.742\n+ x3    1    708.13  175.74 39.853\n<none>               883.87 58.852\n+ x2    1     14.99  868.88 60.629\n- x4    1   1831.90 2715.76 71.444\n\nStep:  AIC=28.74\ny ~ x4 + x1\n\n       Df Sum of Sq     RSS    AIC\n+ x2    1     26.79   47.97 24.974\n+ x3    1     23.93   50.84 25.728\n<none>                74.76 28.742\n- x1    1    809.10  883.87 58.852\n- x4    1   1190.92 1265.69 63.519\n\nStep:  AIC=24.97\ny ~ x4 + x1 + x2\n\n       Df Sum of Sq    RSS    AIC\n<none>               47.97 24.974\n- x4    1      9.93  57.90 25.420\n+ x3    1      0.11  47.86 26.944\n- x2    1     26.79  74.76 28.742\n- x1    1    820.91 868.88 60.629\n\n\n\nCall:\nlm(formula = y ~ x4 + x1 + x2, data = dt)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.0919 -1.8016  0.2562  1.2818  3.8982 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  71.6483    14.1424   5.066 0.000675 ***\nx4           -0.2365     0.1733  -1.365 0.205395    \nx1            1.4519     0.1170  12.410 5.78e-07 ***\nx2            0.4161     0.1856   2.242 0.051687 .  \n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\nResidual standard error: 2.309 on 9 degrees of freedom\nMultiple R-squared:  0.9823,    Adjusted R-squared:  0.9764 \nF-statistic: 166.8 on 3 and 9 DF,  p-value: 3.323e-08"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "끄적끄적 coco 올리기 전 아무거나 막 쓰는 용도"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Scribbling",
    "section": "",
    "text": ":::{#quarto-listing-pipeline .hidden} \\(e = mC^2\\)\n:::{.hidden render-id=“pipeline-listing-listing”}\n:::{.list .quarto-listing-default}\n\n\n\n\n2023년 학회 발표 준비\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n\n\n\n변수선택 실습\n\n\n\n\n\n\n\nApplied statistics\n\n\n\n\n\n\n\n\n\n\n\nMay 9, 2023\n\n\n김보람\n\n\n\n\n\n\n\n\n\n회귀진단 실습\n\n\n\n\n\n\n\nApplied statistics\n\n\n\n\n\n\n\n\n\n\n\nMay 6, 2023\n\n\n김보람\n\n\n\n\n\n\n\n\nCH4. 지도 그래프 학습(그래프 정규화 방법)\n\n\n\n\n\n\n\ngraph\n\n\n\n\n\n\n\n\n\n\n\nApr 6, 2023\n\n\n김보람\n\n\n\n\n\n\n\n\nVAE(-ing)\n\n\n\n\n\n\n\nVAE\n\n\nAuto-Encoding Variational Bayes\n\n\n\n\n\n\n\n\n\n\n\nMar 14, 2023\n\n\n김보람\n\n\n\n\n:::\n\n\n\nNo matching items\n\n:::\n:::"
  }
]