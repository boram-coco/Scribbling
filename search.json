[
  {
    "objectID": "posts/VAE.html",
    "href": "posts/VAE.html",
    "title": "VAE(-ing)",
    "section": "",
    "text": "https://arxiv.org/pdf/1312.6114.pdf"
  },
  {
    "objectID": "posts/VAE.html#오토인코더",
    "href": "posts/VAE.html#오토인코더",
    "title": "VAE(-ing)",
    "section": "오토인코더",
    "text": "오토인코더\n\nEncoder, Decoder 네트워크로 구성된 모델\n학습 데이터-> encoder에 입력값"
  },
  {
    "objectID": "posts/회귀진단 실습.html",
    "href": "posts/회귀진단 실습.html",
    "title": "6. 회귀진단 실습",
    "section": "",
    "text": "해당 자료는 전북대학교 이영미 교수님 2023응용통계학 자료임"
  },
  {
    "objectID": "posts/회귀진단 실습.html#leverage-vs.-outlier-vs.-influence",
    "href": "posts/회귀진단 실습.html#leverage-vs.-outlier-vs.-influence",
    "title": "6. 회귀진단 실습",
    "section": "Leverage vs. Outlier vs. Influence",
    "text": "Leverage vs. Outlier vs. Influence\n\nlibrary(lmtest)\n\nLoading required package: zoo\n\n\nAttaching package: ‘zoo’\n\n\nThe following objects are masked from ‘package:base’:\n\n    as.Date, as.Date.numeric\n\n\n\n\n\ndt <- data.frame(x = c(15,26,10,9,15,20,18,11,\n 8,20,7,9,10,11,11,10,12,42,17,11,10),\n y = c(95,71,83,91,102,87,93,100,\n 104,94,113,96,83,84,102,100,\n 105,57,121,86,100))\n\n\n######## 산점도\nplot(y~x, dt,pch = 20,cex = 2,col = \"darkorange\")"
  },
  {
    "objectID": "posts/회귀진단 실습.html#회귀모형-적합-ybeta_0-beta_1x-epsilon",
    "href": "posts/회귀진단 실습.html#회귀모형-적합-ybeta_0-beta_1x-epsilon",
    "title": "6. 회귀진단 실습",
    "section": "회귀모형 적합: \\(y=\\beta_0+ \\beta_1x + \\epsilon\\)",
    "text": "회귀모형 적합: \\(y=\\beta_0+ \\beta_1x + \\epsilon\\)\n\n######## 회귀적합\nmodel_reg <- lm(y~x, dt)\nsummary(model_reg)\n\n\nCall:\nlm(formula = y ~ x, data = dt)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-15.604  -8.731   1.396   4.523  30.285 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 109.8738     5.0678  21.681 7.31e-15 ***\nx            -1.1270     0.3102  -3.633  0.00177 ** \n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\nResidual standard error: 11.02 on 19 degrees of freedom\nMultiple R-squared:   0.41, Adjusted R-squared:  0.3789 \nF-statistic:  13.2 on 1 and 19 DF,  p-value: 0.001769\n\n\n\nplot(y~x, dt,pch = 20,cex = 2,col = \"darkorange\")\nabline(model_reg, col='steelblue', lwd=2)"
  },
  {
    "objectID": "posts/회귀진단 실습.html#hat-matrix",
    "href": "posts/회귀진단 실습.html#hat-matrix",
    "title": "6. 회귀진단 실습",
    "section": "Hat Matrix",
    "text": "Hat Matrix\n\nX = cbind(rep(1, nrow(dt)), dt$x)\nH = X %*% solve(t(X) %*% X) %*% t(X)\ndiag(H)\n\n\n0.04792247945102180.1545132342960560.06281577558253530.07054520775205490.04792247945102180.07261895784631620.05798959354498150.05666993439408790.07985823090264690.07261895784631620.09075484503431120.07054520775205490.06281577558253530.05666993439408790.05666993439408790.06281577558253530.05210768418671290.651609984164090.05305029786592260.05666993439408790.0628157755825353\n\n\n\nsum(diag(H))\n\n2\n\n\n\nhatvalues(model_reg)\n\n10.047922479451021820.15451323429605630.062815775582535340.070545207752054950.047922479451021860.072618957846316370.057989593544981580.056669934394087990.0798582309026469100.0726189578463163110.0907548450343111120.0705452077520549130.0628157755825353140.0566699343940879150.0566699343940879160.0628157755825353170.0521076841867129180.65160998416409190.0530502978659226200.0566699343940879210.0628157755825353\n\n\n\nwhich.max(hatvalues(model_reg))\nhatvalues(model_reg)[which.max(hatvalues(model_reg))] ##h_{18,18}\n\n18: 18\n\n\n18: 0.65160998416409\n\n\n\n2*(1+1)/nrow(dt)\n\n0.19047619047619\n\n\n\n\\(h_{18,18} > 2 \\bar h\\)이므로 18번째 관측값이 leverage point 로 고려 가능\n\n\nplot(y~x, dt,pch = 20,cex = 2,col = \"darkorange\")\ntext(dt[18,],\"18\", pos=2)\nabline(model_reg, col='steelblue', lwd=2)"
  },
  {
    "objectID": "posts/회귀진단 실습.html#잔차-e_i-y_i---hat-y_i",
    "href": "posts/회귀진단 실습.html#잔차-e_i-y_i---hat-y_i",
    "title": "6. 회귀진단 실습",
    "section": "잔차: \\(e_i = y_i - \\hat y_i\\)",
    "text": "잔차: \\(e_i = y_i - \\hat y_i\\)\n\nresidual <- model_reg$residuals\nhead(residual)\n\n12.030993137772482-9.572128798733133-15.60395143654324-8.7309403514063859.030993137772416-0.334062287911925\n\n\n\nhist(residual)"
  },
  {
    "objectID": "posts/회귀진단 실습.html#내적으로-표준화된-잔차-internally-standardized-residual",
    "href": "posts/회귀진단 실습.html#내적으로-표준화된-잔차-internally-standardized-residual",
    "title": "6. 회귀진단 실습",
    "section": "내적으로 표준화된 잔차 ((internally) standardized residual)",
    "text": "내적으로 표준화된 잔차 ((internally) standardized residual)\n\\[r_i = \\dfrac{e_i}{\\hat \\sigma \\sqrt{1-h_{ii}}}\\]\n\ns_residual <- rstandard(model_reg)\nhead(s_residual)\n\n10.1888322174200252-0.9444063949896653-1.462264369447094-0.82158155071330550.8396593902729546-0.0314703908632008\n\n\n\n# 또는\ns_xx <- sum((dt$x-mean(dt$x))^2) #S_xx\nh_ii <- 1/21 + (dt$x- mean(dt$x))^2/s_xx\n### h_ii <- hatvalues(model_reg)\n### h_ii <- influence(model_reg)$hat\nhat_sigma <- summary(model_reg)$sigma #hat sigma\ns_residual <- resid(model_reg)/(hat_sigma*sqrt(1-h_ii)) ## 내적\n\n\nhist(s_residual)"
  },
  {
    "objectID": "posts/회귀진단 실습.html#외적으로-표준화된-잔차-externally-standardized-residual",
    "href": "posts/회귀진단 실습.html#외적으로-표준화된-잔차-externally-standardized-residual",
    "title": "6. 회귀진단 실습",
    "section": "외적으로 표준화된 잔차 ((externally) standardized residual)",
    "text": "외적으로 표준화된 잔차 ((externally) standardized residual)\n\\[r_i^* = \\dfrac{e_i}{\\hat \\sigma_i \\sqrt{1-h_{ii}}}\\]\n\\(\\hat \\sigma_i : i\\)번째 측정값 \\(y_i\\)를 제외하고 얻어진 \\(\\hat \\sigma\\)\n\\(\\hat \\sigma_i^2 = \\left[ (n-p-1) \\hat \\sigma^2 - \\dfrac{e_i^2}{1-h_{ii}} \\right] / (n-p-2)\\)\n\ns_residual_i <- rstudent(model_reg)\nhead(s_residual_i)\n\n10.1839684933793942-0.9415833513782013-1.510811922917994-0.81426336315943850.8328629175207956-0.030631827537088\n\n\n\n# 또는\nhat_sigma_i <- sqrt(((21-1-1)*hat_sigma^2 - residual^2/(1-h_ii) )/(21-1-2))\n## hat_sigma_i <- influence(model_reg)$sigma\ns_residual_i <- residual/(hat_sigma_i*sqrt(1-h_ii)) ## 외적\n\n\nhist(s_residual_i)\n\n\n\n\n\nwhich.max(s_residual_i)\ns_residual_i[which.max(s_residual_i)]\n\n19: 19\n\n\n19: 3.60697972130439\n\n\n\nqt(0.975, 21-1-2)\n\n2.10092204024104\n\n\n\\(|r_i^*| \\geq t_{\\alpha/2}(n-p-2)\\)이면, 유의수준 \\(\\alpha\\)에서, \\(i\\)번째 관측값이 이상점이라고 할 수 있다.\n따라서 19번째 관측값은 유의수준 0.05에서 이상점이라고 할 수 있다.\n\nplot(y~x, dt,pch = 20,cex = 2,col = \"darkorange\")\ntext(dt[19,],\"19\", pos=2)\nabline(model_reg, col='steelblue', lwd=2)\n\n\n\n\n\ns_residual_i[which(abs(s_residual_i)>qt(0.975,21-2))]\n\n19: 3.60697972130439\n\n\n\n## 잔차그림\npar(mfrow = c(2, 2))\nplot(fitted(model_reg), residual,\n pch=20,cex = 2,col = \"darkorange\",\n xlab = \"Fitted\", ylab = \"Residuals\",\n main = \"residual plot\")\nabline(h=0, lty=2)\nplot(fitted(model_reg), s_residual,\n pch=20,cex = 2,col = \"darkorange\",\n xlab = \"Fitted\", ylab = \"S_Residuals\",\n ylim=c(min(-3, min(s_residual)),\n max(3,max(s_residual))),\n main = \"standardized residual plot\")\nabline(h=c(-2,0,2), lty=2)\nplot(fitted(model_reg), s_residual_i,\n pch=20,cex = 2,col = \"darkorange\",\n xlab = \"Fitted\", ylab = \"S_Residuals_(i)\",\n ylim=c(min(-3, min(s_residual_i)),\n max(3,max(s_residual_i))),\n main = \"studentized residual plot\")\nabline(h=c(-3,-2,0,2,3), lty=2)\nplot(fitted(model_reg), s_residual_i,\n pch=20,cex = 2,col = \"darkorange\",\n xlab = \"Fitted\", ylab = \"S_Residuals_(i)\",\n ylim=c(min(-3, min(s_residual_i)),\n max(3,max(s_residual_i))),\n main = \"studentized residual plot\")\nabline(h=c(-qt(0.975,21-2),0,qt(0.975,21-2)), lty=2)\ntext (fitted(model_reg)[which(abs(s_residual_i)>qt(0.975,21-2))],\n s_residual_i[which(abs(s_residual_i)>qt(0.975,21-2))],\n which(abs(s_residual_i)>qt(0.975,21-2)),adj = c(0,1))"
  },
  {
    "objectID": "posts/회귀진단 실습.html#정규성-검정",
    "href": "posts/회귀진단 실습.html#정규성-검정",
    "title": "6. 회귀진단 실습",
    "section": "정규성 검정",
    "text": "정규성 검정\n\n## 정규성 검정\npar(mfrow=c(1,2))\nhist(resid(model_reg),\n xlab = \"Residuals\",\n main = \"Histogram of Residuals\",\n col = \"darkorange\",\n border = \"dodgerblue\",\n breaks = 20)\nqqnorm(resid(model_reg),\n main = \"Normal Q-Q Plot\",\n col = \"darkgrey\",\n pch=16)\nqqline(resid(model_reg), col = \"dodgerblue\", lwd = 2)\n\n\n\n\n\n## Shapiro-Wilk Test\n## H0 : normal distribution vs. H1 : not H0\nshapiro.test(resid(model_reg))\n\n\n    Shapiro-Wilk normality test\n\ndata:  resid(model_reg)\nW = 0.92578, p-value = 0.1133\n\n\n\n## 독립성 검정\nlmtest::dwtest(model_reg)\n\n\n    Durbin-Watson test\n\ndata:  model_reg\nDW = 2.0844, p-value = 0.5716\nalternative hypothesis: true autocorrelation is greater than 0\n\n\n\n### 등분산성\n## H0 : 등분산 vs. H1 : 이분산 (Heteroscedasticity)\nbptest(model_reg)\n\n\n    studentized Breusch-Pagan test\n\ndata:  model_reg\nBP = 0.0014282, df = 1, p-value = 0.9699"
  },
  {
    "objectID": "posts/회귀진단 실습.html#영향점",
    "href": "posts/회귀진단 실습.html#영향점",
    "title": "6. 회귀진단 실습",
    "section": "영향점",
    "text": "영향점\n\nplot(y~x, dt,pch = 20,cex = 2,col = \"darkorange\")\nabline(model_reg, col='steelblue', lwd=2)\n\n\n\n\n\ninfluence(model_reg)\n\n\n\n    $hat\n        10.047922479451021820.15451323429605630.062815775582535340.070545207752054950.047922479451021860.072618957846316370.057989593544981580.056669934394087990.0798582309026469100.0726189578463163110.0907548450343111120.0705452077520549130.0628157755825353140.0566699343940879150.0566699343940879160.0628157755825353170.0521076841867129180.65160998416409190.0530502978659226200.0566699343940879210.0628157755825353\n\n    $coefficients\n        \nA matrix: 21 × 2 of type dbl\n\n    (Intercept)x\n\n\n    1 0.086545033 0.001045618\n    2 0.958749611-0.104156236\n    3-1.623423657 0.057755211\n    4-1.022877601 0.040022565\n    5 0.384830249 0.004649436\n    6 0.005894578-0.001602673\n    7 0.023216186 0.010379001\n    8 0.230329653-0.007159985\n    9 0.410719785-0.017252806\n    10-0.117621441 0.031980023\n    11 1.595051450-0.070799821\n    12-0.437100147 0.017102603\n    13-1.623423657 0.057755211\n    14-1.230320253 0.038245507\n    15 0.412910892-0.012835671\n    16 0.145243868-0.005167222\n    17 0.681955144-0.017203712\n    18 4.243970455-0.347768136\n    19 0.569162106 0.066321856\n    20-1.047739014 0.032569820\n    21 0.145243868-0.005167222\n\n\n\n    $sigma\n        111.3143301900592211.0559573770349310.6687048798294411.1219769595247511.1128596831455611.3246668862836711.2946094952564811.3083981658447911.29861430792641011.20682225901661110.99278346332161211.28816820706241310.66870487982941410.84242194398621511.27164317318071611.31986011811681711.12966417084631811.1067560007425198.628196059920932010.97712792942652111.3198601181168\n\n    $wt.res\n        12.030993137772482-9.572128798733133-15.60395143654324-8.7309403514063859.030993137772416-0.33406228791192573.4119598823618182.5230374783198893.14207073373048106.665937712088081111.015081818867412-3.7309403514063813-15.603951436543214-13.4769625216801154.52303747831988161.39604856345675178.6500263931830218-5.540306160923011930.284970967498720-11.4769625216801211.39604856345675\n\n\n\n\n\ninfluence.measures(model_reg)\n\nInfluence measures of\n     lm(formula = y ~ x, data = dt) :\n\n     dfb.1_    dfb.x    dffit cov.r   cook.d    hat inf\n1   0.01664  0.00328  0.04127 1.166 8.97e-04 0.0479    \n2   0.18862 -0.33480 -0.40252 1.197 8.15e-02 0.1545    \n3  -0.33098  0.19239 -0.39114 0.936 7.17e-02 0.0628    \n4  -0.20004  0.12788 -0.22433 1.115 2.56e-02 0.0705    \n5   0.07532  0.01487  0.18686 1.085 1.77e-02 0.0479    \n6   0.00113 -0.00503 -0.00857 1.201 3.88e-05 0.0726    \n7   0.00447  0.03266  0.07722 1.170 3.13e-03 0.0580    \n8   0.04430 -0.02250  0.05630 1.174 1.67e-03 0.0567    \n9   0.07907 -0.05427  0.08541 1.200 3.83e-03 0.0799    \n10 -0.02283  0.10141  0.17284 1.152 1.54e-02 0.0726    \n11  0.31560 -0.22889  0.33200 1.088 5.48e-02 0.0908    \n12 -0.08422  0.05384 -0.09445 1.183 4.68e-03 0.0705    \n13 -0.33098  0.19239 -0.39114 0.936 7.17e-02 0.0628    \n14 -0.24681  0.12536 -0.31367 0.992 4.76e-02 0.0567    \n15  0.07968 -0.04047  0.10126 1.159 5.36e-03 0.0567    \n16  0.02791 -0.01622  0.03298 1.187 5.74e-04 0.0628    \n17  0.13328 -0.05493  0.18717 1.096 1.79e-02 0.0521    \n18  0.83112 -1.11275 -1.15578 2.959 6.78e-01 0.6516   *\n19  0.14348  0.27317  0.85374 0.396 2.23e-01 0.0531   *\n20 -0.20761  0.10544 -0.26385 1.043 3.45e-02 0.0567    \n21  0.02791 -0.01622  0.03298 1.187 5.74e-04 0.0628    \n\n\n\nDFFITS: \\(DFFITS(i) = \\dfrac{\\hat y_i - \\tilde y_i(i)}{\\hat \\sigma_{(i)} \\sqrt{h_{ii}}}\\)\n\\(|DFFITS(i)| \\geq 2 \\sqrt{\\dfrac{p+1}{n-p-1}}\\)이면 영향점\n\n\ndffits(model_reg) \n\n10.04127403575140562-0.4025206873025253-0.3911400454742154-0.22432853366080450.1868559838824216-0.0085717364067812270.077223952838937980.056303486522047690.085407472693718100.172840518129759110.33199685399425312-0.094449643042361813-0.39114004547421514-0.313673908094842150.101264129345836160.0329813827461469170.18716612805440518-1.15577873097521190.85373710713076620-0.263846244162542210.0329813827461469\n\n\n\nwhich(abs(dffits(model_reg)) > 2*sqrt(2/(21-2)))\n\n18181919\n\n\n\nCook’s Distance\n\\(D(i) = \\dfrac{\\sum_{i=1}^n (\\hat y_j - \\hat y_j(i))^2}{(p+1)\\hat \\sigma^2}=\\dfrac{(\\hat \\beta - \\hat \\beta(i))^T X^T X (\\hat \\beta - \\hat \\beta(i))}{(p+1) \\hat \\sigma^2}\\)\n\\(\\hat \\beta(i): i\\)번째 관측치를 제외하고 \\(n-1\\)개의 관측값에서 구한 \\(\\hat \\beta\\)의 최소제곱추저량\n\n\ncooks.distance(model_reg)\n\n10.00089740639287069120.081497955150763530.071658144221383340.025615958245264150.017743662633501363.87762740910137e-0570.003130574802994980.0016682085781346990.00383194880672965100.0154395158127621110.0548101351203612120.00467762256482442130.0716581442213833140.0475978118328145150.00536121617564154160.000573584529113046170.017856495213809180.678112028575845190.223288273631179200.0345188940892692210.000573584529113046\n\n\n\n\\(D(i) \\geq F_{0.5}(p+1, n-p-1)\\)이면 영향점으로 의심\n\n\nqf(0.5,2,21-2)\n\n0.719060569091733\n\n\n\nwhich(cooks.distance(model_reg) >qf(0.5,2,21-2))\n\n\n\n\n\nCOVRATIO\n\n\\(COVRATIO(i) = \\dfrac{1}{\\left[1+\\dfrac{(r_i^*)^2-1}{n-p-1}\\right]^{p+1}(1-h_{ii})}\\)\n\\(|COVRATIO(i)-1| \\geq 3(p+1)/n\\)이면 \\(i\\)번째 관측치를 영향을 크게 주는 측정값으로 볼 수 있음\n\ncovratio(model_reg)\n\n11.1658918168321921.1969989767629630.93634739734183941.1151026899392951.0850410825772861.2013199827549771.1701575789867381.1742372676080391.19966823450598101.15209128858604111.08783960928084121.18326164825873130.936347397341839140.992331347870996151.15904532932769161.18673688685713171.09643883044992182.95868271380702190.396431612340971201.04257281407241211.18673688685713\n\n\n\nwhich(abs(covratio(model_reg)-1) > 3*(1+1)/21)\n\n18181919\n\n\n\n영향점\n\n\nsummary(influence.measures(model_reg))\n\nPotentially influential observations of\n     lm(formula = y ~ x, data = dt) :\n\n   dfb.1_ dfb.x   dffit   cov.r   cook.d hat    \n18  0.83  -1.11_* -1.16_*  2.96_*  0.68   0.65_*\n19  0.14   0.27    0.85    0.40_*  0.22   0.05  \n\n\n\n## 18제거 전후\nplot(y~x, dt,pch = 20,\n cex = 2,col = \"darkorange\",\n main = \"18번 제거\")\nabline(model_reg, col='steelblue', lwd=2)\nabline(lm(y~x, dt[-18,]), col='red', lwd=2)\ntext(dt[18,], pos=2, \"18\")\nlegend('topright', legend=c(\"full\", \"del(18)\"),\n col=c('steelblue', 'red'), lty=1, lwd=2)\n# high leverage and high influence, not outlier\n\n\n\n\n\n## 19제거 전후\nplot(y~x, dt,pch = 20,\n cex = 2,col = \"darkorange\",\n main = \"19번 제거\")\nabline(model_reg, col='steelblue', lwd=2)\nabline(lm(y~x, dt[-19,]), col='red', lwd=2)\ntext(dt[19,], pos=2, \"19\")\nlegend('topright', legend=c(\"full\", \"del(19)\"),\n col=c('steelblue', 'red'), lty=1, lwd=2)\n# not leverage and high influence, outlier\n\n\n\n\n\n## 18, 19제거 전후\nplot(y~x, dt,pch = 20,\n cex = 2,col = \"darkorange\",\n main = \"18,19번 제거\")\nabline(model_reg, col='steelblue', lwd=2)\nabline(lm(y~x, dt[-c(18,19),]), col='red', lwd=2)\ntext(dt[c(18,19),], pos=2, c(\"18\",\"19\"))\nlegend('topright', legend=c(\"full\", \"del(18,19)\"),\n col=c('steelblue', 'red'), lty=1, lwd=2)\n\n\n\n\n\n## 회귀진단 그림\npar(mfrow = c(2, 2))\nplot(model_reg, pch=16)"
  },
  {
    "objectID": "posts/graph4-3.html",
    "href": "posts/graph4-3.html",
    "title": "CH4. 지도 그래프 학습(그래프 정규화 방법)",
    "section": "",
    "text": "그래프 머신러닝\ngithub"
  },
  {
    "objectID": "posts/graph4-3.html#load-dataset",
    "href": "posts/graph4-3.html#load-dataset",
    "title": "CH4. 지도 그래프 학습(그래프 정규화 방법)",
    "section": "Load Dataset",
    "text": "Load Dataset\n- 데이터셋: Cora\n\n7개의 클래스로 라벨링돼 있는 2,708개의 컴퓨터 사이언스 논문\n각 논문은 인용을 기반으로 다른 노드와 연결된 노드\n총 5,429개의 간선\n\n\nfrom stellargraph import datasets\n\n2023-04-06 21:44:50.486139: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n\n\n\ndataset = datasets.Cora()\n\n\n%config Completer.use_jedi = False\n\n\ndataset.download()\n\n\nlabel_index = {\n      'Case_Based': 0,\n      'Genetic_Algorithms': 1,\n      'Neural_Networks': 2,\n      'Probabilistic_Methods': 3,\n      'Reinforcement_Learning': 4,\n      'Rule_Learning': 5,\n      'Theory': 6,\n  }\n\n\nG, labels = dataset.load()\n\n\nG: 네트워크 노드, 간선, BOW표현 설명\nlabea : 논문id와 클래스 중 하나 사이의 매핑\n훈련 샘플: 이웃과 관련된 정보가 포함 -> 훈련을 정규화 하는데 사용\n검증 샘플: 이웃과 관련된 정보 불포함 , 예측된 라벨은 노드 특증, bow표현에만 의존\n\n\nimport numpy as np\nfrom sklearn import preprocessing, feature_extraction, model_selection\n\n\nimport tensorflow as tf\nfrom tensorflow.train import Example, Features, Feature, Int64List, BytesList, FloatList\n\n\nGRAPH_PREFIX=\"NL_nbr\"\n\n\ndef _int64_feature(*value):\n    \"\"\"Returns int64 tf.train.Feature from a bool / enum / int / uint.\"\"\"\n    return Feature(int64_list=Int64List(value=list(value)))\n\ndef _bytes_feature(value):\n    \"\"\"Returns bytes tf.train.Feature from a string.\"\"\"\n    return Feature(\n        bytes_list=BytesList(value=[value.encode('utf-8')])\n    )\n\ndef _float_feature(*value):\n    return Feature(float_list=FloatList(value=list(value)))\n\n\n_int64_feature 함수는 bool, enum, int, uint 데이터 타입을 입력 받아 int64_list 타입의 tf.train.Feature 객체를 반환\n_bytes_feature 함수는 문자열 값을 입력 받아 utf-8로 인코딩하여 bytes_list 타입의 tf.train.Feature 객체를 반환\n_float_feature 함수는 float 데이터 타입을 입력 받아 float_list 타입의 tf.train.Feature 객체를 반환\n\n- 반지도 학습 데이터 셋 만드는 함수 정의\n\nfrom functools import reduce\nfrom typing import List, Tuple\nimport pandas as pd\nimport six\n\ndef addFeatures(x, y):\n    res = Features()\n    res.CopyFrom(x)\n    res.MergeFrom(y)\n    return res\n\ndef neighborFeatures(features: Features, weight: float, prefix: str):  # 객체, 가중치, 접두어 입력으로 받음\n    data = {f\"{prefix}_weight\": _float_feature(weight)}\n    for name, feature in six.iteritems(features.feature):\n        data[f\"{prefix}_{name}\"] = feature \n    return Features(feature=data)\n\ndef neighborsFeatures(neighbors: List[Tuple[Features, float]]):\n    return reduce(\n        addFeatures, \n        [neighborFeatures(sample, weight, f\"{GRAPH_PREFIX}_{ith}\") for ith, (sample, weight) in enumerate(neighbors)],\n        Features()\n    )\n\ndef getNeighbors(idx, adjMatrix, topn=5): #인덱스와 인접행렬 이용하여 이웃 데이터셋 추출 \n    weights = adjMatrix.loc[idx]\n    return weights[weights>0].sort_values(ascending=False).head(topn).to_dict()\n    \n\ndef semisupervisedDataset(G, labels, ratio=0.2, topn=5):  #라벨이 있는 데이터와 없는 데이터 추출\n     #ratio:라벨 유무 비율 설정\n     #topn: 함수에서 추출할 이웃 데이터셋 크기 설정\n    n = int(np.round(len(labels)*ratio)) \n    \n    labelled, unlabelled = model_selection.train_test_split(\n        labels, train_size=n, test_size=None, stratify=labels\n    )\n\n\n1. 노드 특징 df로 구성하고 그래프 인접행렬로 저장\n\nadjMatrix = pd.DataFrame.sparse.from_spmatrix(G.to_adjacency_matrix(), index=G.nodes(), columns=G.nodes())\n    \nfeatures = pd.DataFrame(G.node_features(), index=G.nodes())\n\n\n\n2. adjMatrix사용해 노드ID와 간선 가중치 반환하여 노드의 가장 가까운 TOPN이웃 검색하는 도우미 함수 구현\n\ndef getNeighbors(idx, adjMatrix, topn=5): #인덱스와 인접행렬 이용하여 이웃 데이터셋 추출 \n    weights = adjMatrix.loc[idx]\n    neighbors = weights[weights>0]\\\n        .sort_values(ascending=False)\\\n        .head(topn)\n    return [(k,v) for k, v in neighbors.iteritems()]\n    \n\n\n3. 정보를 단일 df로 병합\n\ndataset = {\n        index: Features(feature = {\n            #\"id\": _bytes_feature(str(index)), \n            \"id\": _int64_feature(index),\n            \"words\": _float_feature(*[float(x) for x in features.loc[index].values]), \n            \"label\": _int64_feature(label_index[label])\n        })\n        for index, label in pd.concat([labelled, unlabelled]).items()\n    }\n\nNameError: name 'labelled' is not defined\n\n\n\nfrom functools import reduce\nfrom typing import List, Tuple\nimport pandas as pd\nimport six\n\ndef addFeatures(x, y):\n    res = Features()\n    res.CopyFrom(x)\n    res.MergeFrom(y)\n    return res\n\ndef neighborFeatures(features: Features, weight: float, prefix: str):\n    data = {f\"{prefix}_weight\": _float_feature(weight)}\n    for name, feature in six.iteritems(features.feature):\n        data[f\"{prefix}_{name}\"] = feature \n    return Features(feature=data)\n\ndef neighborsFeatures(neighbors: List[Tuple[Features, float]]):\n    return reduce(\n        addFeatures, \n        [neighborFeatures(sample, weight, f\"{GRAPH_PREFIX}_{ith}\") for ith, (sample, weight) in enumerate(neighbors)],\n        Features()\n    )\n\ndef getNeighbors(idx, adjMatrix, topn=5):\n    weights = adjMatrix.loc[idx]\n    return weights[weights>0].sort_values(ascending=False).head(topn).to_dict()\n    \n\ndef semisupervisedDataset(G, labels, ratio=0.2, topn=5):\n    n = int(np.round(len(labels)*ratio))\n    \n    labelled, unlabelled = model_selection.train_test_split(\n        labels, train_size=n, test_size=None, stratify=labels\n    )\n    \n    adjMatrix = pd.DataFrame.sparse.from_spmatrix(G.to_adjacency_matrix(), index=G.nodes(), columns=G.nodes())\n    \n    features = pd.DataFrame(G.node_features(), index=G.nodes())\n    \n    dataset = {\n        index: Features(feature = {\n            #\"id\": _bytes_feature(str(index)), \n            \"id\": _int64_feature(index),\n            \"words\": _float_feature(*[float(x) for x in features.loc[index].values]), \n            \"label\": _int64_feature(label_index[label])\n        })\n        for index, label in pd.concat([labelled, unlabelled]).items()\n    }\n    \n    trainingSet = [\n        Example(features=addFeatures(\n            dataset[exampleId], \n            neighborsFeatures(\n                [(dataset[nodeId], weight) for nodeId, weight in getNeighbors(exampleId, adjMatrix, topn).items()]\n            )\n        ))\n        for exampleId in labelled.index\n    ]\n    \n    testSet = [Example(features=dataset[exampleId]) for exampleId in unlabelled.index]\n\n    serializer = lambda _list: [e.SerializeToString() for e in _list]\n    \n    return serializer(trainingSet), serializer(testSet)"
  },
  {
    "objectID": "posts/학회.html",
    "href": "posts/학회.html",
    "title": "Scribbling",
    "section": "",
    "text": "일정:23. 6.29(목) ~ 7.1 (토)\n장소: 부경대학교(부산)\n발표신청 및 초록제출: 3.20.(월) ~ 4.20.(목)\n발표요약본제출(석사과정) : ~4.20.(목)\n포스터파일제출: ~ 5.19.(금)\n\n\n\n\n\n하계: 2023. 7.6.(목) ~ 7.7.(금)\n고려대학교\n발표신청 및 사전등록: 23.5.29.(월)\n초록 또는 논문제출: ~5.31.(수)\n\n\n\n\n\n일정: 6.23.(금) ~ 6.24.(토)\n장소: 강릉원주대학교\n발표논문 초록 제출: ~5.5.(금)\n발표 논문 제출: ~6.16.(금)\n학생논문: ~6.9.(금)"
  },
  {
    "objectID": "posts/graph8(frac=0.4).html",
    "href": "posts/graph8(frac=0.4).html",
    "title": "CH8. 신용카드 거래에 대한 그래프 분석(frac=0.4)",
    "section": "",
    "text": "그래프 머신러닝\ngithub\nCredit Card Transactions Fraud Detection Dataset\n컬리이미지\nnetworkx"
  },
  {
    "objectID": "posts/graph8(frac=0.4).html#사기-탐지를-위한-지도-및-비지도-임베딩",
    "href": "posts/graph8(frac=0.4).html#사기-탐지를-위한-지도-및-비지도-임베딩",
    "title": "CH8. 신용카드 거래에 대한 그래프 분석(frac=0.4)",
    "section": "사기 탐지를 위한 지도 및 비지도 임베딩",
    "text": "사기 탐지를 위한 지도 및 비지도 임베딩\n\n트랜잭션 간선으로 표기\n각 간선을 올바른 클래스(사기 또는 정상)으로 분류\n\n\n지도학습\n\nfrom sklearn.utils import resample\n\ndf_majority = df[df.is_fraud==0]\ndf_minority = df[df.is_fraud==1]\n\ndf_maj_dowsampled = resample(df_majority,\n                             n_samples=len(df_minority),\n                             random_state=42)\n\ndf_downsampled = pd.concat([df_minority, df_maj_dowsampled])\n\nprint(df_downsampled.is_fraud.value_counts())\nG_down = build_graph_bipartite(df_downsampled)\n\n1    6006\n0    6006\nName: is_fraud, dtype: int64\n\n\n\n무작위 언더샘플링 사용\n소수 클래스(사기거래)이 샘플 수 와 일치시키려고 다수 클래스(정상거래)의 하위 샘플을 가져옴\n데이터 불균형을 처리하기 위해서\n\n\nfrom sklearn.model_selection import train_test_split\n\n\ntrain_edges, test_edges, train_labels, test_labels = train_test_split(list(range(len(G_down.edges))), \n                                                                      list(nx.get_edge_attributes(G_down, \"label\").values()), \n                                                                      test_size=0.20, \n                                                                      random_state=42)\n\n\nedgs = list(G_down.edges)\ntrain_graph = G_down.edge_subgraph([edgs[x] for x in train_edges]).copy()\ntrain_graph.add_nodes_from(list(set(G_down.nodes) - set(train_graph.nodes)))\n\n\n데이터 8:2 비율로 학습 검증\n\n\npip install node2vec\n\nCollecting node2vec\n  Downloading node2vec-0.4.6-py3-none-any.whl (7.0 kB)\nRequirement already satisfied: joblib<2.0.0,>=1.1.0 in /home/coco/anaconda3/envs/py38/lib/python3.8/site-packages (from node2vec) (1.2.0)\nCollecting gensim<5.0.0,>=4.1.2\n  Downloading gensim-4.3.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.5 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 26.5/26.5 MB 71.8 MB/s eta 0:00:0000:0100:01\nCollecting tqdm<5.0.0,>=4.55.1\n  Downloading tqdm-4.65.0-py3-none-any.whl (77 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 77.1/77.1 kB 18.6 MB/s eta 0:00:00\nCollecting networkx<3.0,>=2.5\n  Downloading networkx-2.8.8-py3-none-any.whl (2.0 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.0/2.0 MB 90.4 MB/s eta 0:00:00\nRequirement already satisfied: numpy<2.0.0,>=1.19.5 in /home/coco/anaconda3/envs/py38/lib/python3.8/site-packages (from node2vec) (1.24.2)\nRequirement already satisfied: scipy>=1.7.0 in /home/coco/anaconda3/envs/py38/lib/python3.8/site-packages (from gensim<5.0.0,>=4.1.2->node2vec) (1.10.1)\nCollecting smart-open>=1.8.1\n  Downloading smart_open-6.3.0-py3-none-any.whl (56 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 56.8/56.8 kB 13.6 MB/s eta 0:00:00\nInstalling collected packages: tqdm, smart-open, networkx, gensim, node2vec\n  Attempting uninstall: networkx\n    Found existing installation: networkx 3.0\n    Uninstalling networkx-3.0:\n      Successfully uninstalled networkx-3.0\nSuccessfully installed gensim-4.3.1 networkx-2.8.8 node2vec-0.4.6 smart-open-6.3.0 tqdm-4.65.0\nNote: you may need to restart the kernel to use updated packages.\n\n\n\nfrom node2vec import Node2Vec\nfrom node2vec.edges import HadamardEmbedder, AverageEmbedder, WeightedL1Embedder, WeightedL2Embedder\n\nnode2vec_train = Node2Vec(train_graph, weight_key='weight')\nmodel_train = node2vec_train.fit(window=10)\n\n\n\n\nGenerating walks (CPU: 1): 100%|██████████| 10/10 [00:03<00:00,  2.62it/s]\n\n\n\nNode2Vec 알고리즘 사용해 특징 공간 구축\n\n\nfrom sklearn.ensemble import RandomForestClassifier \nfrom sklearn import metrics \n\nclasses = [HadamardEmbedder, AverageEmbedder, WeightedL1Embedder, WeightedL2Embedder]\nfor cl in classes:\n    embeddings_train = cl(keyed_vectors=model_train.wv) \n    # 벡터스페이스 상에 edge를 투영.. \n\n    train_embeddings = [embeddings_train[str(edgs[x][0]), str(edgs[x][1])] for x in train_edges]\n    test_embeddings = [embeddings_train[str(edgs[x][0]), str(edgs[x][1])] for x in test_edges]\n    \n    rf = RandomForestClassifier(n_estimators=1000, random_state=42) \n    rf.fit(train_embeddings, train_labels); \n\n    y_pred = rf.predict(test_embeddings)\n    print(cl)\n    print('Precision:', metrics.precision_score(test_labels, y_pred)) \n    print('Recall:', metrics.recall_score(test_labels, y_pred)) \n    print('F1-Score:', metrics.f1_score(test_labels, y_pred)) \n\n<class 'node2vec.edges.HadamardEmbedder'>\nPrecision: 0.7451737451737451\nRecall: 0.16538131962296487\nF1-Score: 0.270687237026648\n<class 'node2vec.edges.AverageEmbedder'>\nPrecision: 0.691699604743083\nRecall: 0.7497857754927164\nF1-Score: 0.7195723684210527\n<class 'node2vec.edges.WeightedL1Embedder'>\nPrecision: 0.7142857142857143\nRecall: 0.029991431019708654\nF1-Score: 0.0575657894736842\n<class 'node2vec.edges.WeightedL2Embedder'>\nPrecision: 0.64\nRecall: 0.027420736932305057\nF1-Score: 0.052588331963845526\n\n\n\nNode2Vec 알고리즘 사용해 각 Edge2Vec 알고리즘으로 특징 공간 생성\nsklearn 파이썬 라이브러리의 RandomForestClassifier은 이전 단계에서 생성한 특징에 대해 학습\n검증 테스트 위해 정밀도, 재현율, F1-score 성능 지표 측정\n\n\n\n비지도학습\n\nk-means 알고리즘 사용\n지도학습과의 차이점은 특징 공간이 학습-검증 분할을 안함.\n\n\nnod2vec_unsup = Node2Vec(G_down, weight_key='weight')\nunsup_vals = nod2vec_unsup.fit(window=10)\n\n\n\n\nGenerating walks (CPU: 1): 100%|██████████| 10/10 [00:04<00:00,  2.32it/s]\n\n\n\n다운샘플링 절차에 전체 그래프 알고리즘 계산\n\n\nfrom sklearn.cluster import KMeans\n\nclasses = [HadamardEmbedder, AverageEmbedder, WeightedL1Embedder, WeightedL2Embedder]\ntrue_labels = [x for x in nx.get_edge_attributes(G_down, \"label\").values()]\n\nfor cl in classes:\n    embedding_edge = cl(keyed_vectors=unsup_vals.wv) \n\n    embedding = [embedding_edge[str(x[0]), str(x[1])] for x in G_down.edges()]\n    kmeans = KMeans(2, random_state=42).fit(embedding)\n    \n    \n    nmi = metrics.adjusted_mutual_info_score(true_labels, kmeans.labels_)\n    ho = metrics.homogeneity_score(true_labels, kmeans.labels_)\n    co = metrics.completeness_score(true_labels, kmeans.labels_)\n    vmeasure = metrics.v_measure_score(true_labels, kmeans.labels_)\n    \n    print(cl)\n    print('NMI:', nmi)\n    print('Homogeneity:', ho)\n    print('Completeness:', co)\n    print('V-Measure:', vmeasure)\n\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n\n\n<class 'node2vec.edges.HadamardEmbedder'>\nNMI: 0.04336246478827236\nHomogeneity: 0.0383178531539123\nCompleteness: 0.05011351404941123\nV-Measure: 0.043428985282038965\n\n\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n\n\n<class 'node2vec.edges.AverageEmbedder'>\nNMI: 0.11206093720015026\nHomogeneity: 0.10817496918905492\nCompleteness: 0.11635805522328385\nV-Measure: 0.11211739628609738\n\n\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n\n\n<class 'node2vec.edges.WeightedL1Embedder'>\nNMI: 0.16558117117175825\nHomogeneity: 0.16557714823761863\nCompleteness: 0.16568764408717976\nV-Measure: 0.16563237773404058\n\n\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n\n\n<class 'node2vec.edges.WeightedL2Embedder'>\nNMI: 0.1349652677966787\nHomogeneity: 0.1337881599748603\nCompleteness: 0.1362723387302234\nV-Measure: 0.13501882386803338\n\n\n- NMI(Normalized Mutual Information)\n\n두 개의 군집 결과 비교\n0~1이며 1에 가까울수록 높은 성능\n\n- Homogeneity\n\n하나의 실제 군집 내에서 같은 군집에 속한 샘플들이 군집화 결과에서 같은 군집에 속할 비율\n1에 가까울수록 높은 성능\n\n- Completeness\n\n하나의 예측 군집 내에서 같은 실제 군집에 속한 샘플들이 군집화 결과에서 같은 군집에 속할 비율\n0~1이며 1에 가까울수록 높은 성능\n\n- V-measure\n\nHomogeneity와 Completeness의 조화 평균\n0~1이며 1에 가까울수록 높은 성능\n비지도 학습에 이상치 탐지 방법\nk-means/LOF/One-class SVM 등이 있다.. 한번 같이 해보자.\n조금씩 다 커졌넹..\n\n- 지도학습에서 정상거래에서 다운샘플링을 했는데\n만약, 사기거래에서 업샘플링을 하게되면 어떻게 될까?"
  },
  {
    "objectID": "posts/graph8(frac=0.3).html",
    "href": "posts/graph8(frac=0.3).html",
    "title": "CH8. 신용카드 거래에 대한 그래프 분석(frac=0.3)",
    "section": "",
    "text": "그래프 머신러닝\ngithub\nCredit Card Transactions Fraud Detection Dataset\n컬리이미지\nnetworkx"
  },
  {
    "objectID": "posts/graph8(frac=0.3).html#네트워크-토폴로지",
    "href": "posts/graph8(frac=0.3).html#네트워크-토폴로지",
    "title": "CH8. 신용카드 거래에 대한 그래프 분석(frac=0.3)",
    "section": "네트워크 토폴로지",
    "text": "네트워크 토폴로지\n\n각 그래프별 차수 분포 살펴보기\n\n\nfor G in [G_bu, G_tu]:\n    plt.figure(figsize=(10,10))\n    degrees = pd.Series({k:v for k, v in nx.degree(G)})\n    degrees.plot.hist()\n    plt.yscale(\"log\")\n\n\n\n\n\n\n\n\nx축: 노드의 연결도\ny축: 로그 스케일(연결도가 큰 노드의 수가 매우 적으므로)\n\n- 각 그래프 간선 가중치 분포\n\nfor G in [G_bu, G_tu]:\n    allEdgeWeights = pd.Series({\n        (d[0],d[1]):d[2][\"weight\"]  #d[0],d[1]을 key로 d[2]를 weight로\n        #d는 G.edges(data=True)로 (u,v,data)형태의 튜플을 반복하는 반복문\n        for d in G.edges(data=True)})\n    np.quantile(allEdgeWeights.values,\n               [0.10, 0.50, 0.70, 0.9])\n    \n\n\nnp.quantile(allEdgeWeights.values,[0.10, 0.50, 0.70, 0.9])\n\narray([  4.15,  48.01,  75.75, 141.91])\n\n\n- 매게 중심성 측정 지표\n\nfor G in [G_bu, G_tu]:\n    plt.figure(figsize=(10,10))\n    bc_distr = pd.Series(nx.betweenness_centrality(G))\n    bc_distr.plot.hist()\n    plt.yscale(\"log\")\n\nKeyboardInterrupt: \n\n\n<Figure size 1000x1000 with 0 Axes>\n\n\n\n그래프 내에서 노드가 얼마나 중심적인 역할을 하는지 나타내는 지표\n해당 노드가 얼마나 많은 최단경로에 포함되는지 살피기\n노드가 많은 최단경로를 포함하면 해당노드의 매개중심성은 커진다.\n\n- 상관계수\n\nfor G in [G_bu, G_tu]:\n    print(nx.degree_pearson_correlation_coefficient(G))\n\n-0.12467174727090688\n-0.8051895351325623\n\n\n\n음의 동류성(서로 다른 속성을 가진 노드들끼리 연결되어 있다.)\n0~ -1 사이의 값을 가짐\n-1에 가까울수록 서로 다른 속성을 가진 노드들끼리 강한 음의 상관관계\n0에 가까울수록 노드들이 연결될 때 서로 다른 속성을 가진 노드들끼리 큰 차이가 없음 =>\n연결도 높은 개인이 연골도 낮은 개인과 연관돼 있다.\n이분그래프: 낮은 차수의 고객은 들어오는 트랜잭션 수가 많은 높은 차수의 판매자와만 연결되어 상관계수가 낮다.\n삼분그래프:동류성이 훨씬 더 낮다. 트랜잭션 노드가 있기 댸문에?"
  },
  {
    "objectID": "posts/graph8(frac=0.3).html#커뮤니티-감지",
    "href": "posts/graph8(frac=0.3).html#커뮤니티-감지",
    "title": "CH8. 신용카드 거래에 대한 그래프 분석(frac=0.3)",
    "section": "커뮤니티 감지",
    "text": "커뮤니티 감지\n\n# pip install python-louvain\n\n\nimport networkx as nx\nimport community\n\n\nimport community\nfor G in [G_bu, G_tu]:\n    parts = community.best_partition(G, random_state=42, weight='weight')\n\n\ncommunities = pd.Series(parts)\n\n\ncommunities\n\n255288    72\n204367    72\n65143     92\n10004     23\n194072     3\n          ..\n286119    78\n194740    88\n53644     57\n300283     9\n313041    66\nLength: 320413, dtype: int64\n\n\n\nprint(communities.value_counts().sort_values(ascending=False))\n\n4      9426\n94     6025\n6      5835\n42     5636\n50     5016\n       ... \n112    1341\n91     1307\n18     1104\n62     1057\n85      585\nLength: 113, dtype: int64\n\n\n\n커뮤니티 종류가 늘었따. 96>>113개로\n커뮤니티 감지를 통해 특정 사기 패턴 식별\n커뮤니티 추출 후 포함된 노드 수에 따라 정렬\n\n\ncommunities.value_counts().plot.hist(bins=20)\n\n<Axes: ylabel='Frequency'>\n\n\n\n\n\n\n9426개 이상한거 하나있고.. 약간 2000~3000사이에 집중되어 보인다.\n\n\ngraphs = [] # 부분그래프 저장\nd = {}  # 부정 거래 비율 저장 \nfor x in communities.unique():\n    tmp = nx.subgraph(G, communities[communities==x].index)\n    fraud_edges = sum(nx.get_edge_attributes(tmp, \"label\").values())\n    ratio = 0 if fraud_edges == 0 else (fraud_edges/tmp.number_of_edges())*100\n    d[x] = ratio\n    graphs += [tmp]\n\npd.Series(d).sort_values(ascending=False)\n\n56     5.281326\n59     4.709632\n111    4.399142\n77     4.149798\n15     3.975843\n         ...   \n90     0.409650\n112    0.297398\n110    0.292826\n67     0.277008\n18     0.180180\nLength: 113, dtype: float64\n\n\n\n사기 거래 비율 계산. 사기 거래가 집중된 특정 하위 그래프 식별\n특정 커뮤니티에 포함된 노드를 사용하여 노드 유도 하위 그래프 생성\n하위 그래프: 모든 간선 수에 대한 사기 거래 간선 수의 비율로 사기 거래 백분율 계싼\n\n\ngId = 10\nplt.figure(figsize=(10,10))\nspring_pos = nx.spring_layout(graphs[gId])\nplt.axis(\"off\")\nedge_colors = [\"r\" if x == 1 else \"g\" for x in nx.get_edge_attributes(graphs[gId], 'label').values()]  #r:빨간색, g:녹색\nnx.draw_networkx(graphs[gId], pos=spring_pos, node_color=default_node_color, \n                 edge_color=edge_colors, with_labels=False, node_size=15)\n\n\n\n\n\n커뮤니티 감지 알고리즘에 의해 감지된 노드 유도 하위 그래프 그리기\n특정 커뮤니티 인덱스 gId가 주어지면 해당 커뮤니티에서 사용 가능한 노드로 유도 하위 그래프 추출하고 얻는다.\n\n\ngId = 56\nplt.figure(figsize=(10,10))\nspring_pos = nx.spring_layout(graphs[gId])\nplt.axis(\"off\")\nedge_colors = [\"r\" if x == 1 else \"g\" for x in nx.get_edge_attributes(graphs[gId], 'label').values()]  #r:빨간색, g:녹색\nnx.draw_networkx(graphs[gId], pos=spring_pos, node_color=default_node_color, \n                 edge_color=edge_colors, with_labels=False, node_size=15)\n\n\n\n\n\ngId = 18\nplt.figure(figsize=(10,10))\nspring_pos = nx.spring_layout(graphs[gId])\nplt.axis(\"off\")\nedge_colors = [\"r\" if x == 1 else \"g\" for x in nx.get_edge_attributes(graphs[gId], 'label').values()]  #r:빨간색, g:녹색\nnx.draw_networkx(graphs[gId], pos=spring_pos, node_color=default_node_color, \n                 edge_color=edge_colors, with_labels=False, node_size=15)\n\n\n\n\n\npd.Series(d).plot.hist(bins=20)\n\n<Axes: ylabel='Frequency'>"
  },
  {
    "objectID": "posts/graph8(frac=0.3).html#사기-탐지를-위한-지도-및-비지도-임베딩",
    "href": "posts/graph8(frac=0.3).html#사기-탐지를-위한-지도-및-비지도-임베딩",
    "title": "CH8. 신용카드 거래에 대한 그래프 분석(frac=0.3)",
    "section": "사기 탐지를 위한 지도 및 비지도 임베딩",
    "text": "사기 탐지를 위한 지도 및 비지도 임베딩\n\n트랜잭션 간선으로 표기\n각 간선을 올바른 클래스(사기 또는 정상)으로 분류\n\n\n지도학습\n\nfrom sklearn.utils import resample\n\ndf_majority = df[df.is_fraud==0]\ndf_minority = df[df.is_fraud==1]\n\ndf_maj_dowsampled = resample(df_majority,\n                             n_samples=len(df_minority),\n                             random_state=42)\n\ndf_downsampled = pd.concat([df_minority, df_maj_dowsampled])\n\nprint(df_downsampled.is_fraud.value_counts())\nG_down = build_graph_bipartite(df_downsampled)\n\n1    6006\n0    6006\nName: is_fraud, dtype: int64\n\n\n\n무작위 언더샘플링 사용\n소수 클래스(사기거래)이 샘플 수 와 일치시키려고 다수 클래스(정상거래)의 하위 샘플을 가져옴\n데이터 불균형을 처리하기 위해서\n\n\nfrom sklearn.model_selection import train_test_split\n\n\ntrain_edges, test_edges, train_labels, test_labels = train_test_split(list(range(len(G_down.edges))), \n                                                                      list(nx.get_edge_attributes(G_down, \"label\").values()), \n                                                                      test_size=0.20, \n                                                                      random_state=42)\n\n\nedgs = list(G_down.edges)\ntrain_graph = G_down.edge_subgraph([edgs[x] for x in train_edges]).copy()\ntrain_graph.add_nodes_from(list(set(G_down.nodes) - set(train_graph.nodes)))\n\n\n데이터 8:2 비율로 학습 검증\n\n\npip install node2vec\n\nCollecting node2vec\n  Downloading node2vec-0.4.6-py3-none-any.whl (7.0 kB)\nRequirement already satisfied: joblib<2.0.0,>=1.1.0 in /home/coco/anaconda3/envs/py38/lib/python3.8/site-packages (from node2vec) (1.2.0)\nCollecting gensim<5.0.0,>=4.1.2\n  Downloading gensim-4.3.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.5 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 26.5/26.5 MB 71.8 MB/s eta 0:00:0000:0100:01\nCollecting tqdm<5.0.0,>=4.55.1\n  Downloading tqdm-4.65.0-py3-none-any.whl (77 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 77.1/77.1 kB 18.6 MB/s eta 0:00:00\nCollecting networkx<3.0,>=2.5\n  Downloading networkx-2.8.8-py3-none-any.whl (2.0 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.0/2.0 MB 90.4 MB/s eta 0:00:00\nRequirement already satisfied: numpy<2.0.0,>=1.19.5 in /home/coco/anaconda3/envs/py38/lib/python3.8/site-packages (from node2vec) (1.24.2)\nRequirement already satisfied: scipy>=1.7.0 in /home/coco/anaconda3/envs/py38/lib/python3.8/site-packages (from gensim<5.0.0,>=4.1.2->node2vec) (1.10.1)\nCollecting smart-open>=1.8.1\n  Downloading smart_open-6.3.0-py3-none-any.whl (56 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 56.8/56.8 kB 13.6 MB/s eta 0:00:00\nInstalling collected packages: tqdm, smart-open, networkx, gensim, node2vec\n  Attempting uninstall: networkx\n    Found existing installation: networkx 3.0\n    Uninstalling networkx-3.0:\n      Successfully uninstalled networkx-3.0\nSuccessfully installed gensim-4.3.1 networkx-2.8.8 node2vec-0.4.6 smart-open-6.3.0 tqdm-4.65.0\nNote: you may need to restart the kernel to use updated packages.\n\n\n\nfrom node2vec import Node2Vec\nfrom node2vec.edges import HadamardEmbedder, AverageEmbedder, WeightedL1Embedder, WeightedL2Embedder\n\nnode2vec_train = Node2Vec(train_graph, weight_key='weight')\nmodel_train = node2vec_train.fit(window=10)\n\n\n\n\nGenerating walks (CPU: 1): 100%|██████████| 10/10 [00:04<00:00,  2.47it/s]\n\n\n\nNode2Vec 알고리즘 사용해 특징 공간 구축\n\n\nfrom sklearn.ensemble import RandomForestClassifier \nfrom sklearn import metrics \n\nclasses = [HadamardEmbedder, AverageEmbedder, WeightedL1Embedder, WeightedL2Embedder]\nfor cl in classes:\n    embeddings_train = cl(keyed_vectors=model_train.wv) \n    # 벡터스페이스 상에 edge를 투영.. \n\n    train_embeddings = [embeddings_train[str(edgs[x][0]), str(edgs[x][1])] for x in train_edges]\n    test_embeddings = [embeddings_train[str(edgs[x][0]), str(edgs[x][1])] for x in test_edges]\n    \n    rf = RandomForestClassifier(n_estimators=1000, random_state=42) \n    rf.fit(train_embeddings, train_labels); \n\n    y_pred = rf.predict(test_embeddings)\n    print(cl)\n    print('Precision:', metrics.precision_score(test_labels, y_pred)) \n    print('Recall:', metrics.recall_score(test_labels, y_pred)) \n    print('F1-Score:', metrics.f1_score(test_labels, y_pred)) \n\n<class 'node2vec.edges.HadamardEmbedder'>\nPrecision: 0.7349397590361446\nRecall: 0.15996503496503497\nF1-Score: 0.26274228284278534\n<class 'node2vec.edges.AverageEmbedder'>\nPrecision: 0.6856264411990777\nRecall: 0.7797202797202797\nF1-Score: 0.7296523517382413\n<class 'node2vec.edges.WeightedL1Embedder'>\nPrecision: 0.5737704918032787\nRecall: 0.030594405594405596\nF1-Score: 0.05809128630705394\n<class 'node2vec.edges.WeightedL2Embedder'>\nPrecision: 0.609375\nRecall: 0.03409090909090909\nF1-Score: 0.06456953642384106\n\n\n\nNode2Vec 알고리즘 사용해 각 Edge2Vec 알고리즘으로 특징 공간 생성\nsklearn 파이썬 라이브러리의 RandomForestClassifier은 이전 단계에서 생성한 특징에 대해 학습\n검증 테스트 위해 정밀도, 재현율, F1-score 성능 지표 측정\n\n\n\n비지도학습\n\nk-means 알고리즘 사용\n지도학습과의 차이점은 특징 공간이 학습-검증 분할을 안함.\n\n\nnod2vec_unsup = Node2Vec(G_down, weight_key='weight')\nunsup_vals = nod2vec_unsup.fit(window=10)\n\n\n\n\nGenerating walks (CPU: 1): 100%|██████████| 10/10 [00:04<00:00,  2.30it/s]\n\n\n\n다운샘플링 절차에 전체 그래프 알고리즘 계산\n\n\nfrom sklearn.cluster import KMeans\n\nclasses = [HadamardEmbedder, AverageEmbedder, WeightedL1Embedder, WeightedL2Embedder]\ntrue_labels = [x for x in nx.get_edge_attributes(G_down, \"label\").values()]\n\nfor cl in classes:\n    embedding_edge = cl(keyed_vectors=unsup_vals.wv) \n\n    embedding = [embedding_edge[str(x[0]), str(x[1])] for x in G_down.edges()]\n    kmeans = KMeans(2, random_state=42).fit(embedding)\n    \n    \n    nmi = metrics.adjusted_mutual_info_score(true_labels, kmeans.labels_)\n    ho = metrics.homogeneity_score(true_labels, kmeans.labels_)\n    co = metrics.completeness_score(true_labels, kmeans.labels_)\n    vmeasure = metrics.v_measure_score(true_labels, kmeans.labels_)\n    \n    print(cl)\n    print('NMI:', nmi)\n    print('Homogeneity:', ho)\n    print('Completeness:', co)\n    print('V-Measure:', vmeasure)\n\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n\n\n<class 'node2vec.edges.HadamardEmbedder'>\nNMI: 0.04418691434534317\nHomogeneity: 0.0392170155918133\nCompleteness: 0.05077340984619601\nV-Measure: 0.044253187956299615\n\n\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n\n\n<class 'node2vec.edges.AverageEmbedder'>\nNMI: 0.10945180042668563\nHomogeneity: 0.10590886334115046\nCompleteness: 0.11336117407653773\nV-Measure: 0.10950837820667877\n\n\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n\n\n<class 'node2vec.edges.WeightedL1Embedder'>\nNMI: 0.17575054988974667\nHomogeneity: 0.1757509360433583\nCompleteness: 0.17585150874409544\nV-Measure: 0.17580120800977098\n\n\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n\n\n<class 'node2vec.edges.WeightedL2Embedder'>\nNMI: 0.13740583375677415\nHomogeneity: 0.13628828058562012\nCompleteness: 0.1386505946822449\nV-Measure: 0.13745928896382234\n\n\n- NMI(Normalized Mutual Information)\n\n두 개의 군집 결과 비교\n0~1이며 1에 가까울수록 높은 성능\n\n- Homogeneity\n\n하나의 실제 군집 내에서 같은 군집에 속한 샘플들이 군집화 결과에서 같은 군집에 속할 비율\n1에 가까울수록 높은 성능\n\n- Completeness\n\n하나의 예측 군집 내에서 같은 실제 군집에 속한 샘플들이 군집화 결과에서 같은 군집에 속할 비율\n0~1이며 1에 가까울수록 높은 성능\n\n- V-measure\n\nHomogeneity와 Completeness의 조화 평균\n0~1이며 1에 가까울수록 높은 성능\n비지도 학습에 이상치 탐지 방법\nk-means/LOF/One-class SVM 등이 있다.. 한번 같이 해보자.\n조금씩 다 커졌넹..\n\n- 지도학습에서 정상거래에서 다운샘플링을 했는데\n만약, 사기거래에서 업샘플링을 하게되면 어떻게 될까?"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "끄적끄적 coco 올리기 전 아무거나 막 쓰는 용도"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Scribbling",
    "section": "",
    "text": ":::{#quarto-listing-pipeline .hidden} \\(e = mC^2\\)\n:::{.hidden render-id=“pipeline-listing-listing”}\n:::{.list .quarto-listing-default}\n\n\n\n\n2023년 학회 발표 준비\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n\n\n\n회귀진단 실습\n\n\n\n\n\n\n\nApplied statistics\n\n\n\n\n\n\n\n\n\n\n\nMay 6, 2023\n\n\n김보람\n\n\n\n\n\n\n\n\nCH4. 지도 그래프 학습(그래프 정규화 방법)\n\n\n\n\n\n\n\ngraph\n\n\n\n\n\n\n\n\n\n\n\nApr 6, 2023\n\n\n김보람\n\n\n\n\n\n\n\n\nCH8. 신용카드 거래에 대한 그래프 분석(frac=0.4)\n\n\n\n\n\n\n\ngraph\n\n\n\n\n\n\n\n\n\n\n\nApr 4, 2023\n\n\n김보람\n\n\n\n\n\n\n\n\nCH8. 신용카드 거래에 대한 그래프 분석(frac=0.3)\n\n\n\n\n\n\n\ngraph\n\n\n\n\n\n\n\n\n\n\n\nApr 4, 2023\n\n\n김보람\n\n\n\n\n\n\n\n\nVAE(-ing)\n\n\n\n\n\n\n\nVAE\n\n\nAuto-Encoding Variational Bayes\n\n\n\n\n\n\n\n\n\n\n\nMar 14, 2023\n\n\n김보람\n\n\n\n\n:::\n\n\nNo matching items\n\n:::\n:::"
  }
]