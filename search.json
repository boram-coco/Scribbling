[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Scribbling",
    "section": "",
    "text": "GNN\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nCzech Bank’s Financial Data Analysis. Moving from gut feeling to data-driven decisions.\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\n2023년 학회 발표 준비\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\n중간고사 예상문제\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\n분석\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\n[GAN] 논문 정리 (~ing)\n\n\n\n\n\n\n\nGAN\n\n\n\n\n\n\n\n\n\n\n\nAug 1, 2023\n\n\n김보람\n\n\n\n\n\n\n  \n\n\n\n\n[FRAUD] 그래프자료로 데이터정리\n\n\n\n\n\n\n\n\n\n\n\n\nJul 19, 2023\n\n\n신록예찬\n\n\n\n\n\n\n  \n\n\n\n\n[FRAUD] 그래프자료로 데이터정리\n\n\n\n\n\n\n\n\n\n\n\n\nJul 19, 2023\n\n\n신록예찬\n\n\n\n\n\n\n  \n\n\n\n\nAS HW4_5\n\n\n\n\n\n\n\nApplied statistics\n\n\nhomework\n\n\n\n\n\n\n\n\n\n\n\nJun 9, 2023\n\n\n김보람\n\n\n\n\n\n\n  \n\n\n\n\nHands-on Graph Neural Networks with PyTorch & PyTorch Geometric\n\n\n\n\n\n\n\nGNN\n\n\n\n\n\n\n\n\n\n\n\nJun 2, 2023\n\n\n김보람\n\n\n\n\n\n\n  \n\n\n\n\n사기거래 날짜 wmatrix\n\n\n\n\n\n\n\n사기거래\n\n\n\n\n\n\n\n\n\n\n\nJun 2, 2023\n\n\n김보람\n\n\n\n\n\n\n  \n\n\n\n\n사기거래(교수님)\n\n\n\n\n\n\n\n사기거래\n\n\n\n\n\n\n\n\n\n\n\nMay 26, 2023\n\n\n김보람\n\n\n\n\n\n\n  \n\n\n\n\n사기거래(교수님 코드-변형)\n\n\n\n\n\n\n\n사기거래\n\n\n\n\n\n\n\n\n\n\n\nMay 26, 2023\n\n\n김보람\n\n\n\n\n\n\n  \n\n\n\n\n사기거래 변형코드\n\n\n\n\n\n\n\n사기거래\n\n\n\n\n\n\n\n\n\n\n\nMay 26, 2023\n\n\n김보람\n\n\n\n\n\n\n  \n\n\n\n\n사기거래(설명변수 time추가)\n\n\n\n\n\n\n\n사기거래\n\n\n\n\n\n\n\n\n\n\n\nMay 25, 2023\n\n\n김보람\n\n\n\n\n\n\n  \n\n\n\n\n미래 예측 데이터 분석(비트코인 시세 예측)\n\n\n\n\n\n\n\n회귀 분석\n\n\n\n\n\n\n\n\n\n\n\nMay 18, 2023\n\n\n김보람\n\n\n\n\n\n\n  \n\n\n\n\n사기거래(교수님과 같이 정리중)\n\n\n\n\n\n\n\n사기거래\n\n\n\n\n\n\n\n\n\n\n\nMay 12, 2023\n\n\n김보람\n\n\n\n\n\n\n  \n\n\n\n\n지도 학습\n\n\n\n\n\n\n\n지도 학습\n\n\n\n\n\n\n\n\n\n\n\nMay 11, 2023\n\n\n김보람\n\n\n\n\n\n\n  \n\n\n\n\nCH4. 지도 그래프 학습(그래프 정규화 방법)\n\n\n\n\n\n\n\ngraph\n\n\n\n\n\n\n\n\n\n\n\nApr 6, 2023\n\n\n김보람\n\n\n\n\n\n\n  \n\n\n\n\nCH8. 신용카드 거래에 대한 그래프 분석(frac=0.3)\n\n\n\n\n\n\n\ngraph\n\n\n\n\n\n\n\n\n\n\n\nApr 4, 2023\n\n\n김보람\n\n\n\n\n\n\n  \n\n\n\n\nVAE(-ing)\n\n\n\n\n\n\n\nVAE\n\n\nAuto-Encoding Variational Bayes\n\n\n\n\n\n\n\n\n\n\n\nMar 14, 2023\n\n\n김보람\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/사기거래/2023-07-19-그래프자료로 데이터정리.out-Copy1.html",
    "href": "posts/사기거래/2023-07-19-그래프자료로 데이터정리.out-Copy1.html",
    "title": "[FRAUD] 그래프자료로 데이터정리",
    "section": "",
    "text": "import numpy as np\nimport pandas as pd\nimport torch \nimport torch_geometric\n\n- 모든엣지를 고려\n\nN = 10 \nedge_index = torch.tensor([[i,j] for i in range(N) for j in range(N)]).T\n# edge_attr = 그래프의 웨이트 \n\n\nedge_index\n\ntensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2,\n         2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4,\n         4, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 7, 7,\n         7, 7, 7, 7, 7, 7, 7, 7, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 9, 9, 9, 9, 9, 9,\n         9, 9, 9, 9],\n        [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 2, 3,\n         4, 5, 6, 7, 8, 9, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 2, 3, 4, 5, 6, 7,\n         8, 9, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1,\n         2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 2, 3, 4, 5,\n         6, 7, 8, 9]])\n\n\n\nfraudTrain = pd.read_csv(\"~/Desktop/fraudTrain.csv\").iloc[:,1:]\n\n\nfraudTrain = fraudTrain.assign(trans_date_trans_time= list(map(lambda x: pd.to_datetime(x), fraudTrain.trans_date_trans_time)))\nfraudTrain\n\n\n\n\n\n\n\n\ntrans_date_trans_time\ncc_num\nmerchant\ncategory\namt\nfirst\nlast\ngender\nstreet\ncity\n...\nlat\nlong\ncity_pop\njob\ndob\ntrans_num\nunix_time\nmerch_lat\nmerch_long\nis_fraud\n\n\n\n\n0\n2019-01-01 00:00:00\n2.703190e+15\nfraud_Rippin, Kub and Mann\nmisc_net\n4.97\nJennifer\nBanks\nF\n561 Perry Cove\nMoravian Falls\n...\n36.0788\n-81.1781\n3495\nPsychologist, counselling\n1988-03-09\n0b242abb623afc578575680df30655b9\n1325376018\n36.011293\n-82.048315\n0\n\n\n1\n2019-01-01 00:00:00\n6.304230e+11\nfraud_Heller, Gutmann and Zieme\ngrocery_pos\n107.23\nStephanie\nGill\nF\n43039 Riley Greens Suite 393\nOrient\n...\n48.8878\n-118.2105\n149\nSpecial educational needs teacher\n1978-06-21\n1f76529f8574734946361c461b024d99\n1325376044\n49.159047\n-118.186462\n0\n\n\n2\n2019-01-01 00:00:00\n3.885950e+13\nfraud_Lind-Buckridge\nentertainment\n220.11\nEdward\nSanchez\nM\n594 White Dale Suite 530\nMalad City\n...\n42.1808\n-112.2620\n4154\nNature conservation officer\n1962-01-19\na1a22d70485983eac12b5b88dad1cf95\n1325376051\n43.150704\n-112.154481\n0\n\n\n3\n2019-01-01 00:01:00\n3.534090e+15\nfraud_Kutch, Hermiston and Farrell\ngas_transport\n45.00\nJeremy\nWhite\nM\n9443 Cynthia Court Apt. 038\nBoulder\n...\n46.2306\n-112.1138\n1939\nPatent attorney\n1967-01-12\n6b849c168bdad6f867558c3793159a81\n1325376076\n47.034331\n-112.561071\n0\n\n\n4\n2019-01-01 00:03:00\n3.755340e+14\nfraud_Keeling-Crist\nmisc_pos\n41.96\nTyler\nGarcia\nM\n408 Bradley Rest\nDoe Hill\n...\n38.4207\n-79.4629\n99\nDance movement psychotherapist\n1986-03-28\na41d7549acf90789359a9aa5346dcb46\n1325376186\n38.674999\n-78.632459\n0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n1048570\n2020-03-10 16:07:00\n6.011980e+15\nfraud_Fadel Inc\nhealth_fitness\n77.00\nHaley\nWagner\nF\n05561 Farrell Crescent\nAnnapolis\n...\n39.0305\n-76.5515\n92106\nAccountant, chartered certified\n1943-05-28\n45ecd198c65e81e597db22e8d2ef7361\n1362931649\n38.779464\n-76.317042\n0\n\n\n1048571\n2020-03-10 16:07:00\n4.839040e+15\nfraud_Cremin, Hamill and Reichel\nmisc_pos\n116.94\nMeredith\nCampbell\nF\n043 Hanson Turnpike\nHedrick\n...\n41.1826\n-92.3097\n1583\nGeochemist\n1999-06-28\nc00ce51c6ebb7657474a77b9e0b51f34\n1362931670\n41.400318\n-92.726724\n0\n\n\n1048572\n2020-03-10 16:08:00\n5.718440e+11\nfraud_O'Connell, Botsford and Hand\nhome\n21.27\nSusan\nMills\nF\n005 Cody Estates\nLouisville\n...\n38.2507\n-85.7476\n736284\nEngineering geologist\n1952-04-02\n17c9dc8b2a6449ca2473726346e58e6c\n1362931711\n37.293339\n-84.798122\n0\n\n\n1048573\n2020-03-10 16:08:00\n4.646850e+18\nfraud_Thompson-Gleason\nhealth_fitness\n9.52\nJulia\nBell\nF\n576 House Crossroad\nWest Sayville\n...\n40.7320\n-73.1000\n4056\nFilm/video editor\n1990-06-25\n5ca650881b48a6a38754f841c23b77ab\n1362931718\n39.773077\n-72.213209\n0\n\n\n1048574\n2020-03-10 16:08:00\n2.283740e+15\nfraud_Buckridge PLC\nmisc_pos\n6.81\nShannon\nWilliams\nF\n9345 Spencer Junctions Suite 183\nAlpharetta\n...\n34.0770\n-84.3033\n165556\nPrison officer\n1997-12-27\n8d0a575fe635bbde12f1a2bffc126731\n1362931730\n33.601468\n-83.891921\n0\n\n\n\n\n1048575 rows × 22 columns\n\n\n\n\ndiff = fraudTrain.trans_date_trans_time[101]-fraudTrain.trans_date_trans_time[0]\n\n\ndiff\n\nTimedelta('0 days 01:17:00')\n\n\n\ntheta = 86400*1.2\ntheta\n\n103680.0\n\n\n\ntheta = 86400*1.2\nnp.exp(-diff.total_seconds()/theta)\n\n0.9564180361647693\n\n\n\n# !git add .\n# !git commit -m. \n# !git push \n# !quarto publish --no-browser --no-prompt\n\n\n\n\nfraudTrain = fraudTrain.assign(trans_date_trans_time= list(map(lambda x: pd.to_datetime(x), fraudTrain.trans_date_trans_time)))\nfraudTrain\n\n\n\n\n\n\n\n\ntrans_date_trans_time\ncc_num\nmerchant\ncategory\namt\nfirst\nlast\ngender\nstreet\ncity\n...\nlat\nlong\ncity_pop\njob\ndob\ntrans_num\nunix_time\nmerch_lat\nmerch_long\nis_fraud\n\n\n\n\n0\n2019-01-01 00:00:00\n2.703190e+15\nfraud_Rippin, Kub and Mann\nmisc_net\n4.97\nJennifer\nBanks\nF\n561 Perry Cove\nMoravian Falls\n...\n36.0788\n-81.1781\n3495\nPsychologist, counselling\n1988-03-09\n0b242abb623afc578575680df30655b9\n1325376018\n36.011293\n-82.048315\n0\n\n\n1\n2019-01-01 00:00:00\n6.304230e+11\nfraud_Heller, Gutmann and Zieme\ngrocery_pos\n107.23\nStephanie\nGill\nF\n43039 Riley Greens Suite 393\nOrient\n...\n48.8878\n-118.2105\n149\nSpecial educational needs teacher\n1978-06-21\n1f76529f8574734946361c461b024d99\n1325376044\n49.159047\n-118.186462\n0\n\n\n2\n2019-01-01 00:00:00\n3.885950e+13\nfraud_Lind-Buckridge\nentertainment\n220.11\nEdward\nSanchez\nM\n594 White Dale Suite 530\nMalad City\n...\n42.1808\n-112.2620\n4154\nNature conservation officer\n1962-01-19\na1a22d70485983eac12b5b88dad1cf95\n1325376051\n43.150704\n-112.154481\n0\n\n\n3\n2019-01-01 00:01:00\n3.534090e+15\nfraud_Kutch, Hermiston and Farrell\ngas_transport\n45.00\nJeremy\nWhite\nM\n9443 Cynthia Court Apt. 038\nBoulder\n...\n46.2306\n-112.1138\n1939\nPatent attorney\n1967-01-12\n6b849c168bdad6f867558c3793159a81\n1325376076\n47.034331\n-112.561071\n0\n\n\n4\n2019-01-01 00:03:00\n3.755340e+14\nfraud_Keeling-Crist\nmisc_pos\n41.96\nTyler\nGarcia\nM\n408 Bradley Rest\nDoe Hill\n...\n38.4207\n-79.4629\n99\nDance movement psychotherapist\n1986-03-28\na41d7549acf90789359a9aa5346dcb46\n1325376186\n38.674999\n-78.632459\n0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n1048570\n2020-03-10 16:07:00\n6.011980e+15\nfraud_Fadel Inc\nhealth_fitness\n77.00\nHaley\nWagner\nF\n05561 Farrell Crescent\nAnnapolis\n...\n39.0305\n-76.5515\n92106\nAccountant, chartered certified\n1943-05-28\n45ecd198c65e81e597db22e8d2ef7361\n1362931649\n38.779464\n-76.317042\n0\n\n\n1048571\n2020-03-10 16:07:00\n4.839040e+15\nfraud_Cremin, Hamill and Reichel\nmisc_pos\n116.94\nMeredith\nCampbell\nF\n043 Hanson Turnpike\nHedrick\n...\n41.1826\n-92.3097\n1583\nGeochemist\n1999-06-28\nc00ce51c6ebb7657474a77b9e0b51f34\n1362931670\n41.400318\n-92.726724\n0\n\n\n1048572\n2020-03-10 16:08:00\n5.718440e+11\nfraud_O'Connell, Botsford and Hand\nhome\n21.27\nSusan\nMills\nF\n005 Cody Estates\nLouisville\n...\n38.2507\n-85.7476\n736284\nEngineering geologist\n1952-04-02\n17c9dc8b2a6449ca2473726346e58e6c\n1362931711\n37.293339\n-84.798122\n0\n\n\n1048573\n2020-03-10 16:08:00\n4.646850e+18\nfraud_Thompson-Gleason\nhealth_fitness\n9.52\nJulia\nBell\nF\n576 House Crossroad\nWest Sayville\n...\n40.7320\n-73.1000\n4056\nFilm/video editor\n1990-06-25\n5ca650881b48a6a38754f841c23b77ab\n1362931718\n39.773077\n-72.213209\n0\n\n\n1048574\n2020-03-10 16:08:00\n2.283740e+15\nfraud_Buckridge PLC\nmisc_pos\n6.81\nShannon\nWilliams\nF\n9345 Spencer Junctions Suite 183\nAlpharetta\n...\n34.0770\n-84.3033\n165556\nPrison officer\n1997-12-27\n8d0a575fe635bbde12f1a2bffc126731\n1362931730\n33.601468\n-83.891921\n0\n\n\n\n\n1048575 rows × 22 columns\n\n\n\n\nN = len(fraudTrain)\nN\n\n1048575\n\n\n\n# edge_index_list = []\n# for i in range(N):\n#     for j in range(N):\n#         time_difference = (fraudTrain['trans_date_trans_time'][i] - fraudTrain['trans_date_trans_time'][j]).total_seconds()\n#         edge_index_list.append([i, j, time_difference])\n\n\n# edge_index = torch.tensor(edge_index_list).T"
  },
  {
    "objectID": "posts/사기거래/2023-07-19-그래프자료로 데이터정리.out-Copy1.html#해보자",
    "href": "posts/사기거래/2023-07-19-그래프자료로 데이터정리.out-Copy1.html#해보자",
    "title": "[FRAUD] 그래프자료로 데이터정리",
    "section": "",
    "text": "fraudTrain = fraudTrain.assign(trans_date_trans_time= list(map(lambda x: pd.to_datetime(x), fraudTrain.trans_date_trans_time)))\nfraudTrain\n\n\n\n\n\n\n\n\ntrans_date_trans_time\ncc_num\nmerchant\ncategory\namt\nfirst\nlast\ngender\nstreet\ncity\n...\nlat\nlong\ncity_pop\njob\ndob\ntrans_num\nunix_time\nmerch_lat\nmerch_long\nis_fraud\n\n\n\n\n0\n2019-01-01 00:00:00\n2.703190e+15\nfraud_Rippin, Kub and Mann\nmisc_net\n4.97\nJennifer\nBanks\nF\n561 Perry Cove\nMoravian Falls\n...\n36.0788\n-81.1781\n3495\nPsychologist, counselling\n1988-03-09\n0b242abb623afc578575680df30655b9\n1325376018\n36.011293\n-82.048315\n0\n\n\n1\n2019-01-01 00:00:00\n6.304230e+11\nfraud_Heller, Gutmann and Zieme\ngrocery_pos\n107.23\nStephanie\nGill\nF\n43039 Riley Greens Suite 393\nOrient\n...\n48.8878\n-118.2105\n149\nSpecial educational needs teacher\n1978-06-21\n1f76529f8574734946361c461b024d99\n1325376044\n49.159047\n-118.186462\n0\n\n\n2\n2019-01-01 00:00:00\n3.885950e+13\nfraud_Lind-Buckridge\nentertainment\n220.11\nEdward\nSanchez\nM\n594 White Dale Suite 530\nMalad City\n...\n42.1808\n-112.2620\n4154\nNature conservation officer\n1962-01-19\na1a22d70485983eac12b5b88dad1cf95\n1325376051\n43.150704\n-112.154481\n0\n\n\n3\n2019-01-01 00:01:00\n3.534090e+15\nfraud_Kutch, Hermiston and Farrell\ngas_transport\n45.00\nJeremy\nWhite\nM\n9443 Cynthia Court Apt. 038\nBoulder\n...\n46.2306\n-112.1138\n1939\nPatent attorney\n1967-01-12\n6b849c168bdad6f867558c3793159a81\n1325376076\n47.034331\n-112.561071\n0\n\n\n4\n2019-01-01 00:03:00\n3.755340e+14\nfraud_Keeling-Crist\nmisc_pos\n41.96\nTyler\nGarcia\nM\n408 Bradley Rest\nDoe Hill\n...\n38.4207\n-79.4629\n99\nDance movement psychotherapist\n1986-03-28\na41d7549acf90789359a9aa5346dcb46\n1325376186\n38.674999\n-78.632459\n0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n1048570\n2020-03-10 16:07:00\n6.011980e+15\nfraud_Fadel Inc\nhealth_fitness\n77.00\nHaley\nWagner\nF\n05561 Farrell Crescent\nAnnapolis\n...\n39.0305\n-76.5515\n92106\nAccountant, chartered certified\n1943-05-28\n45ecd198c65e81e597db22e8d2ef7361\n1362931649\n38.779464\n-76.317042\n0\n\n\n1048571\n2020-03-10 16:07:00\n4.839040e+15\nfraud_Cremin, Hamill and Reichel\nmisc_pos\n116.94\nMeredith\nCampbell\nF\n043 Hanson Turnpike\nHedrick\n...\n41.1826\n-92.3097\n1583\nGeochemist\n1999-06-28\nc00ce51c6ebb7657474a77b9e0b51f34\n1362931670\n41.400318\n-92.726724\n0\n\n\n1048572\n2020-03-10 16:08:00\n5.718440e+11\nfraud_O'Connell, Botsford and Hand\nhome\n21.27\nSusan\nMills\nF\n005 Cody Estates\nLouisville\n...\n38.2507\n-85.7476\n736284\nEngineering geologist\n1952-04-02\n17c9dc8b2a6449ca2473726346e58e6c\n1362931711\n37.293339\n-84.798122\n0\n\n\n1048573\n2020-03-10 16:08:00\n4.646850e+18\nfraud_Thompson-Gleason\nhealth_fitness\n9.52\nJulia\nBell\nF\n576 House Crossroad\nWest Sayville\n...\n40.7320\n-73.1000\n4056\nFilm/video editor\n1990-06-25\n5ca650881b48a6a38754f841c23b77ab\n1362931718\n39.773077\n-72.213209\n0\n\n\n1048574\n2020-03-10 16:08:00\n2.283740e+15\nfraud_Buckridge PLC\nmisc_pos\n6.81\nShannon\nWilliams\nF\n9345 Spencer Junctions Suite 183\nAlpharetta\n...\n34.0770\n-84.3033\n165556\nPrison officer\n1997-12-27\n8d0a575fe635bbde12f1a2bffc126731\n1362931730\n33.601468\n-83.891921\n0\n\n\n\n\n1048575 rows × 22 columns\n\n\n\n\nN = len(fraudTrain)\nN\n\n1048575\n\n\n\n# edge_index_list = []\n# for i in range(N):\n#     for j in range(N):\n#         time_difference = (fraudTrain['trans_date_trans_time'][i] - fraudTrain['trans_date_trans_time'][j]).total_seconds()\n#         edge_index_list.append([i, j, time_difference])\n\n\n# edge_index = torch.tensor(edge_index_list).T"
  },
  {
    "objectID": "posts/사기거래/graph8(frac.html",
    "href": "posts/사기거래/graph8(frac.html",
    "title": "CH8. 신용카드 거래에 대한 그래프 분석(frac=0.3)",
    "section": "",
    "text": "ref\n\n그래프 머신러닝\ngithub\nCredit Card Transactions Fraud Detection Dataset\n컬리이미지\nnetworkx\n\n\n\n신용카드 거래에 대한 그래프 분석\n\n신용카드 거래 그래프 생성\n그래프에서 속성 및 커뮤니티 추출\n사기 거래 분류에 지도 및 비지도 머신러닝 알고리즘 적용\n\n\nimport pandas as pd\n\n\nimport os\nimport math\nimport numpy as np\nimport networkx as nx\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\ndefault_edge_color = 'gray'\ndefault_node_color = '#407cc9'\nenhanced_node_color = '#f5b042'\nenhanced_edge_color = '#cc2f04'\n\n\nimport pandas as pd\ndf = pd.read_csv(\"~/Desktop/fraudTrain.csv\")\ndf = df[df[\"is_fraud\"]==0].sample(frac=0.20, random_state=42).append(df[df[\"is_fraud\"] == 1])\ndf.head()\n\n\n\n\n\n\n\n\nUnnamed: 0\ntrans_date_trans_time\ncc_num\nmerchant\ncategory\namt\nfirst\nlast\ngender\nstreet\n...\nlat\nlong\ncity_pop\njob\ndob\ntrans_num\nunix_time\nmerch_lat\nmerch_long\nis_fraud\n\n\n\n\n669418\n669418\n2019-10-12 18:21\n4.089100e+18\nfraud_Haley, Jewess and Bechtelar\nshopping_pos\n7.53\nDebra\nStark\nF\n686 Linda Rest\n...\n32.3836\n-94.8653\n24536\nMultimedia programmer\n1983-10-14\nd313353fa30233e5fab5468e852d22fc\n1350066071\n32.202008\n-94.371865\n0\n\n\n32567\n32567\n2019-01-20 13:06\n4.247920e+12\nfraud_Turner LLC\ntravel\n3.79\nJudith\nMoss\nF\n46297 Benjamin Plains Suite 703\n...\n39.5370\n-83.4550\n22305\nTelevision floor manager\n1939-03-09\n88c65b4e1585934d578511e627fe3589\n1327064760\n39.156673\n-82.930503\n0\n\n\n156587\n156587\n2019-03-24 18:09\n4.026220e+12\nfraud_Klein Group\nentertainment\n59.07\nDebbie\nPayne\nF\n204 Ashley Neck Apt. 169\n...\n41.5224\n-71.9934\n4720\nBroadcast presenter\n1977-05-18\n3bd9ede04b5c093143d5e5292940b670\n1332612553\n41.657152\n-72.595751\n0\n\n\n1020243\n1020243\n2020-02-25 15:12\n4.957920e+12\nfraud_Monahan-Morar\npersonal_care\n25.58\nAlan\nParsons\nM\n0547 Russell Ford Suite 574\n...\n39.6171\n-102.4776\n207\nNetwork engineer\n1955-12-04\n19e16ee7a01d229e750359098365e321\n1361805120\n39.080346\n-103.213452\n0\n\n\n116272\n116272\n2019-03-06 23:19\n4.178100e+15\nfraud_Kozey-Kuhlman\npersonal_care\n84.96\nJill\nFlores\nF\n639 Cruz Islands\n...\n41.9488\n-86.4913\n3104\nHorticulturist, commercial\n1981-03-29\na0c8641ca1f5d6e243ed5a2246e66176\n1331075954\n42.502065\n-86.732664\n0\n\n\n\n\n5 rows × 23 columns\n\n\n\n\n_df = pd.read_csv(\"~/Desktop/fraudTrain.csv\")\n\n\ncus_list = set(_df.query('is_fraud==1').cc_num.tolist())\n_df2 = _df.query(\"cc_num in @ cus_list\")\n\n\n_df2.shape\n\n(651430, 23)\n\n\n\n_df2.groupby('is_fraud').agg({'category':np.sum})\n\n- 이분그래프\n\ndef build_graph_bipartite(df_input, graph_type=nx.Graph()):\n    df=df_input.copy()\n    mapping={x:node_id for node_id, x in enumerate(set(df[\"cc_num\"].values.tolist()+\\\n                                                      df[\"merchant\"].values.tolist()))}\n    \n    df[\"from\"]=df[\"cc_num\"].apply(lambda x:mapping[x])  #엣지의 출발점\n    df[\"to\"]=df[\"merchant\"].apply(lambda x:mapping[x])  #엣지의 도착점\n    \n    df = df[['from', 'to', \"amt\", \"is_fraud\"]].groupby(['from','to']).agg({\"is_fraud\":\"sum\",\"amt\":\"sum\"}).reset_index()\n    df[\"is_fraud\"]=df[\"is_fraud\"].apply(lambda x:1 if x&gt;0 else 0)\n    \n    G=nx.from_edgelist(df[[\"from\",\"to\"]].values, create_using=graph_type)\n    \n    nx.set_edge_attributes(G, {(int(x[\"from\"]),int(x[\"to\"])):x[\"is_fraud\"] for idx, x in df[[\"from\",\"to\",\"is_fraud\"]].iterrows()}, \"label\")  #엣지 속성 설정,각 속성의 사기 여부부 \n    \n    nx.set_edge_attributes(G,{(int(x[\"from\"]),int(x[\"to\"])):x[\"amt\"] for idx,x in df[[\"from\",\"to\",\"amt\"]].iterrows()}, \"weight\") # 엣지 속성 설정, 각 엣지의 거래 금액\n\n    return G\n\n\nG_bu = build_graph_bipartite(df, nx.Graph(name=\"Bipartite Undirect\"))\n\n\n지도학습\n\nfrom sklearn.utils import resample\n\ndf_majority = df[df.is_fraud==0]\ndf_minority = df[df.is_fraud==1]\n\ndf_maj_dowsampled = resample(df_majority,\n                             n_samples=len(df_minority),\n                             random_state=42)\n\ndf_downsampled = pd.concat([df_minority, df_maj_dowsampled])\n\nprint(df_downsampled.is_fraud.value_counts())\nG_down = build_graph_bipartite(df_downsampled)\n\n1    6006\n0    6006\nName: is_fraud, dtype: int64\n\n\n\nfrom sklearn.model_selection import train_test_split\n\n\ntrain_edges, test_edges, train_labels, test_labels = train_test_split(list(range(len(G_down.edges))), \n                                                                      list(nx.get_edge_attributes(G_down, \"label\").values()), \n                                                                      test_size=0.20, \n                                                                      random_state=42)\n\n\nedgs = list(G_down.edges)\ntrain_graph = G_down.edge_subgraph([edgs[x] for x in train_edges]).copy()\ntrain_graph.add_nodes_from(list(set(G_down.nodes) - set(train_graph.nodes)))\n\n\nfrom node2vec import Node2Vec\nfrom node2vec.edges import HadamardEmbedder, AverageEmbedder, WeightedL1Embedder, WeightedL2Embedder\n\nnode2vec_train = Node2Vec(train_graph, weight_key='weight')\nmodel_train = node2vec_train.fit(window=10)\n\n\n\n\nGenerating walks (CPU: 1):   0%|          | 0/10 [00:00&lt;?, ?it/s]Generating walks (CPU: 1): 100%|██████████| 10/10 [00:04&lt;00:00,  2.48it/s]\n\n\n\n\n\n\nfrom sklearn.ensemble import RandomForestClassifier \nfrom sklearn import metrics \n\nclasses = [HadamardEmbedder, AverageEmbedder, WeightedL1Embedder, WeightedL2Embedder]\nfor cl in classes:\n    embeddings_train = cl(keyed_vectors=model_train.wv) \n\n\ntrain_embeddings = [embeddings_train[str(edgs[x][0]), str(edgs[x][1])] for x in train_edges]\ntest_embeddings = [embeddings_train[str(edgs[x][0]), str(edgs[x][1])] for x in test_edges]\n\n\nrf = RandomForestClassifier(n_estimators=1000, random_state=42) \n\n\nrf.fit(train_embeddings, train_labels)\n\nRandomForestClassifier(n_estimators=1000, random_state=42)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.RandomForestClassifierRandomForestClassifier(n_estimators=1000, random_state=42)\n\n\n\n    #\n    y_hat = rf.predict_proba(test_embeddings)\n    y_pred = np.argmax(y_hat,axis=1)\n    #y_pred = rf.predict(test_embeddings)\n\n\n\n\n    print(cl)\n    print('Precision:', metrics.precision_score(test_labels, y_pred)) \n    print('Recall:', metrics.recall_score(test_labels, y_pred)) \n    print('F1-Score:', metrics.f1_score(test_labels, y_pred)) \n\n&lt;class 'node2vec.edges.WeightedL2Embedder'&gt;\nPrecision: 0.6481481481481481\nRecall: 0.028641571194762683\nF1-Score: 0.054858934169278985"
  },
  {
    "objectID": "posts/사기거래/교수님 data(re).html",
    "href": "posts/사기거래/교수님 data(re).html",
    "title": "사기거래 변형코드",
    "section": "",
    "text": "신용카드 거래 사기탐지 TRY 변형"
  },
  {
    "objectID": "posts/사기거래/교수님 data(re).html#분석1",
    "href": "posts/사기거래/교수님 data(re).html#분석1",
    "title": "사기거래 변형코드",
    "section": "분석1",
    "text": "분석1\n- step1: data\n\nX = np.array(df02_tr.loc[:,['amt']])\nXX = np.array(df50_test.loc[:,['amt']])\ny = np.array(df02_tr.is_fraud)\nyy = np.array(df50_test.is_fraud)\n\n- step2: lrnr\n\nlrnr = ensemble.GradientBoostingClassifier()\n\n- step3: fit\n\nlrnr.fit(X,y)\n\nGradientBoostingClassifier()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.GradientBoostingClassifierGradientBoostingClassifier()\n\n\n- step4: evaluate\n\nthresh = y.mean()\nyyhat = (lrnr.predict_proba(XX)&gt; thresh)[:,-1]\n#yyhat = lrnr.predict(XX) \n\n\nmetrics = [sklearn.metrics.accuracy_score,\n           sklearn.metrics.precision_score,\n           sklearn.metrics.recall_score,\n           sklearn.metrics.f1_score]\n\n\n_results1= pd.DataFrame({m.__name__:[m(yy,yyhat).round(6)] for m in metrics},index=['분석1'])\n_results1\n\n\n\n\n\n\n\n\naccuracy_score\nprecision_score\nrecall_score\nf1_score\n\n\n\n\n분석1\n0.88678\n0.910677\n0.86025\n0.884746"
  },
  {
    "objectID": "posts/사기거래/교수님 data(re).html#분석2",
    "href": "posts/사기거래/교수님 data(re).html#분석2",
    "title": "사기거래 변형코드",
    "section": "분석2",
    "text": "분석2\n- step1: data\n\ndef amtano(df_train):\n    df = df_train.copy()\n    df = df.assign(amtano1=0)\n    df = df.assign(amtano2=0)\n    df = df.assign(amtano3=0)\n    normalize = lambda arr: (arr-np.median(arr)) if np.std(arr)!=0 else arr*0 \n    rollmax = lambda arr: arr.rolling(window=3, min_periods=1).max()\n    rollmin = lambda arr: arr.rolling(window=3, min_periods=1).min()    \n    rollmean = lambda arr: arr.rolling(window=3, min_periods=1).mean()    \n    rollstd = lambda arr: arr.rolling(window=3, min_periods=1).std()    \n    for cc_num, sub_df in df.groupby('cc_num'):\n        df.loc[df.cc_num == cc_num,['amtano1']] = normalize(sub_df.amt)\n        df.loc[df.cc_num == cc_num,['amtano2']] = (sub_df.amt * rollmean(normalize(sub_df.amt)))/rollmin(sub_df.amt)\n        df.loc[df.cc_num == cc_num,['amtano3']] = rollmin(normalize(sub_df.amt))\n    return df  \n\n\ndf02_tr2 = amtano(df02_tr)\ndf50_test2 = amtano(df50_test)\n\nX = np.array(df02_tr2.loc[:,['amtano2']])\nXX = np.array(df50_test2.loc[:,['amtano2']])\ny = np.array(df02_tr.is_fraud)\nyy = np.array(df50_test.is_fraud)\n\n\ncclist = df02_tr2.cc_num.unique().tolist()\n\n\ni=190\nplt.plot(df02_tr2.query('cc_num==@cclist[@i]').trans_date_trans_time, df02_tr2.query('cc_num==@cclist[@i]').amt,'o')\nplt.plot(df02_tr2.query('cc_num==@cclist[@i]').trans_date_trans_time, df02_tr2.query('cc_num==@cclist[@i]').is_fraud*df02_tr2.query('cc_num==@cclist[@i]').amt,'x')\n\n\n\n\n- step2: lrnr\n\nlrnr = ensemble.GradientBoostingClassifier()\n\n- step3: fit\n\nlrnr.fit(X,y)\n\nGradientBoostingClassifier()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.GradientBoostingClassifierGradientBoostingClassifier()\n\n\n- step4: evaluate\n\n# thresh = y.mean()\n# yyhat = (lrnr.predict_proba(XX)&gt; thresh)[:,-1]\nyyhat = lrnr.predict(XX) \n\n\nmetrics = [sklearn.metrics.accuracy_score,\n           sklearn.metrics.precision_score,\n           sklearn.metrics.recall_score,\n           sklearn.metrics.f1_score]\n\n\n_results2= pd.DataFrame({m.__name__:[m(yy,yyhat).round(6)] for m in metrics},index=['분석2'])\n_results2\n\n\n\n\n\n\n\n\naccuracy_score\nprecision_score\nrecall_score\nf1_score\n\n\n\n\n분석2\n0.53047\n0.908397\n0.078444\n0.144417"
  },
  {
    "objectID": "posts/사기거래/교수님 data(re).html#분석-정리",
    "href": "posts/사기거래/교수님 data(re).html#분석-정리",
    "title": "사기거래 변형코드",
    "section": "분석 정리",
    "text": "분석 정리\n\npd.concat([_results1,_results2])\n\n\n\n\n\n\n\n\naccuracy_score\nprecision_score\nrecall_score\nf1_score\n\n\n\n\n분석1\n0.88678\n0.910677\n0.860250\n0.884746\n\n\n분석2\n0.53047\n0.908397\n0.078444\n0.144417"
  },
  {
    "objectID": "posts/사기거래/교수님 data(원본).html",
    "href": "posts/사기거래/교수님 data(원본).html",
    "title": "사기거래(교수님)",
    "section": "",
    "text": "신용카드 거래 사기탐지 TRY1"
  },
  {
    "objectID": "posts/사기거래/교수님 data(원본).html#분석",
    "href": "posts/사기거래/교수님 data(원본).html#분석",
    "title": "사기거래(교수님)",
    "section": "분석",
    "text": "분석\n\n분석1\n- step1: data\n\nX = np.array(df50_tr.loc[:,['amt']])\nXX = np.array(df50_test.loc[:,['amt']])\ny = np.array(df50_tr.is_fraud)\nyy = np.array(df50_test.is_fraud)\n\n- step2: lrnr생성\n\nlrnr = sklearn.linear_model.LogisticRegression()\n\n- step3: fit\n\nlrnr.fit(X,y)\n\nLogisticRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LogisticRegressionLogisticRegression()\n\n\n- step4: evaluate\n\nthresh = df50_tr.is_fraud.mean()\nyyhat = (lrnr.predict_proba(XX)&gt;thresh)[:,-1]\n\n\nmetrics = [sklearn.metrics.accuracy_score,\n           sklearn.metrics.precision_score,\n           sklearn.metrics.recall_score,\n           sklearn.metrics.f1_score]\n\n\n_results1 = pd.DataFrame({m.__name__:[m(yy,yyhat).round(6)] for m in metrics}, index=['분석1'])\n_results1\n\n\n\n\n\n\n\n\naccuracy_score\nprecision_score\nrecall_score\nf1_score\n\n\n\n\n분석1\n0.849484\n0.933279\n0.756098\n0.835397\n\n\n\n\n\n\n\n\n\n분석2\n- step1: data\n\nX = np.array(fraudTrain_tr.loc[:,['amt']])\nXX = np.array(df50_test.loc[:,['amt']])\ny = np.array(fraudTrain_tr.is_fraud)\nyy = np.array(df50_test.is_fraud)\n\n- step2: lrnr생성\n\nlrnr = sklearn.linear_model.LogisticRegression()\n\n- step3: fit\n\nlrnr.fit(X,y)\n\nLogisticRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LogisticRegressionLogisticRegression()\n\n\n\nthresh = fraudTrain_tr.is_fraud.mean()\nyyhat = (lrnr.predict_proba(XX)&gt;thresh)[:,-1]\n\n\n_results2 = pd.DataFrame({m.__name__:[m(yy,yyhat).round(6)] for m in metrics}, index=['분석2'])\n_results2\n\n\n\n\n\n\n\n\naccuracy_score\nprecision_score\nrecall_score\nf1_score\n\n\n\n\n분석2\n0.829171\n0.885561\n0.760053\n0.818021\n\n\n\n\n\n\n\n\n\n분석3\n- 함수\n\ndef amtano1(df_train):\n    df = df_train.copy()\n    df = df.assign(amtano=0)\n    normalize = lambda arr: (arr-np.median(arr))/np.std(arr) if np.std(arr)!=0 else arr*0\n    for cc_num, sub_df in df.groupby('cc_num'):\n        df.loc[df.cc_num == cc_num,['amtano']] = normalize(sub_df.amt).cumsum()\n        return df\n\n\ndef amtano2(dt_train, df_test):\n    df = pd.concat([df_train, df_test])\n    df_amtano = amtano_train(df)\n    return df_test.assign(amtano = df_amtano.loc[[i in df_test.index for i in df_amtano.index],'amtano'])\n\n\n# amtano2함수정확이 뭔지 헷갈려\n\n- step1: data\n\nX = np.array(amtano1(df50_tr).loc[:,['amt','amtano']])\nXX = np.array(amtano1(df50_test).loc[:,['amt','amtano']])\ny = np.array(df50_tr.is_fraud)\nyy = np.array(df50_test.is_fraud)\n\n- step2: lrnr생성\n\nlrnr = sklearn.linear_model.LogisticRegression()\n\n- step3: fit\n\nlrnr.fit(X,y)\n\nLogisticRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LogisticRegressionLogisticRegression()\n\n\n- step4: evaluate\n\nthresh = df50_tr.is_fraud.mean()\nyyhat = (lrnr.predict_proba(XX)&gt;thresh)[:,-1]\n\n\n_results3= pd.DataFrame({m.__name__:[m(yy,yyhat).round(6)] for m in metrics},index=['분석3'])\n_results3\n\n\n\n\n\n\n\n\naccuracy_score\nprecision_score\nrecall_score\nf1_score\n\n\n\n\n분석3\n0.849484\n0.933279\n0.756098\n0.835397\n\n\n\n\n\n\n\n\n\n분석4\n- step1: data\n\nX = np.array(amtano1(fraudTrain_tr).loc[:,['amt','amtano']])\nXX = np.array(amtano1(df50_test).loc[:,['amt','amtano']])\ny = np.array(fraudTrain_tr.is_fraud)\nyy = np.array(df50_test.is_fraud)\n\n- step2: lrnr\n\nlrnr = sklearn.linear_model.LogisticRegression()\n\n- step3: fit\n\nlrnr.fit(X,y)\n\nLogisticRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LogisticRegressionLogisticRegression()\n\n\n- step4: evaluate\n\nthresh = fraudTrain_tr.is_fraud.mean()\nyyhat = (lrnr.predict_proba(XX)&gt;thresh)[:,-1]\n\n\n_results4= pd.DataFrame({m.__name__:[m(yy,yyhat).round(6)] for m in metrics},index=['분석4'])\n_results4\n\n\n\n\n\n\n\n\naccuracy_score\nprecision_score\nrecall_score\nf1_score\n\n\n\n\n분석4\n0.828505\n0.884202\n0.760053\n0.817441\n\n\n\n\n\n\n\n\n\n분석5\n- step1: data\n\nGtr = build_graph_bipartite(df50_tr)\nGtest = build_graph_bipartite(df50_test)\nX,y = embedding(Gtr)\nXX, yy = embedding(Gtest)\n\n\n\n\nGenerating walks (CPU: 1):   0%|          | 0/10 [00:00&lt;?, ?it/s]Generating walks (CPU: 1): 100%|██████████| 10/10 [00:02&lt;00:00,  3.41it/s]\nGenerating walks (CPU: 1):   0%|          | 0/10 [00:00&lt;?, ?it/s]Generating walks (CPU: 1): 100%|██████████| 10/10 [00:02&lt;00:00,  3.51it/s]\n\n\n\n\n\n\n\n\n\n- step2: lrnr생성\n\nlrnr = sklearn.linear_model.LogisticRegression()\n\n- step3: fit\n\nlrnr.fit(X,y)\n\nLogisticRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LogisticRegressionLogisticRegression()\n\n\n- step4: evaluate\n\nyyhat = lrnr.predict(XX)\n\n\n_results5= pd.DataFrame({m.__name__:[m(yy,yyhat).round(6)] for m in metrics},index=['분석5'])\n_results5\n\n\n\n\n\n\n\n\naccuracy_score\nprecision_score\nrecall_score\nf1_score\n\n\n\n\n분석5\n0.503845\n0.504714\n0.921854\n0.652296"
  },
  {
    "objectID": "posts/사기거래/교수님 data(원본).html#분석-정리",
    "href": "posts/사기거래/교수님 data(원본).html#분석-정리",
    "title": "사기거래(교수님)",
    "section": "분석 정리",
    "text": "분석 정리\n\npd.concat([_results1,_results2,_results3,_results4,_results5])\n\n\n\n\n\n\n\n\naccuracy_score\nprecision_score\nrecall_score\nf1_score\n\n\n\n\n분석1\n0.849484\n0.933279\n0.756098\n0.835397\n\n\n분석2\n0.829171\n0.885561\n0.760053\n0.818021\n\n\n분석3\n0.849484\n0.933279\n0.756098\n0.835397\n\n\n분석4\n0.828505\n0.884202\n0.760053\n0.817441\n\n\n분석5\n0.503845\n0.504714\n0.921854\n0.652296"
  },
  {
    "objectID": "posts/사기거래/교수님 data(원본).html#분석-1",
    "href": "posts/사기거래/교수님 data(원본).html#분석-1",
    "title": "사기거래(교수님)",
    "section": "분석",
    "text": "분석\n\n분석1\n- step1: data\n\nX = np.array(df50_tr.loc[:,['amt']])\nXX = np.array(df50_test.loc[:,['amt']])\ny = np.array(df50_tr.is_fraud)\nyy = np.array(df50_test.is_fraud)\n\n- step2: lrnr생성\n\nlrnr = sklearn.linear_model.LogisticRegression()\n\n- step3: fit\n\nlrnr.fit(X,y)\n\nLogisticRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LogisticRegressionLogisticRegression()\n\n\n- step4: evaluate\n\nyyhat = lrnr.predict(XX)\n\n\n_results1= pd.DataFrame({m.__name__:[m(yy,yyhat).round(6)] for m in metrics},index=['분석1'])\n_results1\n\n\n\n\n\n\n\n\naccuracy_score\nprecision_score\nrecall_score\nf1_score\n\n\n\n\n분석1\n0.849484\n0.933279\n0.756098\n0.835397\n\n\n\n\n\n\n\n\n\n분석2\n- step1: data\n\nX = np.array(df50_tr.loc[:,['amt']])\nXX = np.array(df50_test.loc[:,['amt']])\ny = np.array(df50_tr.is_fraud)\nyy = np.array(df50_test.is_fraud)\n\n- step2: lrnr생성\n\nlrnr = ensemble.GradientBoostingClassifier()\n\n- step3: fit\n\nlrnr.fit(X,y)\n\nGradientBoostingClassifier()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.GradientBoostingClassifierGradientBoostingClassifier()\n\n\n- step4: evaluate\n\nyyhat = lrnr.predict(XX)\n\n\n_results2= pd.DataFrame({m.__name__:[m(yy,yyhat).round(6)] for m in metrics},index=['분석2'])\n_results2\n\n\n\n\n\n\n\n\naccuracy_score\nprecision_score\nrecall_score\nf1_score\n\n\n\n\n분석2\n0.885781\n0.908206\n0.86091\n0.883926\n\n\n\n\n\n\n\n\n\n분석3\n- step1: data\n\nX = np.array(amtano1(df50_tr).loc[:,['amt','amtano']])\nXX = np.array(amtano1(df50_test).loc[:,['amt','amtano']])\ny = np.array(df50_tr.is_fraud)\nyy = np.array(df50_test.is_fraud)\n\n- step2: lrnr생성\n\nlrnr = ensemble.GradientBoostingClassifier()\n\n- step3: fit\n\nlrnr.fit(X,y)\n\nGradientBoostingClassifier()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.GradientBoostingClassifierGradientBoostingClassifier()\n\n\n- step4: evaluate\n\nyyhat = lrnr.predict(XX)\n\n\n_results3= pd.DataFrame({m.__name__:[m(yy,yyhat).round(6)] for m in metrics},index=['분석3'])\n_results3\n\n\n\n\n\n\n\n\naccuracy_score\nprecision_score\nrecall_score\nf1_score\n\n\n\n\n분석3\n0.887113\n0.90846\n0.863546\n0.885434\n\n\n\n\n\n\n\n\n\n분석4\n- step1: data\n\nX = np.array(amtano1(df02_tr).loc[:,['amt','amtano']])\nXX = np.array(amtano1(df50_test).loc[:,['amt','amtano']])\ny = np.array(df02_tr.is_fraud)\nyy = np.array(df50_test.is_fraud)\n\n- step2: lrnr생성\n\nlrnr = ensemble.GradientBoostingClassifier()\n\n- step3: fit\n\nlrnr.fit(X,y)\n\nGradientBoostingClassifier()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.GradientBoostingClassifierGradientBoostingClassifier()\n\n\n- step4: evaluate\n\nthresh = y.mean()\nyyhat = (lrnr.predict_proba(XX)&gt; thresh)[:,-1]\n\n\n_results4= pd.DataFrame({m.__name__:[m(yy,yyhat).round(6)] for m in metrics},index=['분석4'])\n_results4\n\n\n\n\n\n\n\n\naccuracy_score\nprecision_score\nrecall_score\nf1_score\n\n\n\n\n분석4\n0.88678\n0.910677\n0.86025\n0.884746\n\n\n\n\n\n\n\n\n\n분석5\n- step1: data\n\nX = np.array(amtano1(fraudTrain_tr).loc[:,['amt','amtano']])\nXX = np.array(amtano1(df50_test).loc[:,['amt','amtano']])\ny = np.array(fraudTrain_tr.is_fraud)\nyy = np.array(df50_test.is_fraud)\n\n- step2: lrnr생성\n\nlrnr = ensemble.GradientBoostingClassifier()\n\n- step3: fit\n\nlrnr.fit(X,y)\n\nGradientBoostingClassifier()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.GradientBoostingClassifierGradientBoostingClassifier()\n\n\n- step4: evaluate\n\nthresh = y.mean()\nyyhat = (lrnr.predict_proba(XX)&gt; thresh)[:,-1]\n\n\n_results5= pd.DataFrame({m.__name__:[m(yy,yyhat).round(6)] for m in metrics},index=['분석5'])\n_results5\n\n\n\n\n\n\n\n\naccuracy_score\nprecision_score\nrecall_score\nf1_score\n\n\n\n\n분석5\n0.874792\n0.878065\n0.873434\n0.875744"
  },
  {
    "objectID": "posts/사기거래/교수님 data(원본).html#분석정리",
    "href": "posts/사기거래/교수님 data(원본).html#분석정리",
    "title": "사기거래(교수님)",
    "section": "분석정리",
    "text": "분석정리\n\n\n\n\nTrain\nTest\n모형\n설명변수\n비고\n\n\n\n\n분석1\ndf50train\ndf50test\n로지스틱\namt\n\n\n\n분석2\ndf50train\ndf50test\n그레디언트부스팅\namt\nbase\n\n\n분석3\ndf50train\ndf50test\n그레디언트부스팅\namt, amtano\n\n\n\n분석4\ndf02train\ndf50test\n그레디언트부스팅\namt, amtano\n\n\n\n분석5\nfraudTrain_tr\ndf50test\n그레디언트부스팅\namt, amtano\n\n\n\n\n\nlst = [_results1,_results2,_results3,_results4,_results5]\npd.concat(lst)\n\n\n\n\n\n\n\n\naccuracy_score\nprecision_score\nrecall_score\nf1_score\n\n\n\n\n분석1\n0.849484\n0.933279\n0.756098\n0.835397\n\n\n분석2\n0.885781\n0.908206\n0.860910\n0.883926\n\n\n분석3\n0.887113\n0.908460\n0.863546\n0.885434\n\n\n분석4\n0.886780\n0.910677\n0.860250\n0.884746\n\n\n분석5\n0.874792\n0.878065\n0.873434\n0.875744"
  },
  {
    "objectID": "posts/사기거래/사기거래 날짜 230602.html",
    "href": "posts/사기거래/사기거래 날짜 230602.html",
    "title": "사기거래 날짜 wmatrix",
    "section": "",
    "text": "신용카드 거래 사기탐지 TRY 변형"
  },
  {
    "objectID": "posts/사기거래/사기거래 날짜 230602.html#분석1",
    "href": "posts/사기거래/사기거래 날짜 230602.html#분석1",
    "title": "사기거래 날짜 wmatrix",
    "section": "분석1",
    "text": "분석1\n- step1: data\n\nX = np.array(df02_tr.loc[:,['amt']])\nXX = np.array(df50_test.loc[:,['amt']])\ny = np.array(df02_tr.is_fraud)\nyy = np.array(df50_test.is_fraud)\n\n- step2: lrnr\n\nlrnr = ensemble.GradientBoostingClassifier()\n\n- step3: fit\n\nlrnr.fit(X,y)\n\nGradientBoostingClassifier()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.GradientBoostingClassifierGradientBoostingClassifier()\n\n\n- step4: evaluate\n\nthresh = y.mean()\nyyhat = (lrnr.predict_proba(XX)&gt; thresh)[:,-1]\n#yyhat = lrnr.predict(XX) \n\n\nmetrics = [sklearn.metrics.accuracy_score,\n           sklearn.metrics.precision_score,\n           sklearn.metrics.recall_score,\n           sklearn.metrics.f1_score]\n\n\n_results1= pd.DataFrame({m.__name__:[m(yy,yyhat).round(6)] for m in metrics},index=['분석1'])\n_results1\n\n\n\n\n\n\n\n\naccuracy_score\nprecision_score\nrecall_score\nf1_score\n\n\n\n\n분석1\n0.88678\n0.910677\n0.86025\n0.884746"
  },
  {
    "objectID": "posts/사기거래/사기거래 날짜 230602.html#분석2",
    "href": "posts/사기거래/사기거래 날짜 230602.html#분석2",
    "title": "사기거래 날짜 wmatrix",
    "section": "분석2",
    "text": "분석2\n- step1: data\n\ndef amtano(df_train):\n    df = df_train.copy()\n    df = df.assign(amtano1=0)\n    df = df.assign(amtano2=0)\n    df = df.assign(amtano3=0)\n    normalize = lambda arr: (arr-np.median(arr)) if np.std(arr)!=0 else arr*0 \n    rollmax = lambda arr: arr.rolling(window=3, min_periods=1).max()\n    rollmin = lambda arr: arr.rolling(window=3, min_periods=1).min()    \n    rollmean = lambda arr: arr.rolling(window=3, min_periods=1).mean()    \n    rollstd = lambda arr: arr.rolling(window=3, min_periods=1).std()    \n    for cc_num, sub_df in df.groupby('cc_num'):\n        df.loc[df.cc_num == cc_num,['amtano1']] = normalize(sub_df.amt)\n        df.loc[df.cc_num == cc_num,['amtano2']] = (sub_df.amt * rollmean(normalize(sub_df.amt)))/rollmin(sub_df.amt)\n        df.loc[df.cc_num == cc_num,['amtano3']] = rollmin(normalize(sub_df.amt))\n    return df  \n\n\ndf02_tr2 = amtano(df02_tr)\ndf50_test2 = amtano(df50_test)\n\nX = np.array(df02_tr2.loc[:,['amtano2']])\nXX = np.array(df50_test2.loc[:,['amtano2']])\ny = np.array(df02_tr.is_fraud)\nyy = np.array(df50_test.is_fraud)\n\n\ncclist = df02_tr2.cc_num.unique().tolist()\n\n\ni=190\nplt.plot(df02_tr2.query('cc_num==@cclist[@i]').trans_date_trans_time, df02_tr2.query('cc_num==@cclist[@i]').amt,'o')\nplt.plot(df02_tr2.query('cc_num==@cclist[@i]').trans_date_trans_time, df02_tr2.query('cc_num==@cclist[@i]').is_fraud*df02_tr2.query('cc_num==@cclist[@i]').amt,'x')\n\n\n\n\n- step2: lrnr\n\nlrnr = ensemble.GradientBoostingClassifier()\n\n- step3: fit\n\nlrnr.fit(X,y)\n\nGradientBoostingClassifier()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.GradientBoostingClassifierGradientBoostingClassifier()\n\n\n- step4: evaluate\n\n# thresh = y.mean()\n# yyhat = (lrnr.predict_proba(XX)&gt; thresh)[:,-1]\nyyhat = lrnr.predict(XX) \n\n\nmetrics = [sklearn.metrics.accuracy_score,\n           sklearn.metrics.precision_score,\n           sklearn.metrics.recall_score,\n           sklearn.metrics.f1_score]\n\n\n_results2= pd.DataFrame({m.__name__:[m(yy,yyhat).round(6)] for m in metrics},index=['분석2'])\n_results2\n\n\n\n\n\n\n\n\naccuracy_score\nprecision_score\nrecall_score\nf1_score\n\n\n\n\n분석2\n0.53047\n0.908397\n0.078444\n0.144417"
  },
  {
    "objectID": "posts/사기거래/사기거래 날짜 230602.html#분석-정리",
    "href": "posts/사기거래/사기거래 날짜 230602.html#분석-정리",
    "title": "사기거래 날짜 wmatrix",
    "section": "분석 정리",
    "text": "분석 정리\n\npd.concat([_results1,_results2])\n\n\n\n\n\n\n\n\naccuracy_score\nprecision_score\nrecall_score\nf1_score\n\n\n\n\n분석1\n0.88678\n0.910677\n0.860250\n0.884746\n\n\n분석2\n0.53047\n0.908397\n0.078444\n0.144417"
  },
  {
    "objectID": "posts/사기거래/2023-07-19-그래프자료로 데이터정리.out.html",
    "href": "posts/사기거래/2023-07-19-그래프자료로 데이터정리.out.html",
    "title": "[FRAUD] 그래프자료로 데이터정리",
    "section": "",
    "text": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt \nimport networkx as nx\nimport sklearn\nimport torch\n\n# sklearn\nfrom sklearn import model_selection # split함수이용\nfrom sklearn import ensemble # RF,GBM\nfrom sklearn import metrics \n\n# embedding \nfrom node2vec import Node2Vec\nfrom node2vec.edges import HadamardEmbedder, AverageEmbedder, WeightedL1Embedder, WeightedL2Embedder\n\n\ndef build_graph_bipartite(df_input, graph_type=nx.Graph()):\n    df=df_input.copy()\n    mapping={x:node_id for node_id, x in enumerate(set(df[\"cc_num\"].values.tolist()+\\\n                                                      df[\"merchant\"].values.tolist()))}\n    \n    df[\"from\"]=df[\"cc_num\"].apply(lambda x:mapping[x])  #엣지의 출발점\n    df[\"to\"]=df[\"merchant\"].apply(lambda x:mapping[x])  #엣지의 도착점\n    \n    df = df[['from', 'to', \"amt\", \"is_fraud\"]].groupby(['from','to']).agg({\"is_fraud\":\"sum\",\"amt\":\"sum\"}).reset_index()\n    df[\"is_fraud\"]=df[\"is_fraud\"].apply(lambda x:1 if x&gt;0 else 0)\n    \n    G=nx.from_edgelist(df[[\"from\",\"to\"]].values, create_using=graph_type)\n    \n    nx.set_edge_attributes(G,{(int(x[\"from\"]),int(x[\"to\"])):x[\"is_fraud\"] for idx, x in df[[\"from\",\"to\",\"is_fraud\"]].iterrows()}, \"label\")  #엣지 속성 설정,각 속성의 사기 여부부     \n    nx.set_edge_attributes(G,{(int(x[\"from\"]),int(x[\"to\"])):x[\"amt\"] for idx,x in df[[\"from\",\"to\",\"amt\"]].iterrows()}, \"weight\") # 엣지 속성 설정, 각 엣지의 거래 금액\n\n    return G\n\n\ndef build_graph_tripartite(df_input, graph_type=nx.Graph()):\n    df=df_input.copy()\n    mapping={x:node_id for node_id, x in enumerate(set(df.index.values.tolist() + \n                                                       df[\"cc_num\"].values.tolist() +\n                                                       df[\"merchant\"].values.tolist()))}\n    df[\"in_node\"]= df[\"cc_num\"].apply(lambda x: mapping[x])\n    df[\"out_node\"]=df[\"merchant\"].apply(lambda x:mapping[x])\n    \n        \n    G=nx.from_edgelist([(x[\"in_node\"], mapping[idx]) for idx, x in df.iterrows()] +\\\n                        [(x[\"out_node\"], mapping[idx]) for idx, x in df.iterrows()], create_using=graph_type)\n    \n    nx.set_edge_attributes(G,{(x[\"in_node\"], mapping[idx]):x[\"is_fraud\"] for idx, x in df.iterrows()}, \"label\")     \n    nx.set_edge_attributes(G,{(x[\"out_node\"], mapping[idx]):x[\"is_fraud\"] for idx, x in df.iterrows()}, \"label\")   \n    nx.set_edge_attributes(G,{(x[\"in_node\"], mapping[idx]):x[\"amt\"] for idx, x in df.iterrows()}, \"weight\")  \n    nx.set_edge_attributes(G,{(x[\"out_node\"], mapping[idx]):x[\"amt\"] for idx, x in df.iterrows()}, \"weight\")\n\n    return G\n    \n    \ndef down_sample_textbook(df):\n    df_majority = df[df.is_fraud==0].copy()\n    df_minority = df[df.is_fraud==1].copy()\n    df_maj_dowsampled = sklearn.utils.resample(df_majority, n_samples=len(df_minority), replace=False, random_state=42)\n    df_downsampled = pd.concat([df_minority, df_maj_dowsampled])\n    return df_downsampled\n\ndef embedding(Graph):\n    # Graph -&gt; X (feature)\n    _edgs = list(Graph.edges)\n    subGraph = Graph.edge_subgraph([_edgs[x] for x in range(len(Graph.edges))]).copy()\n    subGraph.add_nodes_from(list(set(Graph.nodes) - set(subGraph.nodes)))    \n    embedded = AverageEmbedder(Node2Vec(subGraph, weight_key='weight').fit(window=10).wv)\n    X = [embedded[str(_edgs[x][0]), str(_edgs[x][1])] for x in range(len(Graph.edges))]\n    # Graph -&gt; y (label)\n    y = np.array(list(nx.get_edge_attributes(Graph, \"label\").values()))\n    return X,y \n\ndef anal(df):\n    Graph = build_graph_bipartite(df)\n    X,XX,y,yy = embedding(Graph)\n    lrnr = RandomForestClassifier(n_estimators=100, random_state=42) \n    lrnr.fit(X,y)\n    yyhat = lrnr.predict(XX)\n    df = pd.DataFrame({\n        'acc':[sklearn.metrics.accuracy_score(yy,yyhat)], \n        'pre':[sklearn.metrics.precision_score(yy,yyhat)], \n        'rec':[sklearn.metrics.recall_score(yy,yyhat)],\n        'f1':[sklearn.metrics.f1_score(yy,yyhat)]}\n    )    \n    return df\n\ndef our_sampling1(df):\n    cus_list = set(df.query('is_fraud==1').cc_num.tolist())\n    return df.query(\"cc_num in @ cus_list\")\n\n- 모든엣지를 고려\n\nN = 10 \nedge_index = torch.tensor([[i,j] for i in range(N) for j in range(N)]).T\n# edge_attr = 그래프의 웨이트 \n\n\nedge_index\n\ntensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2,\n         2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4,\n         4, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 7, 7,\n         7, 7, 7, 7, 7, 7, 7, 7, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 9, 9, 9, 9, 9, 9,\n         9, 9, 9, 9],\n        [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 2, 3,\n         4, 5, 6, 7, 8, 9, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 2, 3, 4, 5, 6, 7,\n         8, 9, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1,\n         2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 2, 3, 4, 5,\n         6, 7, 8, 9]])\n\n\n\nfraudTrain = pd.read_csv(\"~/Desktop/fraudTrain.csv\").iloc[:,1:]\n\n\nfraudTrain = fraudTrain.assign(trans_date_trans_time= list(map(lambda x: pd.to_datetime(x), fraudTrain.trans_date_trans_time)))\nfraudTrain\n\n\n\n\n\n\n\n\ntrans_date_trans_time\ncc_num\nmerchant\ncategory\namt\nfirst\nlast\ngender\nstreet\ncity\n...\nlat\nlong\ncity_pop\njob\ndob\ntrans_num\nunix_time\nmerch_lat\nmerch_long\nis_fraud\n\n\n\n\n0\n2019-01-01 00:00:00\n2.703190e+15\nfraud_Rippin, Kub and Mann\nmisc_net\n4.97\nJennifer\nBanks\nF\n561 Perry Cove\nMoravian Falls\n...\n36.0788\n-81.1781\n3495\nPsychologist, counselling\n1988-03-09\n0b242abb623afc578575680df30655b9\n1325376018\n36.011293\n-82.048315\n0\n\n\n1\n2019-01-01 00:00:00\n6.304230e+11\nfraud_Heller, Gutmann and Zieme\ngrocery_pos\n107.23\nStephanie\nGill\nF\n43039 Riley Greens Suite 393\nOrient\n...\n48.8878\n-118.2105\n149\nSpecial educational needs teacher\n1978-06-21\n1f76529f8574734946361c461b024d99\n1325376044\n49.159047\n-118.186462\n0\n\n\n2\n2019-01-01 00:00:00\n3.885950e+13\nfraud_Lind-Buckridge\nentertainment\n220.11\nEdward\nSanchez\nM\n594 White Dale Suite 530\nMalad City\n...\n42.1808\n-112.2620\n4154\nNature conservation officer\n1962-01-19\na1a22d70485983eac12b5b88dad1cf95\n1325376051\n43.150704\n-112.154481\n0\n\n\n3\n2019-01-01 00:01:00\n3.534090e+15\nfraud_Kutch, Hermiston and Farrell\ngas_transport\n45.00\nJeremy\nWhite\nM\n9443 Cynthia Court Apt. 038\nBoulder\n...\n46.2306\n-112.1138\n1939\nPatent attorney\n1967-01-12\n6b849c168bdad6f867558c3793159a81\n1325376076\n47.034331\n-112.561071\n0\n\n\n4\n2019-01-01 00:03:00\n3.755340e+14\nfraud_Keeling-Crist\nmisc_pos\n41.96\nTyler\nGarcia\nM\n408 Bradley Rest\nDoe Hill\n...\n38.4207\n-79.4629\n99\nDance movement psychotherapist\n1986-03-28\na41d7549acf90789359a9aa5346dcb46\n1325376186\n38.674999\n-78.632459\n0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n1048570\n2020-03-10 16:07:00\n6.011980e+15\nfraud_Fadel Inc\nhealth_fitness\n77.00\nHaley\nWagner\nF\n05561 Farrell Crescent\nAnnapolis\n...\n39.0305\n-76.5515\n92106\nAccountant, chartered certified\n1943-05-28\n45ecd198c65e81e597db22e8d2ef7361\n1362931649\n38.779464\n-76.317042\n0\n\n\n1048571\n2020-03-10 16:07:00\n4.839040e+15\nfraud_Cremin, Hamill and Reichel\nmisc_pos\n116.94\nMeredith\nCampbell\nF\n043 Hanson Turnpike\nHedrick\n...\n41.1826\n-92.3097\n1583\nGeochemist\n1999-06-28\nc00ce51c6ebb7657474a77b9e0b51f34\n1362931670\n41.400318\n-92.726724\n0\n\n\n1048572\n2020-03-10 16:08:00\n5.718440e+11\nfraud_O'Connell, Botsford and Hand\nhome\n21.27\nSusan\nMills\nF\n005 Cody Estates\nLouisville\n...\n38.2507\n-85.7476\n736284\nEngineering geologist\n1952-04-02\n17c9dc8b2a6449ca2473726346e58e6c\n1362931711\n37.293339\n-84.798122\n0\n\n\n1048573\n2020-03-10 16:08:00\n4.646850e+18\nfraud_Thompson-Gleason\nhealth_fitness\n9.52\nJulia\nBell\nF\n576 House Crossroad\nWest Sayville\n...\n40.7320\n-73.1000\n4056\nFilm/video editor\n1990-06-25\n5ca650881b48a6a38754f841c23b77ab\n1362931718\n39.773077\n-72.213209\n0\n\n\n1048574\n2020-03-10 16:08:00\n2.283740e+15\nfraud_Buckridge PLC\nmisc_pos\n6.81\nShannon\nWilliams\nF\n9345 Spencer Junctions Suite 183\nAlpharetta\n...\n34.0770\n-84.3033\n165556\nPrison officer\n1997-12-27\n8d0a575fe635bbde12f1a2bffc126731\n1362931730\n33.601468\n-83.891921\n0\n\n\n\n\n1048575 rows × 22 columns\n\n\n\n\ndiff = fraudTrain.trans_date_trans_time[101]-fraudTrain.trans_date_trans_time[0]\n\n\ndiff\n\nTimedelta('0 days 01:17:00')\n\n\n\ndiff.total_seconds()\n\n4620.0\n\n\n\ntheta = 86400*1.2\ntheta\n\n103680.0\n\n\n\ntheta = 86400*1.2\nnp.exp(-diff.total_seconds()/theta)\n\n0.9564180361647693\n\n\n\n# !git add .\n# !git commit -m. \n# !git push \n# !quarto publish --no-browser --no-prompt\n\n\n\n\nfraudTrain = fraudTrain.assign(trans_date_trans_time= list(map(lambda x: pd.to_datetime(x), fraudTrain.trans_date_trans_time)))\nfraudTrain\n\n\n\n\n\n\n\n\ntrans_date_trans_time\ncc_num\nmerchant\ncategory\namt\nfirst\nlast\ngender\nstreet\ncity\n...\nlat\nlong\ncity_pop\njob\ndob\ntrans_num\nunix_time\nmerch_lat\nmerch_long\nis_fraud\n\n\n\n\n0\n2019-01-01 00:00:00\n2.703190e+15\nfraud_Rippin, Kub and Mann\nmisc_net\n4.97\nJennifer\nBanks\nF\n561 Perry Cove\nMoravian Falls\n...\n36.0788\n-81.1781\n3495\nPsychologist, counselling\n1988-03-09\n0b242abb623afc578575680df30655b9\n1325376018\n36.011293\n-82.048315\n0\n\n\n1\n2019-01-01 00:00:00\n6.304230e+11\nfraud_Heller, Gutmann and Zieme\ngrocery_pos\n107.23\nStephanie\nGill\nF\n43039 Riley Greens Suite 393\nOrient\n...\n48.8878\n-118.2105\n149\nSpecial educational needs teacher\n1978-06-21\n1f76529f8574734946361c461b024d99\n1325376044\n49.159047\n-118.186462\n0\n\n\n2\n2019-01-01 00:00:00\n3.885950e+13\nfraud_Lind-Buckridge\nentertainment\n220.11\nEdward\nSanchez\nM\n594 White Dale Suite 530\nMalad City\n...\n42.1808\n-112.2620\n4154\nNature conservation officer\n1962-01-19\na1a22d70485983eac12b5b88dad1cf95\n1325376051\n43.150704\n-112.154481\n0\n\n\n3\n2019-01-01 00:01:00\n3.534090e+15\nfraud_Kutch, Hermiston and Farrell\ngas_transport\n45.00\nJeremy\nWhite\nM\n9443 Cynthia Court Apt. 038\nBoulder\n...\n46.2306\n-112.1138\n1939\nPatent attorney\n1967-01-12\n6b849c168bdad6f867558c3793159a81\n1325376076\n47.034331\n-112.561071\n0\n\n\n4\n2019-01-01 00:03:00\n3.755340e+14\nfraud_Keeling-Crist\nmisc_pos\n41.96\nTyler\nGarcia\nM\n408 Bradley Rest\nDoe Hill\n...\n38.4207\n-79.4629\n99\nDance movement psychotherapist\n1986-03-28\na41d7549acf90789359a9aa5346dcb46\n1325376186\n38.674999\n-78.632459\n0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n1048570\n2020-03-10 16:07:00\n6.011980e+15\nfraud_Fadel Inc\nhealth_fitness\n77.00\nHaley\nWagner\nF\n05561 Farrell Crescent\nAnnapolis\n...\n39.0305\n-76.5515\n92106\nAccountant, chartered certified\n1943-05-28\n45ecd198c65e81e597db22e8d2ef7361\n1362931649\n38.779464\n-76.317042\n0\n\n\n1048571\n2020-03-10 16:07:00\n4.839040e+15\nfraud_Cremin, Hamill and Reichel\nmisc_pos\n116.94\nMeredith\nCampbell\nF\n043 Hanson Turnpike\nHedrick\n...\n41.1826\n-92.3097\n1583\nGeochemist\n1999-06-28\nc00ce51c6ebb7657474a77b9e0b51f34\n1362931670\n41.400318\n-92.726724\n0\n\n\n1048572\n2020-03-10 16:08:00\n5.718440e+11\nfraud_O'Connell, Botsford and Hand\nhome\n21.27\nSusan\nMills\nF\n005 Cody Estates\nLouisville\n...\n38.2507\n-85.7476\n736284\nEngineering geologist\n1952-04-02\n17c9dc8b2a6449ca2473726346e58e6c\n1362931711\n37.293339\n-84.798122\n0\n\n\n1048573\n2020-03-10 16:08:00\n4.646850e+18\nfraud_Thompson-Gleason\nhealth_fitness\n9.52\nJulia\nBell\nF\n576 House Crossroad\nWest Sayville\n...\n40.7320\n-73.1000\n4056\nFilm/video editor\n1990-06-25\n5ca650881b48a6a38754f841c23b77ab\n1362931718\n39.773077\n-72.213209\n0\n\n\n1048574\n2020-03-10 16:08:00\n2.283740e+15\nfraud_Buckridge PLC\nmisc_pos\n6.81\nShannon\nWilliams\nF\n9345 Spencer Junctions Suite 183\nAlpharetta\n...\n34.0770\n-84.3033\n165556\nPrison officer\n1997-12-27\n8d0a575fe635bbde12f1a2bffc126731\n1362931730\n33.601468\n-83.891921\n0\n\n\n\n\n1048575 rows × 22 columns\n\n\n\n\nN = len(fraudTrain)\nN\n\n1048575\n\n\n- 시도1\n\nedge_index_list = []\nfor i in range(N):\n    for j in range(N):\n        time_difference = (fraudTrain['trans_date_trans_time'][i] - fraudTrain['trans_date_trans_time'][j]).total_seconds()\n        edge_index_list.append([i, j, time_difference])\n\n\nedge_index = torch.tensor(edge_index_list).T\n\n- 시도2\n\nN = len(fraudTrain)\nedge_index = torch.tensor([[i,j] for i in range(N) for j in range(N)]).T\n# edge_attr = 그래프의 웨이트 \n\n\n너~무 오래걸린다.\n\n- 시도3\n\ndf02을 이용해서 해보자.\n\n\n_df1 = fraudTrain[fraudTrain[\"is_fraud\"] == 0].sample(frac=0.20, random_state=42)\n_df2 = fraudTrain[fraudTrain[\"is_fraud\"] == 1]\ndf02 = pd.concat([_df1,_df2])\ndf02.shape\n\n(214520, 22)\n\n\n\n214520*214520\n\n46018830400\n\n\n\n# N = len(df02)\n# edge_index = torch.tensor([[i,j] for i in range(N) for j in range(N)]).T\n\n- 시도4: df50\n\ndf50 = down_sample_textbook(df02)\ndf50.shape\n\n(12012, 22)\n\n\n\n12012*12012\n\n144288144\n\n\n\ndf50 = df50.reset_index()\n\n\ndf50_tr,df50_test = sklearn.model_selection.train_test_split(df50, random_state=42)\n\n\ndf50_tr.is_fraud.mean().round(5), df50_test.is_fraud.mean().round(5)\n\n(0.49828, 0.50516)\n\n\n\ndf50_tr.shape, df50_test.shape\n\n((9009, 23), (3003, 23))\n\n\n\nN = len(df50_tr)\nedge_index = torch.tensor([[i,j] for i in range(N) for j in range(N)]).T\nedge_index\n\ntensor([[   0,    0,    0,  ..., 9008, 9008, 9008],\n        [   0,    1,    2,  ..., 9006, 9007, 9008]])\n\n\n\ndf50_tr = df50_tr.reset_index()\n\n\nedge_index_list = []\nfor i in range(N):\n    for j in range(N):\n        time_difference = (df50_tr['trans_date_trans_time'][i] - df50_tr['trans_date_trans_time'][j]).total_seconds()\n        edge_index_list.append([i, j, time_difference])\n\n\nedge_index = np.array(edge_index_list)\n\n\nedge_index.shape\n\n(81162081, 3)\n\n\n\nedge_index[:,2] = np.abs(edge_index[:,2])\n\n\ntheta = edge_index[:,2].mean()\ntheta\n\n12230796.273867842\n\n\n\nedge_index[:,2] = (np.exp(-edge_index[:,2]/theta)!=1) * np.exp(-edge_index[:,2]/theta)\n\n\nedge_index\n\narray([[0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n       [0.00000000e+00, 1.00000000e+00, 1.90157975e-01],\n       [0.00000000e+00, 2.00000000e+00, 9.79646259e-02],\n       ...,\n       [9.00800000e+03, 9.00600000e+03, 6.60662164e-01],\n       [9.00800000e+03, 9.00700000e+03, 1.49150646e-01],\n       [9.00800000e+03, 9.00800000e+03, 0.00000000e+00]])\n\n\n\nedge_index_list_updated = edge_index.tolist()\n\n\nedge_index_list_updated[:5]\n\n[[0.0, 0.0, 0.0],\n [0.0, 1.0, 0.19015797528259762],\n [0.0, 2.0, 0.09796462590589798],\n [0.0, 3.0, 0.1424157407389685],\n [0.0, 4.0, 0.11107338192969567]]\n\n\n\ndf50_tr\n\n\n\n\n\n\n\n\nlevel_0\nindex\ntrans_date_trans_time\ncc_num\nmerchant\ncategory\namt\nfirst\nlast\ngender\n...\nlat\nlong\ncity_pop\njob\ndob\ntrans_num\nunix_time\nmerch_lat\nmerch_long\nis_fraud\n\n\n\n\n0\n476\n51331\n2019-01-31 00:44:00\n3.543590e+15\nfraud_Medhurst PLC\nshopping_net\n921.24\nMargaret\nLam\nF\n...\n40.4603\n-79.0097\n922\nEarly years teacher\n1972-10-04\nc8928ba53be26fdd997b26f7130c757e\n1327970678\n40.064488\n-78.210499\n1\n\n\n1\n3671\n625691\n2019-09-23 00:09:00\n2.610530e+15\nfraud_Torphy-Goyette\nshopping_pos\n698.28\nTanya\nDickerson\nF\n...\n36.2416\n-86.6117\n22191\nPrison officer\n1994-07-27\n90453290b765904ed1c3426882a6788b\n1348358993\n35.884288\n-87.513318\n1\n\n\n2\n6641\n896244\n2019-12-25 21:30:00\n6.011330e+15\nfraud_Monahan-Morar\npersonal_care\n220.56\nLauren\nButler\nF\n...\n36.0557\n-96.0602\n413574\nTeacher, special educational needs\n1971-09-01\n4072a3effcf51cf7cf88f69d00642cd9\n1356471044\n35.789798\n-95.859736\n0\n\n\n3\n4288\n717690\n2019-11-02 22:22:00\n6.011380e+15\nfraud_Daugherty, Pouros and Beahan\nshopping_pos\n905.43\nMartin\nDuarte\nM\n...\n44.6001\n-84.2931\n864\nGeneral practice doctor\n1942-05-04\nf2fa1b25eef2f43fa5c09e3e1bfe7f77\n1351894926\n44.652759\n-84.500359\n1\n\n\n4\n4770\n815813\n2019-12-08 02:50:00\n4.430880e+15\nfraud_Hudson-Ratke\ngrocery_pos\n307.98\nAlicia\nMorales\nF\n...\n39.3199\n-106.6596\n61\nPublic relations account executive\n1939-11-04\nf06eff8da349e36e623cff026de8e970\n1354935056\n38.389399\n-106.111026\n1\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n9004\n11964\n177703\n2019-04-02 21:48:00\n3.572980e+15\nfraud_Ziemann-Waters\nhealth_fitness\n63.89\nWilliam\nLopez\nM\n...\n41.1832\n-96.9882\n614\nAssociate Professor\n1967-06-20\n5b19aad28d65a6b0a912fa7b9d1896de\n1333403300\n42.067169\n-96.876892\n0\n\n\n9005\n5191\n921796\n2019-12-30 23:29:00\n6.762920e+11\nfraud_Wiza, Schaden and Stark\nmisc_pos\n51.41\nLisa\nFitzpatrick\nF\n...\n41.2336\n-75.2389\n104\nFinancial trader\n1927-08-25\nb2a9e44026fc57e54b4e45ade6017668\n1356910178\n40.502189\n-74.814956\n1\n\n\n9006\n5390\n950365\n2020-01-16 03:15:00\n4.807550e+12\nfraud_Murray-Smitham\ngrocery_pos\n357.62\nKimberly\nCastro\nF\n...\n40.2158\n-83.9579\n133\nProfessor Emeritus\n1954-01-29\n4bfa37c329f327074e7220ea6e5d8f8d\n1358306148\n40.620284\n-84.274495\n1\n\n\n9007\n860\n88685\n2019-02-22 02:19:00\n5.738600e+11\nfraud_McDermott-Weimann\ngrocery_pos\n304.75\nCristian\nJones\nM\n...\n42.0765\n-87.7246\n27020\nTrade mark attorney\n1986-07-23\na1c3025ddb615ab2ef890bf82fc3d66a\n1329877195\n42.722479\n-88.362364\n1\n\n\n9008\n7270\n753787\n2019-11-18 10:58:00\n6.042293e+10\nfraud_Terry, Johns and Bins\nmisc_pos\n1.64\nJeffrey\nPowers\nM\n...\n33.6028\n-81.9748\n46944\nSecondary school teacher\n1942-04-02\nee10d61782bde2b5cabc2ad649e977cc\n1353236287\n34.243599\n-82.971344\n0\n\n\n\n\n9009 rows × 24 columns\n\n\n\n\ncc_num로 그룹별로 묶자.\n\n\ndf50_tr[df50_tr['cc_num']==3.543590e+15]\n\n\n\n\n\n\n\n\nlevel_0\nindex\ntrans_date_trans_time\ncc_num\nmerchant\ncategory\namt\nfirst\nlast\ngender\n...\nlat\nlong\ncity_pop\njob\ndob\ntrans_num\nunix_time\nmerch_lat\nmerch_long\nis_fraud\n\n\n\n\n0\n476\n51331\n2019-01-31 00:44:00\n3.543590e+15\nfraud_Medhurst PLC\nshopping_net\n921.24\nMargaret\nLam\nF\n...\n40.4603\n-79.0097\n922\nEarly years teacher\n1972-10-04\nc8928ba53be26fdd997b26f7130c757e\n1327970678\n40.064488\n-78.210499\n1\n\n\n344\n462\n50905\n2019-01-30 16:53:00\n3.543590e+15\nfraud_Lesch Ltd\nshopping_pos\n881.11\nMargaret\nLam\nF\n...\n40.4603\n-79.0097\n922\nEarly years teacher\n1972-10-04\n9f7b7675c4decefd03cce56df045ed1c\n1327942400\n39.591484\n-79.575246\n1\n\n\n1377\n6607\n814736\n2019-12-07 22:17:00\n3.543590e+15\nfraud_Botsford and Sons\nhome\n10.41\nMargaret\nLam\nF\n...\n40.4603\n-79.0097\n922\nEarly years teacher\n1972-10-04\naa9b533e84970309a4ad60a914a8cd77\n1354918668\n41.287791\n-79.980592\n0\n\n\n1447\n485\n51816\n2019-01-31 12:38:00\n3.543590e+15\nfraud_Ruecker-Mayert\nkids_pets\n21.93\nMargaret\nLam\nF\n...\n40.4603\n-79.0097\n922\nEarly years teacher\n1972-10-04\ncec656f154e0978b0f26702c29ddeeca\n1328013517\n39.946187\n-78.078864\n1\n\n\n1639\n11176\n12947\n2019-01-08 11:08:00\n3.543590e+15\nfraud_Stroman, Hudson and Erdman\ngas_transport\n76.03\nMargaret\nLam\nF\n...\n40.4603\n-79.0097\n922\nEarly years teacher\n1972-10-04\nc10451edc4e21b865d049312acf18ecd\n1326020892\n39.503960\n-78.471680\n0\n\n\n2046\n8124\n627045\n2019-09-23 12:53:00\n3.543590e+15\nfraud_Botsford Ltd\nshopping_pos\n3.20\nMargaret\nLam\nF\n...\n40.4603\n-79.0097\n922\nEarly years teacher\n1972-10-04\n003d591d208f7ee52277b5cc4fa4a37f\n1348404838\n40.066686\n-79.326630\n0\n\n\n2093\n477\n51367\n2019-01-31 01:36:00\n3.543590e+15\nfraud_Watsica, Haag and Considine\nshopping_pos\n1090.67\nMargaret\nLam\nF\n...\n40.4603\n-79.0097\n922\nEarly years teacher\n1972-10-04\nb42bc0820a78de54845c5138b9c39dd5\n1327973774\n40.923284\n-78.882504\n1\n\n\n2415\n491\n52402\n2019-01-31 22:17:00\n3.543590e+15\nfraud_Metz, Russel and Metz\nkids_pets\n22.35\nMargaret\nLam\nF\n...\n40.4603\n-79.0097\n922\nEarly years teacher\n1972-10-04\n51f9352216e99bbe9e8b03b082305971\n1328048275\n39.979547\n-78.851379\n1\n\n\n2625\n463\n51047\n2019-01-30 19:35:00\n3.543590e+15\nfraud_Ruecker-Mayert\nkids_pets\n22.95\nMargaret\nLam\nF\n...\n40.4603\n-79.0097\n922\nEarly years teacher\n1972-10-04\n8e804422b761537e3a49a237afd1ea9a\n1327952100\n40.051981\n-79.021769\n1\n\n\n2769\n478\n51374\n2019-01-31 01:42:00\n3.543590e+15\nfraud_Schmidt and Sons\nshopping_net\n1043.59\nMargaret\nLam\nF\n...\n40.4603\n-79.0097\n922\nEarly years teacher\n1972-10-04\nbbe4e9e431cba66e6531199ffaf79657\n1327974178\n40.192896\n-79.366393\n1\n\n\n3192\n505\n52522\n2019-01-31 23:57:00\n3.543590e+15\nfraud_Kutch, Steuber and Gerhold\nfood_dining\n116.45\nMargaret\nLam\nF\n...\n40.4603\n-79.0097\n922\nEarly years teacher\n1972-10-04\nfcf46ca0264437bbb938c29eca2c92ad\n1328054256\n40.288401\n-78.286914\n1\n\n\n3670\n11714\n1010269\n2020-02-20 06:02:00\n3.543590e+15\nfraud_Huels-Hahn\ngas_transport\n51.80\nMargaret\nLam\nF\n...\n40.4603\n-79.0097\n922\nEarly years teacher\n1972-10-04\nb72c0124f4c5662db13e1bea2f04784b\n1361340164\n39.672719\n-79.642589\n0\n\n\n3945\n6087\n243892\n2019-05-02 13:38:00\n3.543590e+15\nfraud_Cruickshank-Mills\nentertainment\n5.72\nMargaret\nLam\nF\n...\n40.4603\n-79.0097\n922\nEarly years teacher\n1972-10-04\n990da059d387e5fa7481d76ff5c29199\n1335965925\n40.577553\n-79.315460\n0\n\n\n5017\n484\n51431\n2019-01-31 03:28:00\n3.543590e+15\nfraud_Cremin, Hamill and Reichel\nmisc_pos\n741.98\nMargaret\nLam\nF\n...\n40.4603\n-79.0097\n922\nEarly years teacher\n1972-10-04\n41312d7d5fc76be3782b5e9cef04726f\n1327980509\n41.290570\n-79.682069\n1\n\n\n5505\n8148\n181398\n2019-04-04 23:32:00\n3.543590e+15\nfraud_Feil, Hilpert and Koss\nfood_dining\n89.23\nMargaret\nLam\nF\n...\n40.4603\n-79.0097\n922\nEarly years teacher\n1972-10-04\ne304fd4ebc897fce190925dadcd2b524\n1333582347\n39.736380\n-79.481667\n0\n\n\n5729\n11116\n329202\n2019-06-06 03:26:00\n3.543590e+15\nfraud_Connelly, Reichert and Fritsch\ngas_transport\n69.36\nMargaret\nLam\nF\n...\n40.4603\n-79.0097\n922\nEarly years teacher\n1972-10-04\nbc0832ac8bac6d26548ab6ab553d5d5e\n1338953171\n40.780469\n-79.668417\n0\n\n\n7605\n481\n51392\n2019-01-31 02:16:00\n3.543590e+15\nfraud_Huels-Hahn\ngas_transport\n12.41\nMargaret\nLam\nF\n...\n40.4603\n-79.0097\n922\nEarly years teacher\n1972-10-04\n5f4379c2fc20457f0f99a126cadda1af\n1327976216\n39.884234\n-79.374966\n1\n\n\n7800\n8609\n55920\n2019-02-03 06:51:00\n3.543590e+15\nfraud_Corwin-Gorczany\nmisc_net\n6.70\nMargaret\nLam\nF\n...\n40.4603\n-79.0097\n922\nEarly years teacher\n1972-10-04\n90f33381b6b6644c6d03c8cdb51d05dc\n1328251865\n40.064532\n-78.920283\n0\n\n\n8100\n10488\n509733\n2019-08-09 11:47:00\n3.543590e+15\nfraud_Kutch and Sons\ngrocery_pos\n108.74\nMargaret\nLam\nF\n...\n40.4603\n-79.0097\n922\nEarly years teacher\n1972-10-04\n40a620cc7c5ba396b1fe112f5361e4a9\n1344512838\n40.057443\n-78.569798\n0\n\n\n8313\n504\n52514\n2019-01-31 23:52:00\n3.543590e+15\nfraud_Douglas, Schneider and Turner\nshopping_pos\n1129.56\nMargaret\nLam\nF\n...\n40.4603\n-79.0097\n922\nEarly years teacher\n1972-10-04\nec208107f178422e0953560343d0cf8b\n1328053975\n40.840340\n-78.027854\n1\n\n\n\n\n20 rows × 24 columns\n\n\n\n\ndf50_grouped=df50_tr.groupby(by='cc_num')\n\n\nedge_index_list = []\nfor i in range(N):\n    for j in range(N):\n        if df50_tr['cc_num'][i] != df50_tr['cc_num'][j]:  # cc_num 값이 같다면\n            time_difference = 0\n        else:\n            time_difference = (df50_tr['trans_date_trans_time'][i] - df50_tr['trans_date_trans_time'][j]).total_seconds()\n        edge_index_list.append([i, j, time_difference])\n\n\nedge_index = np.array(edge_index_list)\n\n\nedge_index.shape\n\n(81162081, 3)\n\n\n\nedge_index\n\narray([[0.000e+00, 0.000e+00, 0.000e+00],\n       [0.000e+00, 1.000e+00, 0.000e+00],\n       [0.000e+00, 2.000e+00, 0.000e+00],\n       ...,\n       [9.008e+03, 9.006e+03, 0.000e+00],\n       [9.008e+03, 9.007e+03, 0.000e+00],\n       [9.008e+03, 9.008e+03, 0.000e+00]])\n\n\n\nedge_index[:,2] = np.abs(edge_index[:,2])\n\n\ntheta = edge_index[:,2].mean()\ntheta\n\n10988.585252761077\n\n\n\nedge_index\n\narray([[0.000e+00, 0.000e+00, 0.000e+00],\n       [0.000e+00, 1.000e+00, 0.000e+00],\n       [0.000e+00, 2.000e+00, 0.000e+00],\n       ...,\n       [9.008e+03, 9.006e+03, 0.000e+00],\n       [9.008e+03, 9.007e+03, 0.000e+00],\n       [9.008e+03, 9.008e+03, 0.000e+00]])\n\n\n\nedge_index[:,2] = (np.exp(-edge_index[:,2]/theta)!=1) * np.exp(-edge_index[:,2]/theta)\n\n\nedge_index\n\narray([[0.00000000e+00, 0.00000000e+00, 9.99909001e-01],\n       [0.00000000e+00, 1.00000000e+00, 9.99909001e-01],\n       [0.00000000e+00, 2.00000000e+00, 9.99909001e-01],\n       ...,\n       [9.00800000e+03, 9.00600000e+03, 9.99909001e-01],\n       [9.00800000e+03, 9.00700000e+03, 9.99909001e-01],\n       [9.00800000e+03, 9.00800000e+03, 9.99909001e-01]])\n\n\n\nedge_index_list_updated = edge_index.tolist()\n\n\nedge_index_list_updated[:5]\n\n[[0.0, 0.0, 0.9999090006150367],\n [0.0, 1.0, 0.9999090006150367],\n [0.0, 2.0, 0.9999090006150367],\n [0.0, 3.0, 0.9999090006150367],\n [0.0, 4.0, 0.9999090006150367]]"
  },
  {
    "objectID": "posts/사기거래/2023-07-19-그래프자료로 데이터정리.out.html#해보자",
    "href": "posts/사기거래/2023-07-19-그래프자료로 데이터정리.out.html#해보자",
    "title": "[FRAUD] 그래프자료로 데이터정리",
    "section": "",
    "text": "fraudTrain = fraudTrain.assign(trans_date_trans_time= list(map(lambda x: pd.to_datetime(x), fraudTrain.trans_date_trans_time)))\nfraudTrain\n\n\n\n\n\n\n\n\ntrans_date_trans_time\ncc_num\nmerchant\ncategory\namt\nfirst\nlast\ngender\nstreet\ncity\n...\nlat\nlong\ncity_pop\njob\ndob\ntrans_num\nunix_time\nmerch_lat\nmerch_long\nis_fraud\n\n\n\n\n0\n2019-01-01 00:00:00\n2.703190e+15\nfraud_Rippin, Kub and Mann\nmisc_net\n4.97\nJennifer\nBanks\nF\n561 Perry Cove\nMoravian Falls\n...\n36.0788\n-81.1781\n3495\nPsychologist, counselling\n1988-03-09\n0b242abb623afc578575680df30655b9\n1325376018\n36.011293\n-82.048315\n0\n\n\n1\n2019-01-01 00:00:00\n6.304230e+11\nfraud_Heller, Gutmann and Zieme\ngrocery_pos\n107.23\nStephanie\nGill\nF\n43039 Riley Greens Suite 393\nOrient\n...\n48.8878\n-118.2105\n149\nSpecial educational needs teacher\n1978-06-21\n1f76529f8574734946361c461b024d99\n1325376044\n49.159047\n-118.186462\n0\n\n\n2\n2019-01-01 00:00:00\n3.885950e+13\nfraud_Lind-Buckridge\nentertainment\n220.11\nEdward\nSanchez\nM\n594 White Dale Suite 530\nMalad City\n...\n42.1808\n-112.2620\n4154\nNature conservation officer\n1962-01-19\na1a22d70485983eac12b5b88dad1cf95\n1325376051\n43.150704\n-112.154481\n0\n\n\n3\n2019-01-01 00:01:00\n3.534090e+15\nfraud_Kutch, Hermiston and Farrell\ngas_transport\n45.00\nJeremy\nWhite\nM\n9443 Cynthia Court Apt. 038\nBoulder\n...\n46.2306\n-112.1138\n1939\nPatent attorney\n1967-01-12\n6b849c168bdad6f867558c3793159a81\n1325376076\n47.034331\n-112.561071\n0\n\n\n4\n2019-01-01 00:03:00\n3.755340e+14\nfraud_Keeling-Crist\nmisc_pos\n41.96\nTyler\nGarcia\nM\n408 Bradley Rest\nDoe Hill\n...\n38.4207\n-79.4629\n99\nDance movement psychotherapist\n1986-03-28\na41d7549acf90789359a9aa5346dcb46\n1325376186\n38.674999\n-78.632459\n0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n1048570\n2020-03-10 16:07:00\n6.011980e+15\nfraud_Fadel Inc\nhealth_fitness\n77.00\nHaley\nWagner\nF\n05561 Farrell Crescent\nAnnapolis\n...\n39.0305\n-76.5515\n92106\nAccountant, chartered certified\n1943-05-28\n45ecd198c65e81e597db22e8d2ef7361\n1362931649\n38.779464\n-76.317042\n0\n\n\n1048571\n2020-03-10 16:07:00\n4.839040e+15\nfraud_Cremin, Hamill and Reichel\nmisc_pos\n116.94\nMeredith\nCampbell\nF\n043 Hanson Turnpike\nHedrick\n...\n41.1826\n-92.3097\n1583\nGeochemist\n1999-06-28\nc00ce51c6ebb7657474a77b9e0b51f34\n1362931670\n41.400318\n-92.726724\n0\n\n\n1048572\n2020-03-10 16:08:00\n5.718440e+11\nfraud_O'Connell, Botsford and Hand\nhome\n21.27\nSusan\nMills\nF\n005 Cody Estates\nLouisville\n...\n38.2507\n-85.7476\n736284\nEngineering geologist\n1952-04-02\n17c9dc8b2a6449ca2473726346e58e6c\n1362931711\n37.293339\n-84.798122\n0\n\n\n1048573\n2020-03-10 16:08:00\n4.646850e+18\nfraud_Thompson-Gleason\nhealth_fitness\n9.52\nJulia\nBell\nF\n576 House Crossroad\nWest Sayville\n...\n40.7320\n-73.1000\n4056\nFilm/video editor\n1990-06-25\n5ca650881b48a6a38754f841c23b77ab\n1362931718\n39.773077\n-72.213209\n0\n\n\n1048574\n2020-03-10 16:08:00\n2.283740e+15\nfraud_Buckridge PLC\nmisc_pos\n6.81\nShannon\nWilliams\nF\n9345 Spencer Junctions Suite 183\nAlpharetta\n...\n34.0770\n-84.3033\n165556\nPrison officer\n1997-12-27\n8d0a575fe635bbde12f1a2bffc126731\n1362931730\n33.601468\n-83.891921\n0\n\n\n\n\n1048575 rows × 22 columns\n\n\n\n\nN = len(fraudTrain)\nN\n\n1048575\n\n\n- 시도1\n\nedge_index_list = []\nfor i in range(N):\n    for j in range(N):\n        time_difference = (fraudTrain['trans_date_trans_time'][i] - fraudTrain['trans_date_trans_time'][j]).total_seconds()\n        edge_index_list.append([i, j, time_difference])\n\n\nedge_index = torch.tensor(edge_index_list).T\n\n- 시도2\n\nN = len(fraudTrain)\nedge_index = torch.tensor([[i,j] for i in range(N) for j in range(N)]).T\n# edge_attr = 그래프의 웨이트 \n\n\n너~무 오래걸린다.\n\n- 시도3\n\ndf02을 이용해서 해보자.\n\n\n_df1 = fraudTrain[fraudTrain[\"is_fraud\"] == 0].sample(frac=0.20, random_state=42)\n_df2 = fraudTrain[fraudTrain[\"is_fraud\"] == 1]\ndf02 = pd.concat([_df1,_df2])\ndf02.shape\n\n(214520, 22)\n\n\n\n214520*214520\n\n46018830400\n\n\n\n# N = len(df02)\n# edge_index = torch.tensor([[i,j] for i in range(N) for j in range(N)]).T\n\n- 시도4: df50\n\ndf50 = down_sample_textbook(df02)\ndf50.shape\n\n(12012, 22)\n\n\n\n12012*12012\n\n144288144\n\n\n\ndf50 = df50.reset_index()\n\n\ndf50_tr,df50_test = sklearn.model_selection.train_test_split(df50, random_state=42)\n\n\ndf50_tr.is_fraud.mean().round(5), df50_test.is_fraud.mean().round(5)\n\n(0.49828, 0.50516)\n\n\n\ndf50_tr.shape, df50_test.shape\n\n((9009, 23), (3003, 23))\n\n\n\nN = len(df50_tr)\nedge_index = torch.tensor([[i,j] for i in range(N) for j in range(N)]).T\nedge_index\n\ntensor([[   0,    0,    0,  ..., 9008, 9008, 9008],\n        [   0,    1,    2,  ..., 9006, 9007, 9008]])\n\n\n\ndf50_tr = df50_tr.reset_index()\n\n\nedge_index_list = []\nfor i in range(N):\n    for j in range(N):\n        time_difference = (df50_tr['trans_date_trans_time'][i] - df50_tr['trans_date_trans_time'][j]).total_seconds()\n        edge_index_list.append([i, j, time_difference])\n\n\nedge_index = np.array(edge_index_list)\n\n\nedge_index.shape\n\n(81162081, 3)\n\n\n\nedge_index[:,2] = np.abs(edge_index[:,2])\n\n\ntheta = edge_index[:,2].mean()\ntheta\n\n12230796.273867842\n\n\n\nedge_index[:,2] = (np.exp(-edge_index[:,2]/theta)!=1) * np.exp(-edge_index[:,2]/theta)\n\n\nedge_index\n\narray([[0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n       [0.00000000e+00, 1.00000000e+00, 1.90157975e-01],\n       [0.00000000e+00, 2.00000000e+00, 9.79646259e-02],\n       ...,\n       [9.00800000e+03, 9.00600000e+03, 6.60662164e-01],\n       [9.00800000e+03, 9.00700000e+03, 1.49150646e-01],\n       [9.00800000e+03, 9.00800000e+03, 0.00000000e+00]])\n\n\n\nedge_index_list_updated = edge_index.tolist()\n\n\nedge_index_list_updated[:5]\n\n[[0.0, 0.0, 0.0],\n [0.0, 1.0, 0.19015797528259762],\n [0.0, 2.0, 0.09796462590589798],\n [0.0, 3.0, 0.1424157407389685],\n [0.0, 4.0, 0.11107338192969567]]\n\n\n\ndf50_tr\n\n\n\n\n\n\n\n\nlevel_0\nindex\ntrans_date_trans_time\ncc_num\nmerchant\ncategory\namt\nfirst\nlast\ngender\n...\nlat\nlong\ncity_pop\njob\ndob\ntrans_num\nunix_time\nmerch_lat\nmerch_long\nis_fraud\n\n\n\n\n0\n476\n51331\n2019-01-31 00:44:00\n3.543590e+15\nfraud_Medhurst PLC\nshopping_net\n921.24\nMargaret\nLam\nF\n...\n40.4603\n-79.0097\n922\nEarly years teacher\n1972-10-04\nc8928ba53be26fdd997b26f7130c757e\n1327970678\n40.064488\n-78.210499\n1\n\n\n1\n3671\n625691\n2019-09-23 00:09:00\n2.610530e+15\nfraud_Torphy-Goyette\nshopping_pos\n698.28\nTanya\nDickerson\nF\n...\n36.2416\n-86.6117\n22191\nPrison officer\n1994-07-27\n90453290b765904ed1c3426882a6788b\n1348358993\n35.884288\n-87.513318\n1\n\n\n2\n6641\n896244\n2019-12-25 21:30:00\n6.011330e+15\nfraud_Monahan-Morar\npersonal_care\n220.56\nLauren\nButler\nF\n...\n36.0557\n-96.0602\n413574\nTeacher, special educational needs\n1971-09-01\n4072a3effcf51cf7cf88f69d00642cd9\n1356471044\n35.789798\n-95.859736\n0\n\n\n3\n4288\n717690\n2019-11-02 22:22:00\n6.011380e+15\nfraud_Daugherty, Pouros and Beahan\nshopping_pos\n905.43\nMartin\nDuarte\nM\n...\n44.6001\n-84.2931\n864\nGeneral practice doctor\n1942-05-04\nf2fa1b25eef2f43fa5c09e3e1bfe7f77\n1351894926\n44.652759\n-84.500359\n1\n\n\n4\n4770\n815813\n2019-12-08 02:50:00\n4.430880e+15\nfraud_Hudson-Ratke\ngrocery_pos\n307.98\nAlicia\nMorales\nF\n...\n39.3199\n-106.6596\n61\nPublic relations account executive\n1939-11-04\nf06eff8da349e36e623cff026de8e970\n1354935056\n38.389399\n-106.111026\n1\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n9004\n11964\n177703\n2019-04-02 21:48:00\n3.572980e+15\nfraud_Ziemann-Waters\nhealth_fitness\n63.89\nWilliam\nLopez\nM\n...\n41.1832\n-96.9882\n614\nAssociate Professor\n1967-06-20\n5b19aad28d65a6b0a912fa7b9d1896de\n1333403300\n42.067169\n-96.876892\n0\n\n\n9005\n5191\n921796\n2019-12-30 23:29:00\n6.762920e+11\nfraud_Wiza, Schaden and Stark\nmisc_pos\n51.41\nLisa\nFitzpatrick\nF\n...\n41.2336\n-75.2389\n104\nFinancial trader\n1927-08-25\nb2a9e44026fc57e54b4e45ade6017668\n1356910178\n40.502189\n-74.814956\n1\n\n\n9006\n5390\n950365\n2020-01-16 03:15:00\n4.807550e+12\nfraud_Murray-Smitham\ngrocery_pos\n357.62\nKimberly\nCastro\nF\n...\n40.2158\n-83.9579\n133\nProfessor Emeritus\n1954-01-29\n4bfa37c329f327074e7220ea6e5d8f8d\n1358306148\n40.620284\n-84.274495\n1\n\n\n9007\n860\n88685\n2019-02-22 02:19:00\n5.738600e+11\nfraud_McDermott-Weimann\ngrocery_pos\n304.75\nCristian\nJones\nM\n...\n42.0765\n-87.7246\n27020\nTrade mark attorney\n1986-07-23\na1c3025ddb615ab2ef890bf82fc3d66a\n1329877195\n42.722479\n-88.362364\n1\n\n\n9008\n7270\n753787\n2019-11-18 10:58:00\n6.042293e+10\nfraud_Terry, Johns and Bins\nmisc_pos\n1.64\nJeffrey\nPowers\nM\n...\n33.6028\n-81.9748\n46944\nSecondary school teacher\n1942-04-02\nee10d61782bde2b5cabc2ad649e977cc\n1353236287\n34.243599\n-82.971344\n0\n\n\n\n\n9009 rows × 24 columns\n\n\n\n\ncc_num로 그룹별로 묶자.\n\n\ndf50_tr[df50_tr['cc_num']==3.543590e+15]\n\n\n\n\n\n\n\n\nlevel_0\nindex\ntrans_date_trans_time\ncc_num\nmerchant\ncategory\namt\nfirst\nlast\ngender\n...\nlat\nlong\ncity_pop\njob\ndob\ntrans_num\nunix_time\nmerch_lat\nmerch_long\nis_fraud\n\n\n\n\n0\n476\n51331\n2019-01-31 00:44:00\n3.543590e+15\nfraud_Medhurst PLC\nshopping_net\n921.24\nMargaret\nLam\nF\n...\n40.4603\n-79.0097\n922\nEarly years teacher\n1972-10-04\nc8928ba53be26fdd997b26f7130c757e\n1327970678\n40.064488\n-78.210499\n1\n\n\n344\n462\n50905\n2019-01-30 16:53:00\n3.543590e+15\nfraud_Lesch Ltd\nshopping_pos\n881.11\nMargaret\nLam\nF\n...\n40.4603\n-79.0097\n922\nEarly years teacher\n1972-10-04\n9f7b7675c4decefd03cce56df045ed1c\n1327942400\n39.591484\n-79.575246\n1\n\n\n1377\n6607\n814736\n2019-12-07 22:17:00\n3.543590e+15\nfraud_Botsford and Sons\nhome\n10.41\nMargaret\nLam\nF\n...\n40.4603\n-79.0097\n922\nEarly years teacher\n1972-10-04\naa9b533e84970309a4ad60a914a8cd77\n1354918668\n41.287791\n-79.980592\n0\n\n\n1447\n485\n51816\n2019-01-31 12:38:00\n3.543590e+15\nfraud_Ruecker-Mayert\nkids_pets\n21.93\nMargaret\nLam\nF\n...\n40.4603\n-79.0097\n922\nEarly years teacher\n1972-10-04\ncec656f154e0978b0f26702c29ddeeca\n1328013517\n39.946187\n-78.078864\n1\n\n\n1639\n11176\n12947\n2019-01-08 11:08:00\n3.543590e+15\nfraud_Stroman, Hudson and Erdman\ngas_transport\n76.03\nMargaret\nLam\nF\n...\n40.4603\n-79.0097\n922\nEarly years teacher\n1972-10-04\nc10451edc4e21b865d049312acf18ecd\n1326020892\n39.503960\n-78.471680\n0\n\n\n2046\n8124\n627045\n2019-09-23 12:53:00\n3.543590e+15\nfraud_Botsford Ltd\nshopping_pos\n3.20\nMargaret\nLam\nF\n...\n40.4603\n-79.0097\n922\nEarly years teacher\n1972-10-04\n003d591d208f7ee52277b5cc4fa4a37f\n1348404838\n40.066686\n-79.326630\n0\n\n\n2093\n477\n51367\n2019-01-31 01:36:00\n3.543590e+15\nfraud_Watsica, Haag and Considine\nshopping_pos\n1090.67\nMargaret\nLam\nF\n...\n40.4603\n-79.0097\n922\nEarly years teacher\n1972-10-04\nb42bc0820a78de54845c5138b9c39dd5\n1327973774\n40.923284\n-78.882504\n1\n\n\n2415\n491\n52402\n2019-01-31 22:17:00\n3.543590e+15\nfraud_Metz, Russel and Metz\nkids_pets\n22.35\nMargaret\nLam\nF\n...\n40.4603\n-79.0097\n922\nEarly years teacher\n1972-10-04\n51f9352216e99bbe9e8b03b082305971\n1328048275\n39.979547\n-78.851379\n1\n\n\n2625\n463\n51047\n2019-01-30 19:35:00\n3.543590e+15\nfraud_Ruecker-Mayert\nkids_pets\n22.95\nMargaret\nLam\nF\n...\n40.4603\n-79.0097\n922\nEarly years teacher\n1972-10-04\n8e804422b761537e3a49a237afd1ea9a\n1327952100\n40.051981\n-79.021769\n1\n\n\n2769\n478\n51374\n2019-01-31 01:42:00\n3.543590e+15\nfraud_Schmidt and Sons\nshopping_net\n1043.59\nMargaret\nLam\nF\n...\n40.4603\n-79.0097\n922\nEarly years teacher\n1972-10-04\nbbe4e9e431cba66e6531199ffaf79657\n1327974178\n40.192896\n-79.366393\n1\n\n\n3192\n505\n52522\n2019-01-31 23:57:00\n3.543590e+15\nfraud_Kutch, Steuber and Gerhold\nfood_dining\n116.45\nMargaret\nLam\nF\n...\n40.4603\n-79.0097\n922\nEarly years teacher\n1972-10-04\nfcf46ca0264437bbb938c29eca2c92ad\n1328054256\n40.288401\n-78.286914\n1\n\n\n3670\n11714\n1010269\n2020-02-20 06:02:00\n3.543590e+15\nfraud_Huels-Hahn\ngas_transport\n51.80\nMargaret\nLam\nF\n...\n40.4603\n-79.0097\n922\nEarly years teacher\n1972-10-04\nb72c0124f4c5662db13e1bea2f04784b\n1361340164\n39.672719\n-79.642589\n0\n\n\n3945\n6087\n243892\n2019-05-02 13:38:00\n3.543590e+15\nfraud_Cruickshank-Mills\nentertainment\n5.72\nMargaret\nLam\nF\n...\n40.4603\n-79.0097\n922\nEarly years teacher\n1972-10-04\n990da059d387e5fa7481d76ff5c29199\n1335965925\n40.577553\n-79.315460\n0\n\n\n5017\n484\n51431\n2019-01-31 03:28:00\n3.543590e+15\nfraud_Cremin, Hamill and Reichel\nmisc_pos\n741.98\nMargaret\nLam\nF\n...\n40.4603\n-79.0097\n922\nEarly years teacher\n1972-10-04\n41312d7d5fc76be3782b5e9cef04726f\n1327980509\n41.290570\n-79.682069\n1\n\n\n5505\n8148\n181398\n2019-04-04 23:32:00\n3.543590e+15\nfraud_Feil, Hilpert and Koss\nfood_dining\n89.23\nMargaret\nLam\nF\n...\n40.4603\n-79.0097\n922\nEarly years teacher\n1972-10-04\ne304fd4ebc897fce190925dadcd2b524\n1333582347\n39.736380\n-79.481667\n0\n\n\n5729\n11116\n329202\n2019-06-06 03:26:00\n3.543590e+15\nfraud_Connelly, Reichert and Fritsch\ngas_transport\n69.36\nMargaret\nLam\nF\n...\n40.4603\n-79.0097\n922\nEarly years teacher\n1972-10-04\nbc0832ac8bac6d26548ab6ab553d5d5e\n1338953171\n40.780469\n-79.668417\n0\n\n\n7605\n481\n51392\n2019-01-31 02:16:00\n3.543590e+15\nfraud_Huels-Hahn\ngas_transport\n12.41\nMargaret\nLam\nF\n...\n40.4603\n-79.0097\n922\nEarly years teacher\n1972-10-04\n5f4379c2fc20457f0f99a126cadda1af\n1327976216\n39.884234\n-79.374966\n1\n\n\n7800\n8609\n55920\n2019-02-03 06:51:00\n3.543590e+15\nfraud_Corwin-Gorczany\nmisc_net\n6.70\nMargaret\nLam\nF\n...\n40.4603\n-79.0097\n922\nEarly years teacher\n1972-10-04\n90f33381b6b6644c6d03c8cdb51d05dc\n1328251865\n40.064532\n-78.920283\n0\n\n\n8100\n10488\n509733\n2019-08-09 11:47:00\n3.543590e+15\nfraud_Kutch and Sons\ngrocery_pos\n108.74\nMargaret\nLam\nF\n...\n40.4603\n-79.0097\n922\nEarly years teacher\n1972-10-04\n40a620cc7c5ba396b1fe112f5361e4a9\n1344512838\n40.057443\n-78.569798\n0\n\n\n8313\n504\n52514\n2019-01-31 23:52:00\n3.543590e+15\nfraud_Douglas, Schneider and Turner\nshopping_pos\n1129.56\nMargaret\nLam\nF\n...\n40.4603\n-79.0097\n922\nEarly years teacher\n1972-10-04\nec208107f178422e0953560343d0cf8b\n1328053975\n40.840340\n-78.027854\n1\n\n\n\n\n20 rows × 24 columns\n\n\n\n\ndf50_grouped=df50_tr.groupby(by='cc_num')\n\n\nedge_index_list = []\nfor i in range(N):\n    for j in range(N):\n        if df50_tr['cc_num'][i] != df50_tr['cc_num'][j]:  # cc_num 값이 같다면\n            time_difference = 0\n        else:\n            time_difference = (df50_tr['trans_date_trans_time'][i] - df50_tr['trans_date_trans_time'][j]).total_seconds()\n        edge_index_list.append([i, j, time_difference])\n\n\nedge_index = np.array(edge_index_list)\n\n\nedge_index.shape\n\n(81162081, 3)\n\n\n\nedge_index\n\narray([[0.000e+00, 0.000e+00, 0.000e+00],\n       [0.000e+00, 1.000e+00, 0.000e+00],\n       [0.000e+00, 2.000e+00, 0.000e+00],\n       ...,\n       [9.008e+03, 9.006e+03, 0.000e+00],\n       [9.008e+03, 9.007e+03, 0.000e+00],\n       [9.008e+03, 9.008e+03, 0.000e+00]])\n\n\n\nedge_index[:,2] = np.abs(edge_index[:,2])\n\n\ntheta = edge_index[:,2].mean()\ntheta\n\n10988.585252761077\n\n\n\nedge_index\n\narray([[0.000e+00, 0.000e+00, 0.000e+00],\n       [0.000e+00, 1.000e+00, 0.000e+00],\n       [0.000e+00, 2.000e+00, 0.000e+00],\n       ...,\n       [9.008e+03, 9.006e+03, 0.000e+00],\n       [9.008e+03, 9.007e+03, 0.000e+00],\n       [9.008e+03, 9.008e+03, 0.000e+00]])\n\n\n\nedge_index[:,2] = (np.exp(-edge_index[:,2]/theta)!=1) * np.exp(-edge_index[:,2]/theta)\n\n\nedge_index\n\narray([[0.00000000e+00, 0.00000000e+00, 9.99909001e-01],\n       [0.00000000e+00, 1.00000000e+00, 9.99909001e-01],\n       [0.00000000e+00, 2.00000000e+00, 9.99909001e-01],\n       ...,\n       [9.00800000e+03, 9.00600000e+03, 9.99909001e-01],\n       [9.00800000e+03, 9.00700000e+03, 9.99909001e-01],\n       [9.00800000e+03, 9.00800000e+03, 9.99909001e-01]])\n\n\n\nedge_index_list_updated = edge_index.tolist()\n\n\nedge_index_list_updated[:5]\n\n[[0.0, 0.0, 0.9999090006150367],\n [0.0, 1.0, 0.9999090006150367],\n [0.0, 2.0, 0.9999090006150367],\n [0.0, 3.0, 0.9999090006150367],\n [0.0, 4.0, 0.9999090006150367]]"
  },
  {
    "objectID": "posts/2022_04_23_중간고사예상문제.html",
    "href": "posts/2022_04_23_중간고사예상문제.html",
    "title": "중간고사 예상문제",
    "section": "",
    "text": "import numpy as np\nimport tensorflow as tf\nimport tensorflow.experimental.numpy as tnp\nimport matplotlib.pyplot as plt\ntnp.experimental_enable_numpy_behavior()"
  },
  {
    "objectID": "posts/2022_04_23_중간고사예상문제.html#경사하강법과-tf.gradienttape의-사용방법-30점",
    "href": "posts/2022_04_23_중간고사예상문제.html#경사하강법과-tf.gradienttape의-사용방법-30점",
    "title": "중간고사 예상문제",
    "section": "1. 경사하강법과 tf.GradientTape()의 사용방법 (30점)",
    "text": "1. 경사하강법과 tf.GradientTape()의 사용방법 (30점)\n(1) 아래는 \\(X_i \\overset{iid}{\\sim} N(3,2^2)\\) 를 생성하는 코드이다. (10점)\n\ntf.random.set_seed(43052)\nx= tnp.random.randn(10000)*2+3\nx\n\n&lt;tf.Tensor: shape=(10000,), dtype=float64, numpy=\narray([ 4.12539849,  5.46696729,  5.27243374, ...,  2.89712332,\n        5.01072291, -1.13050477])&gt;\n\n\n함수 \\(L(\\mu,\\sigma)\\)을 최대화하는 \\((\\mu,\\sigma)\\)를 tf.GradeintTape()를 활용하여 추정하라. (경사하강법 혹은 경사상승법을 사용하고 \\(\\mu\\)의 초기값은 2로 \\(\\sigma\\)의 초기값은 3으로 설정할 것)\n\\[L(\\mu,\\sigma)=\\prod_{i=1}^{n}f(x_i), \\quad f(x_i)=\\frac{1}{\\sqrt{2\\pi}\\sigma}e^{-\\frac{1}{2}(\\frac{x_i-\\mu}{\\sigma})^2}\\]\nhint: \\(L(\\mu,\\sigma)\\)를 최대화하는 \\((\\mu,\\sigma)\\)는 \\(\\log L(\\mu,\\sigma)\\)를 역시 최대화한다는 사실을 이용할 것.\nhint: \\(\\mu\\)의 참값은 3, \\(\\sigma\\)의 참값은 2이다. (따라서 \\(\\mu\\)와 \\(\\sigma\\)는 각각 2와 3근처로 추정되어야 한다.)\n\nmu = tf.Variable(2.0)\nsigma = tf.Variable(3.0)\n\n\nwith tf.GradientTape() as tape:\n    pdf = 1/sigma * tnp.exp(-0.5*((x-mu)/sigma)**2)\n    log = tf.reduce_sum(tnp.log(pdf))\ntape.gradient(log, [mu, sigma])\n\n[&lt;tf.Tensor: shape=(), dtype=float32, numpy=1129.3353&gt;,\n &lt;tf.Tensor: shape=(), dtype=float32, numpy=-1488.3431&gt;]\n\n\n\nfor i in range(1000):\n    with tf.GradientTape() as tape:\n        pdf = 1/sigma * tnp.exp(-0.5*((x-mu)/sigma)**2)\n        log = tf.reduce_sum(tnp.log(pdf))\n    slope1, slope2 = tape.gradient(log, [mu, sigma])\n    mu.assign_add(slope1 * 0.1 / 10000)\n    sigma.assign_add(slope2 * 0.1 / 10000)\n\n(2)\n(3)"
  },
  {
    "objectID": "posts/2022_04_23_중간고사예상문제.html#회귀분석의-이론적해와-tf.keras.optimizer-이용방법-20점",
    "href": "posts/2022_04_23_중간고사예상문제.html#회귀분석의-이론적해와-tf.keras.optimizer-이용방법-20점",
    "title": "중간고사 예상문제",
    "section": "2. 회귀분석의 이론적해와 tf.keras.optimizer 이용방법 (20점)",
    "text": "2. 회귀분석의 이론적해와 tf.keras.optimizer 이용방법 (20점)\n아래와 같은 선형모형을 고려하자.\n\\[y_i = \\beta_0 + \\beta_1 x_i +\\epsilon_i.\\]\n이때 오차항은 정규분포로 가정한다. 즉 \\(\\epsilon_i \\overset{iid}{\\sim} N(0,\\sigma^2)\\)라고 가정한다.\n관측데이터가 아래와 같을때 아래의 물음에 답하라.\n\nx= tnp.array([20.1, 22.2, 22.7, 23.3, 24.4, 25.1, 26.2, 27.3, 28.4, 30.4])\ny= tnp.array([55.4183651 , 58.19427589, 61.23082496, 62.31255873, 63.1070028 ,\n              63.69569103, 67.24704918, 71.43650092, 73.10130336, 77.84988286])\n# X= tnp.array([[1.0, 20.1], [1.0, 22.2], [1.0, 22.7], [1.0, 23.3], [1.0, 24.4],\n#               [1.0, 25.1], [1.0, 26.2], [1.0, 27.3], [1.0, 28.4], [1.0, 30.4]])\n\n(1) MSE loss를 최소화 하는 \\(\\beta_0,\\beta_1\\)의 해석해를 구하라.\n(2) 경사하강법과 MSE loss의 도함수를 이용하여 \\(\\beta_0,\\beta_1\\)을 추정하라.\n주의 tf.GradeintTape()를 이용하지 말고 MSE loss의 해석적 도함수를 사용할 것.\n(3) tf.keras.optimizers의 apply_gradients()를 이용하여 \\(\\beta_0,\\beta_1\\)을 추정하라.\n(4) tf.keras.optimizers의 minimize()를 이용하여 \\(\\beta_0,\\beta_1\\)을 추정하라.\nhint1 alpha=0.0015로 설정할 것\nhint2 epoc은 10000번정도 반복실행하며 적당한 횟수를 찾을 것\nhint3 (1)의 최적값에 반드시 정확히 수렴시킬 필요는 없음 (너무 많은 에폭이 소모됨)\nhint4 초기값으로 [5,10] 정도 이용할 것"
  },
  {
    "objectID": "posts/2022_04_23_중간고사예상문제.html#keras를-이용한-풀이-30점",
    "href": "posts/2022_04_23_중간고사예상문제.html#keras를-이용한-풀이-30점",
    "title": "중간고사 예상문제",
    "section": "3. keras를 이용한 풀이 (30점)",
    "text": "3. keras를 이용한 풀이 (30점)\n(1) 아래와 같은 모형을 고려하자.\n\\[y_i= \\beta_0 + \\sum_{k=1}^{5} \\beta_k \\cos(k t_i)+\\epsilon_i, \\quad i=0,1,\\dots, 999\\]\n여기에서 \\(t_i=\\frac{2\\pi i}{1000}\\) 이다. 그리고 \\(\\epsilon_i \\sim i.i.d~ N(0,\\sigma^2)\\), 즉 서로 독립인 표준정규분포에서 추출된 샘플이다. 위의 모형에서 아래와 같은 데이터를 관측했다고 가정하자.\n\nnp.random.seed(43052)\nt= np.array(range(1000))* np.pi/1000\ny = -2+ 3*np.cos(t) + 1*np.cos(2*t) + 0.5*np.cos(5*t) + np.random.randn(1000)*0.2\nplt.plot(t,y,'.',alpha=0.2)\n\n\n\n\ntf.keras를 이용하여 \\(\\beta_0,\\dots,\\beta_5\\)를 추정하라. (\\(\\beta_0,\\dots,\\beta_5\\)의 참값은 각각 -2,3,1,0,0,0.5 이다)\n(2) 아래와 같은 모형을 고려하자.\n\\[y_i \\sim Ber(\\pi_i), ~ \\text{where} ~ \\pi_i=\\frac{\\exp(w_0+w_1x_i)}{1+\\exp(w_0+w_1x_i)}\\]\n위의 모형에서 관측한 데이터는 아래와 같다.\n\ntf.random.set_seed(43052)\nx = tnp.linspace(-1,1,2000)\ny = tf.constant(np.random.binomial(1, tf.nn.sigmoid(-1+5*x)),dtype=tf.float64)\nplt.plot(x,y,'.',alpha=0.05)\n\n\n\n\ntf.keras를 이용하여 \\(w_0,w_1\\)을 추정하라. (참고: \\(w_0, w_1\\)에 대한 참값은 -1과 5이다.)"
  },
  {
    "objectID": "posts/2022_04_23_중간고사예상문제.html#piecewise-linear-regression-15점",
    "href": "posts/2022_04_23_중간고사예상문제.html#piecewise-linear-regression-15점",
    "title": "중간고사 예상문제",
    "section": "4. Piecewise-linear regression (15점)",
    "text": "4. Piecewise-linear regression (15점)"
  },
  {
    "objectID": "posts/2022_04_23_중간고사예상문제.html#다음을-잘-읽고-참과-거짓을-판단하라.-5점",
    "href": "posts/2022_04_23_중간고사예상문제.html#다음을-잘-읽고-참과-거짓을-판단하라.-5점",
    "title": "중간고사 예상문제",
    "section": "5. 다음을 잘 읽고 참과 거짓을 판단하라. (5점)",
    "text": "5. 다음을 잘 읽고 참과 거짓을 판단하라. (5점)\n(1) 적절한 학습률이 선택된다면, 경사하강법은 손실함수가 convex일때 언제 전역최소해를 찾을 수 있다.\n(2)\n(3)\n(4)\n(5)\n\nsome notes\n- 용어를 모르겠는 분은 질문하시기 바랍니다.\n- 풀다가 에러나는 코드 질문하면 에러 수정해드립니다."
  },
  {
    "objectID": "posts/hadamangeo/coin prediction.html",
    "href": "posts/hadamangeo/coin prediction.html",
    "title": "미래 예측 데이터 분석(비트코인 시세 예측)",
    "section": "",
    "text": "python-data-analysis data\nData Source\n이것이 데이터 분석이다 with 파이썬\n\n%matplotlib inline\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")"
  },
  {
    "objectID": "posts/hadamangeo/coin prediction.html#arima",
    "href": "posts/hadamangeo/coin prediction.html#arima",
    "title": "미래 예측 데이터 분석(비트코인 시세 예측)",
    "section": "ARIMA",
    "text": "ARIMA\n\nAR:자기 자신의 과거를 정보로 사용. &gt; 현재의 상태는 이전의 상태를 참고해서 계산\nMA: 이전 항에서의 오차를 이용해 현재 항의 상태 추론\nARMA: AR+MA\nARIMA: ARMA모델에 추세 변동 경향성 반영\n\n- 만약 ARIMA클래스 order=(2,1,2)이라면\n\n첫번째 2: AR이 몇번째 과거까지 바라보는지\n두번째 1: 차분(difference) - 현재 상태의 변수에서 바로 전 상태의 변수를 빼줌(경향성)\n세번째 2: MA가 몇번째 과거까지 바라보는지\n\n\nfrom statsmodels.tsa.arima.model import ARIMA\n\nimport statsmodels.api as sm"
  },
  {
    "objectID": "posts/hadamangeo/coin prediction.html#학습",
    "href": "posts/hadamangeo/coin prediction.html#학습",
    "title": "미래 예측 데이터 분석(비트코인 시세 예측)",
    "section": "학습",
    "text": "학습\n\n\nmodel = ARIMA(bitcoin_df.price.values, order=(2, 1, 2))\nmodel_fit = model.fit()\nprint(model_fit.summary())\n\n                               SARIMAX Results                                \n==============================================================================\nDep. Variable:                      y   No. Observations:                  365\nModel:                 ARIMA(2, 1, 2)   Log Likelihood               -2787.490\nDate:                Thu, 18 May 2023   AIC                           5584.980\nTime:                        14:55:27   BIC                           5604.466\nSample:                             0   HQIC                          5592.725\n                                - 365                                         \nCovariance Type:                  opg                                         \n==============================================================================\n                 coef    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nar.L1          0.2418      0.305      0.794      0.427      -0.355       0.839\nar.L2          0.6072      0.200      3.038      0.002       0.215       0.999\nma.L1         -0.1513      0.306     -0.495      0.621      -0.751       0.448\nma.L2         -0.6705      0.205     -3.279      0.001      -1.071      -0.270\nsigma2      2.624e+05   1.05e+04     24.899      0.000    2.42e+05    2.83e+05\n===================================================================================\nLjung-Box (L1) (Q):                   0.07   Jarque-Bera (JB):               721.96\nProb(Q):                              0.79   Prob(JB):                         0.00\nHeteroskedasticity (H):               0.14   Skew:                             0.01\nProb(H) (two-sided):                  0.00   Kurtosis:                         9.90\n===================================================================================\n\nWarnings:\n[1] Covariance matrix calculated using the outer product of gradients (complex-step).\n\n\nfig = model_fit.plot_predict() # 학습 데이터에 대한 예측 결과입니다. (첫번째 그래프)\nresiduals = pd.DataFrame(model_fit.resid) # 잔차의 변동을 시각화합니다. (두번째 그래프)\nresiduals.plot()\nforecast_data = model_fit.forecast(steps=5) # 학습 데이터셋으로부터 5일 뒤를 예측합니다.\n\n# 테스트 데이터셋을 불러옵니다.\ntest_file_path = '../data/market-price-test.csv'\nbitcoin_test_df = pd.read_csv(test_file_path, names=['ds', 'y'])\n\npred_y = forecast_data[0].tolist() # 마지막 5일의 예측 데이터입니다. (2018-08-27 ~ 2018-08-31)\ntest_y = bitcoin_test_df.y.values # 실제 5일 가격 데이터입니다. (2018-08-27 ~ 2018-08-31)\npred_y_lower = [] # 마지막 5일의 예측 데이터의 최소값입니다.\npred_y_upper = [] # 마지막 5일의 예측 데이터의 최대값입니다.\nfor lower_upper in forecast_data[2]:\n    lower = lower_upper[0]\n    upper = lower_upper[1]\n    pred_y_lower.append(lower)\n    pred_y_upper.append(upper)\n\n\nplt.plot(pred_y, color=\"gold\") # 모델이 예상한 가격 그래프입니다.\nplt.plot(pred_y_lower, color=\"red\") # 모델이 예상한 최소가격 그래프입니다.\nplt.plot(pred_y_upper, color=\"blue\") # 모델이 예상한 최대가격 그래프입니다.\nplt.plot(test_y, color=\"green\") # 실제 가격 그래프입니다.\n\n\nplt.plot(pred_y, color=\"gold\") # 모델이 예상한 가격 그래프입니다.\nplt.plot(test_y, color=\"green\") # 실제 가격 그래프입니다.\n\n\nfrom sklearn.metrics import mean_squared_error, r2_score\nfrom math import sqrt\n\nrmse = sqrt(mean_squared_error(pred_y, test_y))\nprint(rmse)"
  },
  {
    "objectID": "posts/hadamangeo/coin prediction.html#상한가-및-하한가-선정",
    "href": "posts/hadamangeo/coin prediction.html#상한가-및-하한가-선정",
    "title": "미래 예측 데이터 분석(비트코인 시세 예측)",
    "section": "상한가 및 하한가 선정",
    "text": "상한가 및 하한가 선정\n\nbitcoin_df = pd.read_csv(file_path, names=['ds', 'y'])\n\n# 상한가를 설정합니다.\nbitcoin_df['cap'] = 20000\n\n# 상한가 적용을 위한 파라미터를 다음과 같이 설정합니다.\nprophet = Prophet(seasonality_mode='multiplicative', \n                  growth='logistic',\n                  yearly_seasonality=True,\n                  weekly_seasonality=True, daily_seasonality=True,\n                  changepoint_prior_scale=0.5)\nprophet.fit(bitcoin_df)\n\n\n\n# 5일을 내다보며 예측합니다.\nfuture_data = prophet.make_future_dataframe(periods=5, freq='d')\n\n# 상한가를 설정합니다.\nfuture_data['cap'] = 20000\nforecast_data = prophet.predict(future_data)\n\n\nfig = prophet.plot(forecast_data)\n\n\nbitcoin_test_df = pd.read_csv(test_file_path, names=['ds', 'y'])\n\n# 모델이 예상한 마지막 5일의 가격 데이터를 가져옵니다.\npred_y = forecast_data.yhat.values[-5:]\ntest_y = bitcoin_test_df.y.values\npred_y_lower = forecast_data.yhat_lower.values[-5:]\npred_y_upper = forecast_data.yhat_upper.values[-5:]\n\n\nplt.plot(pred_y, color=\"gold\") # 모델이 예상한 가격 그래프입니다.\nplt.plot(pred_y_lower, color=\"red\") # 모델이 예상한 최소가격 그래프입니다.\nplt.plot(pred_y_upper, color=\"blue\") # 모델이 예상한 최대가격 그래프입니다.\nplt.plot(test_y, color=\"green\") # 실제 가격 그래프입니다.\n\n\nplt.plot(pred_y, color=\"gold\") # 모델이 예상한 가격 그래프입니다.\nplt.plot(test_y, color=\"green\") # 실제 가격 그래프입니다.\n\n\nrmse = sqrt(mean_squared_error(pred_y, test_y))\nprint(rmse)"
  },
  {
    "objectID": "posts/hadamangeo/coin prediction.html#이상치-제거",
    "href": "posts/hadamangeo/coin prediction.html#이상치-제거",
    "title": "미래 예측 데이터 분석(비트코인 시세 예측)",
    "section": "이상치 제거",
    "text": "이상치 제거\n\n# 18000 이상의 데이터는 이상치라고 판단\nbitcoin_df = pd.read_csv(file_path, names=['ds', 'y'])\nbitcoin_df.loc[bitcoin_df['y'] &gt; 18000, 'y'] = None\n\n\n# prophet 모델을 학습합니다.\nprophet = Prophet(seasonality_mode='multiplicative',\n                  yearly_seasonality=True,\n                  weekly_seasonality=True, daily_seasonality=True,\n                  changepoint_prior_scale=0.5)\nprophet.fit(bitcoin_df)\n\n# 5일단위의 미래를 예측합니다.\nfuture_data = prophet.make_future_dataframe(periods=5, freq='d')\nforecast_data = prophet.predict(future_data)\n\n# 예측 결과를 그래프로 출력합니다.\nfig = prophet.plot(forecast_data)\n\n\nbitcoin_test_df = pd.read_csv(test_file_path, names = ['ds', 'y'])\n\n# 모델이 예상한 마지막 5일의 가격 데이터를 가져옵니다.\npred_y = forecast_data.yhat.values[-5:]\ntest_y = bitcoin_test_df.y.values\npred_y_lower = forecast_data.yhat_lower.values[-5:]\npred_y_upper = forecast_data.yhat_upper.values[-5:]\n\n\nplt.plot(pred_y, color=\"gold\") # 모델이 예상한 가격 그래프입니다.\nplt.plot(pred_y_lower, color=\"red\") # 모델이 예상한 최소가격 그래프입니다.\nplt.plot(pred_y_upper, color=\"blue\") # 모델이 예상한 최대가격 그래프입니다.\nplt.plot(test_y, color=\"green\") # 실제 가격 그래프입니다.\n\n\nplt.plot(pred_y, color=\"gold\") # 모델이 예상한 가격 그래프입니다.\nplt.plot(test_y, color=\"green\") # 실제 가격 그래프입니다.\n\n\n# 테스트 데이터의 RMSE를 출력합니다.\nrmse = sqrt(mean_squared_error(pred_y, test_y))\nprint(rmse)"
  },
  {
    "objectID": "posts/hadamangeo/coin prediction.html#arima-모델의-하이퍼-파라미터-선정하기",
    "href": "posts/hadamangeo/coin prediction.html#arima-모델의-하이퍼-파라미터-선정하기",
    "title": "미래 예측 데이터 분석(비트코인 시세 예측)",
    "section": "ARIMA 모델의 하이퍼 파라미터 선정하기",
    "text": "ARIMA 모델의 하이퍼 파라미터 선정하기\n\nbitcoin_df = pd.read_csv(file_path, names = ['day', 'price'])\nbitcoin_df['day'] = pd.to_datetime(bitcoin_df['day'])\nbitcoin_df.index = bitcoin_df['day']\nbitcoin_df.set_index('day', inplace=True)\n\n\nfrom statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n\nplot_acf(bitcoin_df.price.values)\nplot_pacf(bitcoin_df.price.values)\nplt.show()\n\n\nmodel = ARIMA(bitcoin_df.price.values, order=(15,2,0))\nmodel_fit = model.fit(trend='c',full_output=True, disp=True)\nprint(model_fit.summary())\n\n\nmodel = ARIMA(bitcoin_df.price.values, order=(2,1,2))\nmodel_fit = model.fit(trend='c',full_output=True, disp=True)\nprint(model_fit.summary())"
  },
  {
    "objectID": "posts/hadamangeo/graph4-3.html",
    "href": "posts/hadamangeo/graph4-3.html",
    "title": "CH4. 지도 그래프 학습(그래프 정규화 방법)",
    "section": "",
    "text": "그래프 머신러닝\ngithub"
  },
  {
    "objectID": "posts/hadamangeo/graph4-3.html#load-dataset",
    "href": "posts/hadamangeo/graph4-3.html#load-dataset",
    "title": "CH4. 지도 그래프 학습(그래프 정규화 방법)",
    "section": "Load Dataset",
    "text": "Load Dataset\n- 데이터셋: Cora\n\n7개의 클래스로 라벨링돼 있는 2,708개의 컴퓨터 사이언스 논문\n각 논문은 인용을 기반으로 다른 노드와 연결된 노드\n총 5,429개의 간선\n\n\nfrom stellargraph import datasets\n\n2023-04-06 21:44:50.486139: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n\n\n\ndataset = datasets.Cora()\n\n\n%config Completer.use_jedi = False\n\n\ndataset.download()\n\n\nlabel_index = {\n      'Case_Based': 0,\n      'Genetic_Algorithms': 1,\n      'Neural_Networks': 2,\n      'Probabilistic_Methods': 3,\n      'Reinforcement_Learning': 4,\n      'Rule_Learning': 5,\n      'Theory': 6,\n  }\n\n\nG, labels = dataset.load()\n\n\nG: 네트워크 노드, 간선, BOW표현 설명\nlabea : 논문id와 클래스 중 하나 사이의 매핑\n훈련 샘플: 이웃과 관련된 정보가 포함 -&gt; 훈련을 정규화 하는데 사용\n검증 샘플: 이웃과 관련된 정보 불포함 , 예측된 라벨은 노드 특증, bow표현에만 의존\n\n\nimport numpy as np\nfrom sklearn import preprocessing, feature_extraction, model_selection\n\n\nimport tensorflow as tf\nfrom tensorflow.train import Example, Features, Feature, Int64List, BytesList, FloatList\n\n\nGRAPH_PREFIX=\"NL_nbr\"\n\n\ndef _int64_feature(*value):\n    \"\"\"Returns int64 tf.train.Feature from a bool / enum / int / uint.\"\"\"\n    return Feature(int64_list=Int64List(value=list(value)))\n\ndef _bytes_feature(value):\n    \"\"\"Returns bytes tf.train.Feature from a string.\"\"\"\n    return Feature(\n        bytes_list=BytesList(value=[value.encode('utf-8')])\n    )\n\ndef _float_feature(*value):\n    return Feature(float_list=FloatList(value=list(value)))\n\n\n_int64_feature 함수는 bool, enum, int, uint 데이터 타입을 입력 받아 int64_list 타입의 tf.train.Feature 객체를 반환\n_bytes_feature 함수는 문자열 값을 입력 받아 utf-8로 인코딩하여 bytes_list 타입의 tf.train.Feature 객체를 반환\n_float_feature 함수는 float 데이터 타입을 입력 받아 float_list 타입의 tf.train.Feature 객체를 반환\n\n- 반지도 학습 데이터 셋 만드는 함수 정의\n\nfrom functools import reduce\nfrom typing import List, Tuple\nimport pandas as pd\nimport six\n\ndef addFeatures(x, y):\n    res = Features()\n    res.CopyFrom(x)\n    res.MergeFrom(y)\n    return res\n\ndef neighborFeatures(features: Features, weight: float, prefix: str):  # 객체, 가중치, 접두어 입력으로 받음\n    data = {f\"{prefix}_weight\": _float_feature(weight)}\n    for name, feature in six.iteritems(features.feature):\n        data[f\"{prefix}_{name}\"] = feature \n    return Features(feature=data)\n\ndef neighborsFeatures(neighbors: List[Tuple[Features, float]]):\n    return reduce(\n        addFeatures, \n        [neighborFeatures(sample, weight, f\"{GRAPH_PREFIX}_{ith}\") for ith, (sample, weight) in enumerate(neighbors)],\n        Features()\n    )\n\ndef getNeighbors(idx, adjMatrix, topn=5): #인덱스와 인접행렬 이용하여 이웃 데이터셋 추출 \n    weights = adjMatrix.loc[idx]\n    return weights[weights&gt;0].sort_values(ascending=False).head(topn).to_dict()\n    \n\ndef semisupervisedDataset(G, labels, ratio=0.2, topn=5):  #라벨이 있는 데이터와 없는 데이터 추출\n     #ratio:라벨 유무 비율 설정\n     #topn: 함수에서 추출할 이웃 데이터셋 크기 설정\n    n = int(np.round(len(labels)*ratio)) \n    \n    labelled, unlabelled = model_selection.train_test_split(\n        labels, train_size=n, test_size=None, stratify=labels\n    )\n\n\n1. 노드 특징 df로 구성하고 그래프 인접행렬로 저장\n\nadjMatrix = pd.DataFrame.sparse.from_spmatrix(G.to_adjacency_matrix(), index=G.nodes(), columns=G.nodes())\n    \nfeatures = pd.DataFrame(G.node_features(), index=G.nodes())\n\n\n\n2. adjMatrix사용해 노드ID와 간선 가중치 반환하여 노드의 가장 가까운 TOPN이웃 검색하는 도우미 함수 구현\n\ndef getNeighbors(idx, adjMatrix, topn=5): #인덱스와 인접행렬 이용하여 이웃 데이터셋 추출 \n    weights = adjMatrix.loc[idx]\n    neighbors = weights[weights&gt;0]\\\n        .sort_values(ascending=False)\\\n        .head(topn)\n    return [(k,v) for k, v in neighbors.iteritems()]\n    \n\n\n3. 정보를 단일 df로 병합\n\ndataset = {\n        index: Features(feature = {\n            #\"id\": _bytes_feature(str(index)), \n            \"id\": _int64_feature(index),\n            \"words\": _float_feature(*[float(x) for x in features.loc[index].values]), \n            \"label\": _int64_feature(label_index[label])\n        })\n        for index, label in pd.concat([labelled, unlabelled]).items()\n    }\n\nNameError: name 'labelled' is not defined\n\n\n\nfrom functools import reduce\nfrom typing import List, Tuple\nimport pandas as pd\nimport six\n\ndef addFeatures(x, y):\n    res = Features()\n    res.CopyFrom(x)\n    res.MergeFrom(y)\n    return res\n\ndef neighborFeatures(features: Features, weight: float, prefix: str):\n    data = {f\"{prefix}_weight\": _float_feature(weight)}\n    for name, feature in six.iteritems(features.feature):\n        data[f\"{prefix}_{name}\"] = feature \n    return Features(feature=data)\n\ndef neighborsFeatures(neighbors: List[Tuple[Features, float]]):\n    return reduce(\n        addFeatures, \n        [neighborFeatures(sample, weight, f\"{GRAPH_PREFIX}_{ith}\") for ith, (sample, weight) in enumerate(neighbors)],\n        Features()\n    )\n\ndef getNeighbors(idx, adjMatrix, topn=5):\n    weights = adjMatrix.loc[idx]\n    return weights[weights&gt;0].sort_values(ascending=False).head(topn).to_dict()\n    \n\ndef semisupervisedDataset(G, labels, ratio=0.2, topn=5):\n    n = int(np.round(len(labels)*ratio))\n    \n    labelled, unlabelled = model_selection.train_test_split(\n        labels, train_size=n, test_size=None, stratify=labels\n    )\n    \n    adjMatrix = pd.DataFrame.sparse.from_spmatrix(G.to_adjacency_matrix(), index=G.nodes(), columns=G.nodes())\n    \n    features = pd.DataFrame(G.node_features(), index=G.nodes())\n    \n    dataset = {\n        index: Features(feature = {\n            #\"id\": _bytes_feature(str(index)), \n            \"id\": _int64_feature(index),\n            \"words\": _float_feature(*[float(x) for x in features.loc[index].values]), \n            \"label\": _int64_feature(label_index[label])\n        })\n        for index, label in pd.concat([labelled, unlabelled]).items()\n    }\n    \n    trainingSet = [\n        Example(features=addFeatures(\n            dataset[exampleId], \n            neighborsFeatures(\n                [(dataset[nodeId], weight) for nodeId, weight in getNeighbors(exampleId, adjMatrix, topn).items()]\n            )\n        ))\n        for exampleId in labelled.index\n    ]\n    \n    testSet = [Example(features=dataset[exampleId]) for exampleId in unlabelled.index]\n\n    serializer = lambda _list: [e.SerializeToString() for e in _list]\n    \n    return serializer(trainingSet), serializer(testSet)"
  },
  {
    "objectID": "posts/hadamangeo/Czech-banking-customer-trans-analysis.html",
    "href": "posts/hadamangeo/Czech-banking-customer-trans-analysis.html",
    "title": "Czech Bank’s Financial Data Analysis. Moving from gut feeling to data-driven decisions.",
    "section": "",
    "text": "Czech"
  },
  {
    "objectID": "posts/hadamangeo/Czech-banking-customer-trans-analysis.html#datasets-structure",
    "href": "posts/hadamangeo/Czech-banking-customer-trans-analysis.html#datasets-structure",
    "title": "Czech Bank’s Financial Data Analysis. Moving from gut feeling to data-driven decisions.",
    "section": "Datasets structure",
    "text": "Datasets structure\n\n\n\ndata map.gif\n\n\nThanks to TSilveira for data visualization examples: https://www.kaggle.com/tsilveira/applying-heatmaps-for-categorical-data-analysis\n\nimport numpy as np # \nimport pandas as pd # \n\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\ncolor = sns.color_palette()\n\n%matplotlib inline\n\nfrom datetime import datetime\nfrom datetime import date\nfrom plotly import tools\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\n\nfrom collections import Counter\n\nimport os\nprint(os.listdir(\"../czech-banking-fin-analysis\"))\n\n\n\n\n['.git', '.ipynb_checkpoints', 'account.csv', 'card.csv', 'client.csv', 'Czech-banking-customer-trans-analysis.ipynb', 'czech.jpg', 'Data dictionary.pdf', 'disp.csv', 'district.csv', 'loan.csv', 'order.csv', 'Plots', 'trans.csv']\n\n\n\n#defining all necessary functions\n\ndef date_correction (df, col_name):\n    \"\"\" Function that will re-format cells into date format. Input should be in the format of YYMMDD, e.g. 950107\n    input: dataframe and column name of the dataframe\n    output: updated dataframe\n    \"\"\"\n    df [col_name] = pd.to_datetime (df [col_name], format = '%y%m%d', errors = 'coerce')\n    return df\n\ndef date_misinterp_cor (date): #correction of date misinterpretation when 45 treated as 2045 and not 1945\n    \"\"\"Function that corrects year misinterpretation when 45 treated as 2045 and not 1945\n    input: date\n    output: corrected date\"\"\"\n    if date.year &gt; 2000: \n        date = date.replace (year = date.year - 100)\n        return date\n    else:\n        return date\n\ndef calculate_age (born_date):\n    \"\"\"Age calculation as of 31.12.1998 \n    input: born date\n    outpute: age (int)\"\"\"\n    born = born_date.year\n    return 1998 - born\n\ndef date_parsing (date):\n    \"\"\"Extracting year from a string. \n    input: a value that contains date in the first 6 symbols.\n    output: year\"\"\"\n    date = int (str (date) [0:6])\n    date2 = pd.to_datetime (date, format = '%y%m%d', errors = 'coerce')\n    if date2.year &gt; 2000: \n        return date2.year - 100\n    else:\n        return date2.year\n    \ndef year_extract (date):\n    \"\"\" Function that will extract a year from date and return it\n        input should be date format\"\"\"\n    if date.year &gt; 2000: \n        return date.year - 100\n    else:\n        return date.year\n    \ndef df_row_normalize(dataframe):\n    '''Normalizes the values of a given pandas.Dataframe by the total sum of each line.\n    Algorithm based on https://stackoverflow.com/questions/26537878/pandas-sum-across-columns-and-divide-each-cell-from-that-value'''\n    return dataframe.div(dataframe.sum(axis=1), axis=0)\n\n\n#loading all the datasets\n\naccounts_df = pd.read_csv ('account.csv', sep = ';')\ncards_df = pd.read_csv ('card.csv', sep = ';')\nclients_df = pd.read_csv ('client.csv', sep = ';')\ndispos_df = pd.read_csv ('disp.csv', sep = ';')\ndistrict_df = pd.read_csv ('district.csv', sep = ';')\nloan_df = pd.read_csv ('loan.csv', sep = ';')\norder_df = pd.read_csv ('order.csv', sep = ';')\ntrans_df = pd.read_csv ('trans.csv', sep = ';')\n\n\nD:\\Users\\Kusainov\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2785: DtypeWarning:\n\nColumns (8) have mixed types. Specify dtype option on import or set low_memory=False.\n\n\n\n\n#list of all dataframes\nfiles = [accounts_df, cards_df, clients_df, dispos_df, district_df, loan_df, order_df, trans_df]\ndate_cor_files = [trans_df, accounts_df, loan_df]\nfiles_name = ['accounts_df', 'cards_df', 'clients_df', 'dispos_df', 'district_df', 'loan_df', 'order_df', 'trans_df']\n\n\nInitial observations:\n\n#verifying numeric statistics and missing values in the datasets  \nfor id, item in enumerate (files): \n    print ('Dataframe name: ' + str (files_name [id]) + \" with number of rows:\" + str (item.shape [0]) + ' and columns:' + str (item.shape [1]) )\n    display (item.describe ())\n    print (item.isnull ().sum ())\n    print ('\\n')\n\nDataframe name: accounts_df with number of rows:4500 and columns:4\naccount_id     0\ndistrict_id    0\nfrequency      0\ndate           0\ndtype: int64\n\n\nDataframe name: cards_df with number of rows:892 and columns:4\ncard_id    0\ndisp_id    0\ntype       0\nissued     0\ndtype: int64\n\n\nDataframe name: clients_df with number of rows:5369 and columns:3\nclient_id       0\nbirth_number    0\ndistrict_id     0\ndtype: int64\n\n\nDataframe name: dispos_df with number of rows:5369 and columns:4\ndisp_id       0\nclient_id     0\naccount_id    0\ntype          0\ndtype: int64\n\n\nDataframe name: district_df with number of rows:77 and columns:16\nA1     0\nA2     0\nA3     0\nA4     0\nA5     0\nA6     0\nA7     0\nA8     0\nA9     0\nA10    0\nA11    0\nA12    0\nA13    0\nA14    0\nA15    0\nA16    0\ndtype: int64\n\n\nDataframe name: loan_df with number of rows:682 and columns:7\nloan_id       0\naccount_id    0\ndate          0\namount        0\nduration      0\npayments      0\nstatus        0\ndtype: int64\n\n\nDataframe name: order_df with number of rows:6471 and columns:6\norder_id      0\naccount_id    0\nbank_to       0\naccount_to    0\namount        0\nk_symbol      0\ndtype: int64\n\n\nDataframe name: trans_df with number of rows:1056320 and columns:10\ntrans_id           0\naccount_id         0\ndate               0\ntype               0\noperation     183114\namount             0\nbalance            0\nk_symbol      481881\nbank          782812\naccount       760931\ndtype: int64\n\n\n\n\n\n\n\n\n\n\n\naccount_id\ndistrict_id\ndate\n\n\n\n\ncount\n4500.000000\n4500.000000\n4500.000000\n\n\nmean\n2786.067556\n37.310444\n951654.608667\n\n\nstd\n2313.811984\n25.177217\n14842.188377\n\n\nmin\n1.000000\n1.000000\n930101.000000\n\n\n25%\n1182.750000\n13.000000\n931227.000000\n\n\n50%\n2368.000000\n38.000000\n960102.000000\n\n\n75%\n3552.250000\n60.000000\n961101.000000\n\n\nmax\n11382.000000\n77.000000\n971229.000000\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncard_id\ndisp_id\n\n\n\n\ncount\n892.000000\n892.000000\n\n\nmean\n480.855381\n3511.862108\n\n\nstd\n306.933982\n2984.373626\n\n\nmin\n1.000000\n9.000000\n\n\n25%\n229.750000\n1387.000000\n\n\n50%\n456.500000\n2938.500000\n\n\n75%\n684.250000\n4459.500000\n\n\nmax\n1247.000000\n13660.000000\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nclient_id\nbirth_number\ndistrict_id\n\n\n\n\ncount\n5369.000000\n5369.000000\n5369.000000\n\n\nmean\n3359.011920\n535114.970013\n37.310114\n\n\nstd\n2832.911984\n172895.618429\n25.043690\n\n\nmin\n1.000000\n110820.000000\n1.000000\n\n\n25%\n1418.000000\n406009.000000\n14.000000\n\n\n50%\n2839.000000\n540829.000000\n38.000000\n\n\n75%\n4257.000000\n681013.000000\n60.000000\n\n\nmax\n13998.000000\n875927.000000\n77.000000\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndisp_id\nclient_id\naccount_id\n\n\n\n\ncount\n5369.000000\n5369.000000\n5369.000000\n\n\nmean\n3337.097970\n3359.011920\n2767.496927\n\n\nstd\n2770.418826\n2832.911984\n2307.843630\n\n\nmin\n1.000000\n1.000000\n1.000000\n\n\n25%\n1418.000000\n1418.000000\n1178.000000\n\n\n50%\n2839.000000\n2839.000000\n2349.000000\n\n\n75%\n4257.000000\n4257.000000\n3526.000000\n\n\nmax\n13690.000000\n13998.000000\n11382.000000\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA1\nA4\nA5\nA6\nA7\nA8\nA9\nA10\nA11\nA13\nA14\nA16\n\n\n\n\ncount\n77.000000\n7.700000e+01\n77.000000\n77.000000\n77.000000\n77.000000\n77.000000\n77.000000\n77.000000\n77.000000\n77.000000\n77.000000\n\n\nmean\n39.000000\n1.338849e+05\n48.623377\n24.324675\n6.272727\n1.727273\n6.259740\n63.035065\n9031.675325\n3.787013\n116.129870\n5030.831169\n\n\nstd\n22.371857\n1.369135e+05\n32.741829\n12.780991\n4.015222\n1.008338\n2.435497\n16.221727\n790.202347\n1.908480\n16.608773\n11270.796786\n\n\nmin\n1.000000\n4.282100e+04\n0.000000\n0.000000\n0.000000\n0.000000\n1.000000\n33.900000\n8110.000000\n0.430000\n81.000000\n888.000000\n\n\n25%\n20.000000\n8.585200e+04\n22.000000\n16.000000\n4.000000\n1.000000\n5.000000\n51.900000\n8512.000000\n2.310000\n105.000000\n2122.000000\n\n\n50%\n39.000000\n1.088710e+05\n49.000000\n25.000000\n6.000000\n2.000000\n6.000000\n59.800000\n8814.000000\n3.600000\n113.000000\n3040.000000\n\n\n75%\n58.000000\n1.390120e+05\n71.000000\n32.000000\n8.000000\n2.000000\n8.000000\n73.500000\n9317.000000\n4.790000\n126.000000\n4595.000000\n\n\nmax\n77.000000\n1.204953e+06\n151.000000\n70.000000\n20.000000\n5.000000\n11.000000\n100.000000\n12541.000000\n9.400000\n167.000000\n99107.000000\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nloan_id\naccount_id\ndate\namount\nduration\npayments\n\n\n\n\ncount\n682.000000\n682.000000\n682.000000\n682.000000\n682.000000\n682.000000\n\n\nmean\n6172.466276\n5824.162757\n963027.910557\n151410.175953\n36.492669\n4190.664223\n\n\nstd\n682.579279\n3283.512681\n14616.406049\n113372.406310\n17.075219\n2215.830344\n\n\nmin\n4959.000000\n2.000000\n930705.000000\n4980.000000\n12.000000\n304.000000\n\n\n25%\n5577.500000\n2967.000000\n950704.500000\n66732.000000\n24.000000\n2477.000000\n\n\n50%\n6176.500000\n5738.500000\n970206.500000\n116928.000000\n36.000000\n3934.000000\n\n\n75%\n6752.500000\n8686.000000\n971212.500000\n210654.000000\n48.000000\n5813.500000\n\n\nmax\n7308.000000\n11362.000000\n981208.000000\n590820.000000\n60.000000\n9910.000000\n\n\n\n\n\n\n\n\n\n\n\n\n\n\norder_id\naccount_id\naccount_to\namount\n\n\n\n\ncount\n6471.000000\n6471.000000\n6.471000e+03\n6471.000000\n\n\nmean\n33778.197497\n2962.302890\n4.939904e+07\n3280.635698\n\n\nstd\n3737.681949\n2518.503228\n2.888356e+07\n2714.475335\n\n\nmin\n29401.000000\n1.000000\n3.990000e+02\n1.000000\n\n\n25%\n31187.500000\n1223.000000\n2.415918e+07\n1241.500000\n\n\n50%\n32988.000000\n2433.000000\n4.975606e+07\n2596.000000\n\n\n75%\n34785.500000\n3645.500000\n7.400045e+07\n4613.500000\n\n\nmax\n46338.000000\n11362.000000\n9.999420e+07\n14882.000000\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntrans_id\naccount_id\ndate\namount\nbalance\naccount\n\n\n\n\ncount\n1.056320e+06\n1.056320e+06\n1.056320e+06\n1.056320e+06\n1.056320e+06\n2.953890e+05\n\n\nmean\n1.335311e+06\n2.936867e+03\n9.656748e+05\n5.924146e+03\n3.851833e+04\n4.567092e+07\n\n\nstd\n1.227487e+06\n2.477345e+03\n1.394535e+04\n9.522735e+03\n2.211787e+04\n3.066340e+07\n\n\nmin\n1.000000e+00\n1.000000e+00\n9.301010e+05\n0.000000e+00\n-4.112570e+04\n0.000000e+00\n\n\n25%\n4.302628e+05\n1.204000e+03\n9.601160e+05\n1.359000e+02\n2.240250e+04\n1.782858e+07\n\n\n50%\n8.585065e+05\n2.434000e+03\n9.704100e+05\n2.100000e+03\n3.314340e+04\n4.575095e+07\n\n\n75%\n2.060979e+06\n3.660000e+03\n9.802280e+05\n6.800000e+03\n4.960362e+04\n7.201341e+07\n\n\nmax\n3.682987e+06\n1.138200e+04\n9.812310e+05\n8.740000e+04\n2.096370e+05\n9.999420e+07\n\n\n\n\n\n\n\nFrom the first sight only transactions dataset contains NaN values for operation, k_symbol, bank and account.\n\n#Dataframes header\nfor id, item in enumerate (files): \n    print ('Dataframe name:' + str (files_name [id]) )\n    display (item.head (n=3))\n    print ('\\n')\n\nDataframe name:accounts_df\n\n\nDataframe name:cards_df\n\n\nDataframe name:clients_df\n\n\nDataframe name:dispos_df\n\n\nDataframe name:district_df\n\n\nDataframe name:loan_df\n\n\nDataframe name:order_df\n\n\nDataframe name:trans_df\n\n\n\n\n\n\n\n\n\n\n\naccount_id\ndistrict_id\nfrequency\ndate\n\n\n\n\n0\n576\n55\nPOPLATEK MESICNE\n930101\n\n\n1\n3818\n74\nPOPLATEK MESICNE\n930101\n\n\n2\n704\n55\nPOPLATEK MESICNE\n930101\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncard_id\ndisp_id\ntype\nissued\n\n\n\n\n0\n1005\n9285\nclassic\n931107 00:00:00\n\n\n1\n104\n588\nclassic\n940119 00:00:00\n\n\n2\n747\n4915\nclassic\n940205 00:00:00\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nclient_id\nbirth_number\ndistrict_id\n\n\n\n\n0\n1\n706213\n18\n\n\n1\n2\n450204\n1\n\n\n2\n3\n406009\n1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndisp_id\nclient_id\naccount_id\ntype\n\n\n\n\n0\n1\n1\n1\nOWNER\n\n\n1\n2\n2\n2\nOWNER\n\n\n2\n3\n3\n2\nDISPONENT\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA1\nA2\nA3\nA4\nA5\nA6\nA7\nA8\nA9\nA10\nA11\nA12\nA13\nA14\nA15\nA16\n\n\n\n\n0\n1\nHl.m. Praha\nPrague\n1204953\n0\n0\n0\n1\n1\n100.0\n12541\n0.29\n0.43\n167\n85677\n99107\n\n\n1\n2\nBenesov\ncentral Bohemia\n88884\n80\n26\n6\n2\n5\n46.7\n8507\n1.67\n1.85\n132\n2159\n2674\n\n\n2\n3\nBeroun\ncentral Bohemia\n75232\n55\n26\n4\n1\n5\n41.7\n8980\n1.95\n2.21\n111\n2824\n2813\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nloan_id\naccount_id\ndate\namount\nduration\npayments\nstatus\n\n\n\n\n0\n5314\n1787\n930705\n96396\n12\n8033.0\nB\n\n\n1\n5316\n1801\n930711\n165960\n36\n4610.0\nA\n\n\n2\n6863\n9188\n930728\n127080\n60\n2118.0\nA\n\n\n\n\n\n\n\n\n\n\n\n\n\n\norder_id\naccount_id\nbank_to\naccount_to\namount\nk_symbol\n\n\n\n\n0\n29401\n1\nYZ\n87144583\n2452.0\nSIPO\n\n\n1\n29402\n2\nST\n89597016\n3372.7\nUVER\n\n\n2\n29403\n2\nQR\n13943797\n7266.0\nSIPO\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntrans_id\naccount_id\ndate\ntype\noperation\namount\nbalance\nk_symbol\nbank\naccount\n\n\n\n\n0\n695247\n2378\n930101\nPRIJEM\nVKLAD\n700.0\n700.0\nNaN\nNaN\nNaN\n\n\n1\n171812\n576\n930101\nPRIJEM\nVKLAD\n900.0\n900.0\nNaN\nNaN\nNaN\n\n\n2\n207264\n704\n930101\nPRIJEM\nVKLAD\n1000.0\n1000.0\nNaN\nNaN\nNaN\n\n\n\n\n\n\n\nDates in the following dataframes were not loaded as dates:trans_df, accounts_df, cards_df, clients_df (birth date) and loan_df.\n\n#correcting dates for trans_df, accounts_df, and loan_df. \n\nfor id, item in enumerate (date_cor_files): \n    date_cor_files [id] = date_correction (item, 'date')\n\ntrans_df = date_cor_files [0]\naccounts_df = date_cor_files [1]\nloans_df = date_cor_files [2]\n\n\n#verifying transactions execution dates period in trans_df  \ntrans1 = trans_df.sort_values (['date'])\ndisplay (trans1.head (n=3))\ndisplay (trans1.tail (n=3))\n\n\n\n\n\n\n\n\ntrans_id\naccount_id\ndate\ntype\noperation\namount\nbalance\nk_symbol\nbank\naccount\n\n\n\n\n0\n695247\n2378\n1993-01-01\nPRIJEM\nVKLAD\n700.0\n700.0\nNaN\nNaN\nNaN\n\n\n1\n171812\n576\n1993-01-01\nPRIJEM\nVKLAD\n900.0\n900.0\nNaN\nNaN\nNaN\n\n\n2\n207264\n704\n1993-01-01\nPRIJEM\nVKLAD\n1000.0\n1000.0\nNaN\nNaN\nNaN\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntrans_id\naccount_id\ndate\ntype\noperation\namount\nbalance\nk_symbol\nbank\naccount\n\n\n\n\n1053314\n3631495\n3053\n1998-12-31\nPRIJEM\nNaN\n349.8\n86982.1\nUROK\nNaN\nNaN\n\n\n1053320\n3632209\n3077\n1998-12-31\nPRIJEM\nNaN\n267.9\n68530.7\nUROK\nNaN\nNaN\n\n\n1056319\n3626540\n2902\n1998-12-31\nPRIJEM\nNaN\n164.1\n41642.9\nUROK\nNaN\nNaN\n\n\n\n\n\n\n\nFrom above we can see that transactions given for a period from 1st of Jan 1993 till 31st of Dec 1998\n\n\nBusiness questions part #1 - Customer insight\n\nGender proportion of the bank’s customers.\nAge distribution of the bank’s customers.\nClients habitation in different regions of Czech Republic and average salaries of the regions.\nDistricts that could be attractive for future bank’s market invasion in terms of population, average salary and underrepresented clients base.\n\n“Birth number” column should be parced into 3 columns: date of birth, gender and age (as of 31.12.1998)\nInitially “Birth number” in the dataset contains both date of birth and gender: the number in the cells is in the form of YYMMDD for men and in the form YYMM+50DD for women, where YYMMDD is the date of birth.\n\n#adding new columns\nclients_df = clients_df.join (pd.DataFrame ( { 'birth_date': np.nan, 'gender': np.nan, 'age': np.nan}, index = clients_df.index))\n\n\n#next step is to fulfill new columns with data from 'birth_number' and 'age'\nclients_df ['birth_date'] = clients_df ['birth_number']\nfor ids, item in enumerate (clients_df ['birth_number']):\n    if int (str (item) [2:4]) &gt; 50:\n        clients_df.loc [ids, 'gender'] = 0 #female\n        clients_df.loc [ids, 'birth_date'] = item - 5000 \n    else: \n        clients_df.loc [ids, 'gender'] = 1 #male #incorrect way is: clients_df.loc [ids] ['gender']         \n\n\n#converting birth_date into date format\nclients_df = date_correction (clients_df, 'birth_date') \n\n\n#correcting misinterpreted dates\nclients_df ['birth_date'] = clients_df ['birth_date'].apply (date_misinterp_cor)\n\n\n#calculating age as of on 31.12.1998 \nclients_df ['age'] = clients_df ['birth_date'].apply (calculate_age)\ndisplay (clients_df.head (n=3))\n\n\n\n\n\n\n\n\nclient_id\nbirth_number\ndistrict_id\nbirth_date\ngender\nage\n\n\n\n\n0\n1\n706213\n18\n1970-12-13\n0.0\n28\n\n\n1\n2\n450204\n1\n1945-02-04\n1.0\n53\n\n\n2\n3\n406009\n1\n1940-10-09\n0.0\n58\n\n\n\n\n\n\n\n\n#plotting customers distribution by age\nclients_df ['age'].plot (kind = 'hist',\n                    title = 'Clients distribution per age',\n                   # width = 0.75,\n                    figsize = (8, 5.8)\n                    )\n\n&lt;matplotlib.axes._subplots.AxesSubplot at 0x21d38630&gt;\n\n\n\n\n\n\n#Clients gender visualization\n\ntarget1 = clients_df [clients_df ['gender'] == 1.0].shape [0]\ntarget0 = clients_df [clients_df ['gender'] == 0.0].shape [0]\n\ntrace0 = go.Bar (\n    x = ['Men', 'Women'],\n    y = [target1/ (target1 + target0) *100, target0/ (target1 + target0) *100], \n    marker = dict (color=['rgba(34, 167, 240, 1)', 'rgba(222,45,38,0.8)']\n                  ),\n) \ndata = [trace0]\nlayout = go.Layout (title = 'Gender proportion (%)', autosize= False, width= 500, height= 500,\n                    )\nfig = go.Figure (data=data, layout = layout)\n#plotly.offline.iplot()\npy.iplot (fig, filename = 'test')\n\n\n\n\n\nprint ('Percentage of men: '+ str (target1/clients_df.shape [0]*100) +'%' + ' percentage of women: ' + str (target0/clients_df.shape [0]*100) + '%')\n\nPercentage of men: 50.73570497299311% percentage of women: 49.264295027006895%\n\n\n\n#enriching customer info with habitation info   \nclients_dist_df = clients_df\nclients_dist2_df = clients_dist_df.merge (district_df, left_on = 'district_id', right_on = 'A1', \n                                           how = 'left', validate = 'many_to_one') \ndisplay (clients_df.head (n=3))\ndisplay (clients_dist2_df.head (n=3))      \n\n\n\n\n\n\n\n\nclient_id\nbirth_number\ndistrict_id\nbirth_date\ngender\nage\n\n\n\n\n0\n1\n706213\n18\n1970-12-13\n0.0\n28\n\n\n1\n2\n450204\n1\n1945-02-04\n1.0\n53\n\n\n2\n3\n406009\n1\n1940-10-09\n0.0\n58\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nclient_id\nbirth_number\ndistrict_id\nbirth_date\ngender\nage\nA1\nA2\nA3\nA4\n...\nA7\nA8\nA9\nA10\nA11\nA12\nA13\nA14\nA15\nA16\n\n\n\n\n0\n1\n706213\n18\n1970-12-13\n0.0\n28\n18\nPisek\nsouth Bohemia\n70699\n...\n2\n1\n4\n65.3\n8968\n2.83\n3.35\n131\n1740\n1910\n\n\n1\n2\n450204\n1\n1945-02-04\n1.0\n53\n1\nHl.m. Praha\nPrague\n1204953\n...\n0\n1\n1\n100.0\n12541\n0.29\n0.43\n167\n85677\n99107\n\n\n2\n3\n406009\n1\n1940-10-09\n0.0\n58\n1\nHl.m. Praha\nPrague\n1204953\n...\n0\n1\n1\n100.0\n12541\n0.29\n0.43\n167\n85677\n99107\n\n\n\n\n3 rows × 22 columns\n\n\n\n\n#grouping by district (\"A2\") and population of the district ('A4'), then counting number of customers \ndistrict_clients_df = pd.DataFrame (clients_dist2_df.groupby (['A2', 'A4', 'A11'], axis = 0) ['A2'].count ())\ndistrict_clients_df.columns = ['count']\n\n#making columns out of multi-index of A3 and type_x so to make it easier to operate with values\ndistrict_clients_df.reset_index (inplace = True)\n\n#calculating portion of clients in the population of a district\ndistrict_clients_df ['portion'] = district_clients_df ['count'] / district_clients_df ['A4'] * 100\n\ndisplay (district_clients_df .head (n=5))\ndistrict_clients_df ['count'].sum ()\n\n\n\n\n\n\n\n\nA2\nA4\nA11\ncount\nportion\n\n\n\n\n0\nBenesov\n88884\n8507\n46\n0.051753\n\n\n1\nBeroun\n75232\n8980\n63\n0.083741\n\n\n2\nBlansko\n107911\n8240\n57\n0.052821\n\n\n3\nBreclav\n124605\n8772\n54\n0.043337\n\n\n4\nBrno - mesto\n387570\n9897\n155\n0.039993\n\n\n\n\n\n\n\n5369\n\n\n\n#average value of average salaries within districts\ndistrict_df ['A11'].mean ()\n\n9031.675324675325\n\n\n\n#average value of Relative number of clients comparing to district population\ndistrict_clients_df ['portion'].mean ()\n\n0.05795326262662295\n\n\n\n#plotting Number of inhabitants vs. Average salary vs. Customers proportion within population of districts\ndistrict_clients_df2 = district_clients_df.copy ()\ndistrict_clients_df2['A4'] = district_clients_df2['A4'] / 1000000 \n\n## Initialize the matplotlib figure:\nfig2, (ax1, ax2, ax3) = plt.subplots(1,3, figsize=(12,16), sharey=False)\nfig2.tight_layout()  #When working with 'tight_layout', the subplot must be adjusted [https://stackoverflow.com/questions/7066121/how-to-set-a-single-main-title-above-all-the-subplots-with-pyplot]\nfig2.subplots_adjust(top=0.96)  #Adjusting the space for the superior title\n\n## Plot the District population\nsns.set_color_codes(\"muted\")\nsns.barplot(x=\"A4\", y=\"A2\", data=district_clients_df2, label = 'District population (mln)',color=\"b\", ax=ax1)\n## Add a legend and informative axis label\nax1.legend(ncol=2, loc=\"lower right\", frameon=True)\nax1.set(xlim=(0, 1.21), ylabel=\"\", xlabel=\"District population (mln)\")\nsns.despine(left=True, bottom=True,ax=ax1)\n\n## Plot the Average salary\nsns.set_color_codes(\"muted\")\nsns.barplot(x=\"A11\", y=\"A2\", data=district_clients_df, label=\"Average salary\", color=\"b\", ax=ax2)\n## Add a legend and informative axis label\nax2.legend(ncol=2, loc=\"lower right\", frameon=True)\nax2.set(xlim=(0, 12541), ylabel=\"\", xlabel=\"Average salary\")  #The xlim value comes from the maximum value in the dataset.\nax2.set_yticklabels([''])\nsns.despine(left=True, bottom=True, ax=ax2)\n\nsns.set_color_codes(\"muted\")\nsns.barplot(x=\"portion\", y=\"A2\", data=district_clients_df, label=\"% of clients in population\", color=\"b\", ax=ax3)\n## Add a legend and informative axis label\nax3.legend(ncol=2, loc=\"lower right\", frameon=True)\nax3.set(xlim=(0, 0.145), ylabel=\"\", xlabel=\"Relative number of clients comparing to district population (%)\")  #The xlim value comes from the maximum value in the dataset.\nax3.set_yticklabels([''])\nsns.despine(left=True, bottom=True, ax=ax3)\n\nplt.suptitle('Number of inhabitants vs. Average salary vs. Customers proportion within population of districts', fontsize=14, fontweight='bold')\nplt.show()\n\n\n\n\n\n#Plotting clients presence in different districts\nreg_count = Counter (clients_dist2_df ['A2'])\nregion_counts = pd.DataFrame.from_dict (reg_count, orient = 'index')\nregion_counts.columns = ['Number of customers']\nregion_counts2 = region_counts# / (region_counts ['Percentage_ratio'].sum ()) * 100\n\nregion_counts2.plot (kind = 'barh',\n                    title = 'Clients distribution per districts',\n                    width = 0.75,\n                    figsize = (10, 13)\n                    )\n\n&lt;matplotlib.axes._subplots.AxesSubplot at 0x366260f0&gt;\n\n\n\n\n\n\n#Plotting clients presence in different major regions\n\nreg_count = Counter (clients_dist2_df ['A3'])\nregion_counts = pd.DataFrame.from_dict (reg_count, orient = 'index')\nregion_counts.columns = ['Percentage_ratio']\nregion_counts2 = region_counts / (region_counts ['Percentage_ratio'].sum ()) * 100\nregion_counts2.plot (kind = 'bar',\n                    title = 'Clients distribution per regions',\n                    width = 0.75,\n                    figsize = (8, 5.8),\n                    color = [plt.cm.Paired(np.arange(len(region_counts)))])\n\n&lt;matplotlib.axes._subplots.AxesSubplot at 0x18ddceb8&gt;\n\n\n\n\n\n\n#Average salaries in the regions\nregion_salary_df = pd.DataFrame (district_df.groupby (['A3'], axis = 0) ['A11'].mean ()).sort_values ('A11')\ndisplay (region_salary_df.head (n=10))\n\n\n\n\n\n\n\n\nA11\n\n\nA3\n\n\n\n\n\neast Bohemia\n8611.181818\n\n\nsouth Moravia\n8728.500000\n\n\nsouth Bohemia\n8831.500000\n\n\nwest Bohemia\n9015.400000\n\n\nnorth Moravia\n9049.181818\n\n\nnorth Bohemia\n9334.200000\n\n\ncentral Bohemia\n9357.250000\n\n\nPrague\n12541.000000\n\n\n\n\n\n\n\n\n#Plotting average salary per regions\ntrace0 = go.Bar (\n    x = region_salary_df.index,\n    y = region_salary_df ['A11'], \n) \ndata = [trace0]\nlayout = go.Layout (title = 'Average salary per region', autosize= False, width= 600, height= 400,\n                    )\nfig = go.Figure (data=data, layout = layout)\npy.iplot (fig, filename = 'Salaries')\n\n\n\n\n\n\nBusiness questions part #1 conclusion:\n\nMen and women almost equally presented among the bank’s customers, 50.74% for men and 49.26% for women.\nMajority of clients are between age of 20 to 60.\nMost of the customers are living in South and North Moravia, on the contrerary South and West Bohemia are the least presented regions. Average salary is nearly evenly distributed among regions except Prague (capital of the country) where average salary is noticeably higher. Apparently invasion into Prague banking market should be reinforced.\n\nFollowing districts are the most attractive: Hl. m. Praha, Brno-mesto, Karvina, Ostrava-mesto, Praha-Vychod, Frydek-Mistek (pretty close to the criteria) for future market presence grow. \n\n\n\nBusiness questions part #2 - Card products’ patterns\n\nOverall view on the proportion of issued card products.\n\nCard products distribution among different regions of Czech. Is there any regions where specific card products are more popular than another?\n\nCards issuance trends from 1993 to 1998 overview.\n\n\n# Card products popularity \n\nproduct_count = Counter (cards_df ['type'])\nproduct_counts = pd.DataFrame.from_dict (product_count, orient = 'index')\nproduct_counts.columns = ['Percentage_ratio']\nproduct_counts = product_counts / (product_counts ['Percentage_ratio'].sum ()) * 100\nproduct_counts.plot (kind = 'bar',\n                    title = 'Card products distribution',\n                    width = 0.75,\n                    figsize = (7, 4.8),\n                    subplots = True, \n                    color = [plt.cm.Paired(np.arange(len(region_counts)))]\n                    )\ndisplay (product_count)\n\nCounter({'classic': 659, 'junior': 145, 'gold': 88})\n\n\n\n\n\n\n#merging cards_df and corresponding dispos_df so to find client_id and then from client_id to find corresponding region data\n\ncards_disp_df = cards_df\ncards_disp2_df = cards_disp_df.merge (dispos_df, left_on = 'disp_id', right_on = 'disp_id', \n                                           how = 'left', validate = 'many_to_one') \ndisplay (cards_df.head (n=3))\ndisplay (cards_disp2_df.head (n=3))     \nprint (\"Verifying shape. Before: \" + str (cards_df.shape)+ \" After: \" + str (cards_disp2_df.shape))\n\n\n\n\n\n\n\n\ncard_id\ndisp_id\ntype\nissued\n\n\n\n\n0\n1005\n9285\nclassic\n931107 00:00:00\n\n\n1\n104\n588\nclassic\n940119 00:00:00\n\n\n2\n747\n4915\nclassic\n940205 00:00:00\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncard_id\ndisp_id\ntype_x\nissued\nclient_id\naccount_id\ntype_y\n\n\n\n\n0\n1005\n9285\nclassic\n931107 00:00:00\n9593\n7753\nOWNER\n\n\n1\n104\n588\nclassic\n940119 00:00:00\n588\n489\nOWNER\n\n\n2\n747\n4915\nclassic\n940205 00:00:00\n4915\n4078\nOWNER\n\n\n\n\n\n\n\nVerifying shape. Before: (892, 4) After: (892, 7)\n\n\n\n#merging cards_disp2_df with clients_dist2_df. as a result I will have a df with card products and corresponding regions info\n#in the same DF\n\ncards_disp3_df = cards_disp2_df\ncards_disp_reg_df = cards_disp3_df.merge (clients_dist2_df, left_on = 'client_id', right_on = 'client_id', \n                                           how = 'left', validate = 'many_to_one') \ndisplay (cards_disp2_df.head (n=3))\ndisplay (cards_disp_reg_df.head (n=3))     \nprint (\"Verifying shape. Before: \" + str (cards_disp2_df.shape)+ \" After: \" + str (cards_disp_reg_df.shape))\n\n\n\n\n\n\n\n\ncard_id\ndisp_id\ntype_x\nissued\nclient_id\naccount_id\ntype_y\n\n\n\n\n0\n1005\n9285\nclassic\n931107 00:00:00\n9593\n7753\nOWNER\n\n\n1\n104\n588\nclassic\n940119 00:00:00\n588\n489\nOWNER\n\n\n2\n747\n4915\nclassic\n940205 00:00:00\n4915\n4078\nOWNER\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncard_id\ndisp_id\ntype_x\nissued\nclient_id\naccount_id\ntype_y\nbirth_number\ndistrict_id\nbirth_date\n...\nA7\nA8\nA9\nA10\nA11\nA12\nA13\nA14\nA15\nA16\n\n\n\n\n0\n1005\n9285\nclassic\n931107 00:00:00\n9593\n7753\nOWNER\n685128\n74\n1968-01-28\n...\n0\n1\n1\n100.0\n10673\n4.75\n5.44\n100\n18782\n18347\n\n\n1\n104\n588\nclassic\n940119 00:00:00\n588\n489\nOWNER\n606020\n61\n1960-10-20\n...\n5\n1\n6\n53.8\n8814\n4.76\n5.74\n107\n2112\n2059\n\n\n2\n747\n4915\nclassic\n940205 00:00:00\n4915\n4078\nOWNER\n630719\n40\n1963-07-19\n...\n6\n3\n8\n85.3\n9317\n6.49\n7.07\n97\n6949\n6872\n\n\n\n\n3 rows × 28 columns\n\n\n\nVerifying shape. Before: (892, 7) After: (892, 28)\n\n\n\n#preparing dataset for plotting of number of products per regions\n\n#grouping by region (\"A3\") and card product ('type_x'), then counting number of cards \nregion_product_df = pd.DataFrame (cards_disp_reg_df.groupby (['A3', 'type_x'], axis = 0) ['type_x'].count ())\nregion_product_df.columns = ['count']\n\n#making columns out of multi-index of A3 and type_x so to make it easier to operate with values\nregion_product_df.reset_index (level= ['type_x', 'A3'], inplace = True)\ndisplay (region_product_df .head (n=5))\n\n\n\n\n\n\n\n\nA3\ntype_x\ncount\n\n\n\n\n0\nPrague\nclassic\n96\n\n\n1\nPrague\ngold\n12\n\n\n2\nPrague\njunior\n22\n\n\n3\ncentral Bohemia\nclassic\n88\n\n\n4\ncentral Bohemia\ngold\n9\n\n\n\n\n\n\n\n\nregions_duplicate = region_product_df.copy () #should be used as copy () otherwise in the loop both df updated\nfor ids, item in enumerate (region_product_df ['A3']):\n    count_sum = regions_duplicate.loc [ids, 'count']\n    div_sum = regions_duplicate [regions_duplicate ['A3'] == item] ['count']. sum ()\n    region_product_df.loc [ids, 'count'] = round (count_sum / (div_sum) *100)\ndisplay (region_product_df .head (n=5))\n\n\n\n\n\n\n\n\nA3\ntype_x\ncount\n\n\n\n\n0\nPrague\nclassic\n74.0\n\n\n1\nPrague\ngold\n9.0\n\n\n2\nPrague\njunior\n17.0\n\n\n3\ncentral Bohemia\nclassic\n73.0\n\n\n4\ncentral Bohemia\ngold\n8.0\n\n\n\n\n\n\n\n\n#plotting percentage of card products per regions \nx = list (region_product_df ['A3'].unique()) #region names\ny =  list (region_product_df [region_product_df ['type_x'] == 'junior'] ['count']) #junior\ny2 = list (region_product_df [region_product_df ['type_x'] == 'classic'] ['count']) #classic\ny3 = list (region_product_df [region_product_df ['type_x'] == 'gold'] ['count']) #gold\njunior = go.Bar(\n    x=x,\n    y=y,\n    text= y,\n    name = 'Junior',\n    textposition = 'auto',\n    marker=dict(\n        color='rgb(158,202,225)',\n        line=dict(\n            color='rgb(8,48,107)',\n            width=1.5),\n        ),\n    opacity=0.6\n)\nclassic = go.Bar(\n    x=x,\n    y=y2,\n    text=y2,\n    name = 'Classic',\n    textposition = 'auto',\n    marker=dict(\n        color='rgb(0,100,0)',\n        line=dict(\n            color='rgb(8,48,107)',\n            width=1.5),\n        ),\n    opacity=0.6\n)\ngold = go.Bar(\n    x=x,\n    y=y3,\n    text=y3,\n    name = 'Gold',\n    textposition = 'auto',\n    marker=dict(\n        color='rgb(255,215,0)',\n        line=dict(\n            color='rgb(8,48,107)',\n            width=1.5),\n        ),\n    opacity=0.6\n)\ndata = [junior, classic, gold]\npy.iplot(data, filename='grouped-bar-direct-labels')\n\n\n\n\n\n#preparing dataset for plotting trends of cards issuance per years\n\n#adding a column that will reflect specific year when a card was issued\ncards_disp2_df ['issue_year'] = cards_disp2_df ['issued']\n\ncards_disp2_df ['issue_year'] = cards_disp2_df ['issue_year'].apply (date_parsing)\ncards_disp2_df.head (n=3)\n\n\n\n\n\n\n\n\ncard_id\ndisp_id\ntype_x\nissued\nclient_id\naccount_id\ntype_y\nissue_year\n\n\n\n\n0\n1005\n9285\nclassic\n931107 00:00:00\n9593\n7753\nOWNER\n1993\n\n\n1\n104\n588\nclassic\n940119 00:00:00\n588\n489\nOWNER\n1994\n\n\n2\n747\n4915\nclassic\n940205 00:00:00\n4915\n4078\nOWNER\n1994\n\n\n\n\n\n\n\n\n#grouping by issue_year and card product ('type_x'), then counting number of cards \nyear_product_df = pd.DataFrame (cards_disp2_df.groupby (['issue_year', 'type_x'], axis = 0) ['type_x'].count ())\n\n#naming column as 'count' \nyear_product_df.columns = ['count']\n\n#making columns out of multi-index of A3 and type_x so to make it easier to operate with values\nyear_product_df.reset_index (level= ['type_x', 'issue_year'], inplace = True)\ndisplay (year_product_df .head (n=5))\n\n\n\n\n\n\n\n\nissue_year\ntype_x\ncount\n\n\n\n\n0\n1993\nclassic\n1\n\n\n1\n1994\nclassic\n17\n\n\n2\n1994\njunior\n4\n\n\n3\n1995\nclassic\n42\n\n\n4\n1995\ngold\n4\n\n\n\n\n\n\n\n\n#no values for junior and gold products in 1993 and no values for gold cards in 1994\n#adding them as zeroes\n\nlist_of_zer = [pd.Series ([1993, 'junior', 0], index = year_product_df.columns),\n               pd.Series ([1993, 'gold', 0], index = year_product_df.columns),\n               pd.Series ([1994, 'gold', 0], index = year_product_df.columns),\n                ]\nyear_product_df = year_product_df.append (list_of_zer, ignore_index = True)\nyear_product_df.sort_values (['issue_year', 'type_x'], inplace= True)\nyear_product_df.reset_index (inplace = True, drop = True) #making index start from 0 again\ndisplay (year_product_df .head (n=5))\n\n\n\n\n\n\n\n\nissue_year\ntype_x\ncount\n\n\n\n\n0\n1993\nclassic\n1\n\n\n1\n1993\ngold\n0\n\n\n2\n1993\njunior\n0\n\n\n3\n1994\nclassic\n17\n\n\n4\n1994\ngold\n0\n\n\n\n\n\n\n\n\n#plotting number of card products per years \nx = list (year_product_df ['issue_year'].unique()) #years \ny =  list (year_product_df [year_product_df ['type_x'] == 'junior'] ['count']) #junior\ny2 = list (year_product_df [year_product_df ['type_x'] == 'classic'] ['count']) #classic\ny3 = list (year_product_df [year_product_df ['type_x'] == 'gold'] ['count']) #gold\n\njunior = go.Bar(\n    x=x,\n    y=y,\n    text= y,\n    name = 'Junior',\n    textposition = 'auto',\n    marker=dict(\n        color='rgb(158,202,225)',\n        line=dict(\n            color='rgb(8,48,107)',\n            width=1.5),\n        ),\n    opacity=0.6\n)\nclassic = go.Bar(\n    x=x,\n    y=y2,\n    text=y2,\n    name = 'Classic',\n    textposition = 'auto',\n    marker=dict(\n        color='rgb(0,100,0)',\n        line=dict(\n            color='rgb(8,48,107)',\n            width=1.5),\n        ),\n    opacity=0.6\n)\ngold = go.Bar(\n    x=x,\n    y=y3,\n    text=y3,\n    name = 'Gold',\n    textposition = 'auto',\n    marker=dict(\n        color='rgb(255,215,0)',\n        line=dict(\n            color='rgb(8,48,107)',\n            width=1.5),\n        ),\n    opacity=0.6\n)\ndata = [junior, classic, gold]\npy.iplot(data, filename='grouped-bar-direct-labels')\n\n\n\n\n\n#making a new dataset that will contain percentage of each card product per year \n\nyear_duplicate = year_product_df.copy () #should be used as copy () otherwise in the loop both df updated\nfor ids, item in enumerate (year_product_df ['issue_year']):\n    count_sum = year_duplicate.loc [ids, 'count']\n    div_sum = year_duplicate [year_duplicate ['issue_year'] == item] ['count']. sum ()\n    year_product_df.loc [ids, 'count'] = round (count_sum / (div_sum) *100)\n      \ndisplay (year_product_df .head (n=5))\n\n\n\n\n\n\n\n\nissue_year\ntype_x\ncount\n\n\n\n\n0\n1993\nclassic\n100.0\n\n\n1\n1993\ngold\n0.0\n\n\n2\n1993\njunior\n0.0\n\n\n3\n1994\nclassic\n81.0\n\n\n4\n1994\ngold\n0.0\n\n\n\n\n\n\n\n\n#plotting percentafe of card products per years \nx = list (year_product_df ['issue_year'].unique()) #years \ny =  list (year_product_df [year_product_df ['type_x'] == 'junior'] ['count']) #junior\ny2 = list (year_product_df [year_product_df ['type_x'] == 'classic'] ['count']) #classic\ny3 = list (year_product_df [year_product_df ['type_x'] == 'gold'] ['count']) #gold\n\njunior = go.Bar(\n    x=x,\n    y=y,\n    text= y,\n    name = 'Junior',\n    textposition = 'auto',\n    marker=dict(\n        color='rgb(158,202,225)',\n        line=dict(\n            color='rgb(8,48,107)',\n            width=1.5),\n        ),\n    opacity=0.6\n)\nclassic = go.Bar(\n    x=x,\n    y=y2,\n    text=y2,\n    name = 'Classic',\n    textposition = 'auto',\n    marker=dict(\n        color='rgb(0,100,0)',\n        line=dict(\n            color='rgb(8,48,107)',\n            width=1.5),\n        ),\n    opacity=0.6\n)\ngold = go.Bar(\n    x=x,\n    y=y3,\n    text=y3,\n    name = 'Gold',\n    textposition = 'auto',\n    marker=dict(\n        color='rgb(255,215,0)',\n        line=dict(\n            color='rgb(8,48,107)',\n            width=1.5),\n        ),\n    opacity=0.6\n)\ndata = [junior, classic, gold]\npy.iplot(data, filename='grouped-bar-direct-labels')\n\n\n\n\n\n\nBusiness questions part #2 conclusion:\n\nOverall view on the proportion of issued card products: 74% of issued cards are classic cards, 16% - junior cards and 10% - gold.\n\nCard products distribution among different regions of Czech. Is there any regions where specific card products are more popular than another? Majority of cards in all regions are “classic” - more than 70%, junior cards prevail on gold in all regions except for North Bohemia.\n\nCards issuance trends from 1993 to 1998 overview. Number of issued cards growing from year to year. Classic cards remain the most popular among customers. Second place occupied by junior cards, gold cards on the third place, except for 1997 where number of both produced products is equal.\n\n\n\nBusiness questions part #3: loans insight\n\nDoes a loan with higher monthly payment generally tend to be unpaid or overdue?\nDoes longer duration of a loan pretend to protect against overdues in most of the cases?\n\nIs there direct dependence between higher amount of a loan and higher probability the loan to be delinquent?\nLoans status distribution within districts. Are there branches where credit exposure of the bank is not duly managed?\n\n\n#preparing dataset and replacing A, B, C, D with description text\nloans2_df = loans_df.copy ()\nloans2_df ['status_desc'] = loans2_df ['status']\n\ndict1 =  {'A':'Contract finished, no problem', \n      'B':'Contract finised, loan was not paid',\n      'C':'Runing contract, OK so far',\n      'D':'Runing contract, client in debt'\n     }\nloans2_df.status_desc = loans2_df.status_desc.replace (dict1)\n\nloans2_df ['status_numeric'] = loans2_df ['status']\n\n#encoding bad loans as 1 and good ones as -1 \ndict2 =  {'A':-1, \n      'B':1,\n      'C':-1,\n      'D':1\n     }\nloans2_df.status_numeric = loans2_df.status_numeric.replace (dict2)\ndisplay (loans2_df.head (n=3))\n\n\n\n\n\n\n\n\nloan_id\naccount_id\ndate\namount\nduration\npayments\nstatus\nstatus_desc\nstatus_numeric\n\n\n\n\n0\n5314\n1787\n1993-07-05\n96396\n12\n8033.0\nB\nContract finised, loan was not paid\n1\n\n\n1\n5316\n1801\n1993-07-11\n165960\n36\n4610.0\nA\nContract finished, no problem\n-1\n\n\n2\n6863\n9188\n1993-07-28\n127080\n60\n2118.0\nA\nContract finished, no problem\n-1\n\n\n\n\n\n\n\n\n#correlation between loan status and monthly payments\n#display (loans2_df.groupby (['status_desc']).mean () ['payments'].sort_values ())\n\n\n#correlation between loan status and duration of a loan \n#loans2_df.groupby (['status_desc']).mean () ['duration'].sort_values ()\n\n\n#correlation between loan status and loan's amount \n#loans2_df.groupby (['status_desc']).mean () ['amount'].sort_values ()\n\n\n#correlation matrix\nloans3_df = loans2_df.copy ()\nloans3_df.drop (['loan_id', 'account_id'], axis =1, inplace = True)\nsns.heatmap (loans3_df.corr (), annot = True, fmt=\".2f\");\n\n\n\n\n\n# enriching dataframe with accounts_df \n\nloans_acc_df = loans2_df.copy ()\nloans_acc_df = loans_acc_df.merge (accounts_df, left_on = 'account_id', right_on = 'account_id', \n                                           how = 'left', validate = 'many_to_one') \ndisplay (loans2_df.head (n=3))\ndisplay (loans_acc_df.head (n=3))\n\n\n\n\n\n\n\n\nloan_id\naccount_id\ndate\namount\nduration\npayments\nstatus\nstatus_desc\nstatus_numeric\n\n\n\n\n0\n5314\n1787\n1993-07-05\n96396\n12\n8033.0\nB\nContract finised, loan was not paid\n1\n\n\n1\n5316\n1801\n1993-07-11\n165960\n36\n4610.0\nA\nContract finished, no problem\n-1\n\n\n2\n6863\n9188\n1993-07-28\n127080\n60\n2118.0\nA\nContract finished, no problem\n-1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nloan_id\naccount_id\ndate_x\namount\nduration\npayments\nstatus\nstatus_desc\nstatus_numeric\ndistrict_id\nfrequency\ndate_y\n\n\n\n\n0\n5314\n1787\n1993-07-05\n96396\n12\n8033.0\nB\nContract finised, loan was not paid\n1\n30\nPOPLATEK TYDNE\n1993-03-22\n\n\n1\n5316\n1801\n1993-07-11\n165960\n36\n4610.0\nA\nContract finished, no problem\n-1\n46\nPOPLATEK MESICNE\n1993-02-13\n\n\n2\n6863\n9188\n1993-07-28\n127080\n60\n2118.0\nA\nContract finished, no problem\n-1\n45\nPOPLATEK MESICNE\n1993-02-08\n\n\n\n\n\n\n\n\n# enriching dataframe with district_df data\n\nloans_acc_distr_df = loans_acc_df.copy ()\nloans_acc_distr_df = loans_acc_distr_df.merge (district_df, left_on = 'district_id', right_on = 'A1', \n                                           how = 'left', validate = 'many_to_one') \ndisplay (loans_acc_df.head (n=3))\ndisplay (loans_acc_distr_df.head (n=3))\n\n\n\n\n\n\n\n\nloan_id\naccount_id\ndate_x\namount\nduration\npayments\nstatus\nstatus_desc\nstatus_numeric\ndistrict_id\nfrequency\ndate_y\n\n\n\n\n0\n5314\n1787\n1993-07-05\n96396\n12\n8033.0\nB\nContract finised, loan was not paid\n1\n30\nPOPLATEK TYDNE\n1993-03-22\n\n\n1\n5316\n1801\n1993-07-11\n165960\n36\n4610.0\nA\nContract finished, no problem\n-1\n46\nPOPLATEK MESICNE\n1993-02-13\n\n\n2\n6863\n9188\n1993-07-28\n127080\n60\n2118.0\nA\nContract finished, no problem\n-1\n45\nPOPLATEK MESICNE\n1993-02-08\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nloan_id\naccount_id\ndate_x\namount\nduration\npayments\nstatus\nstatus_desc\nstatus_numeric\ndistrict_id\n...\nA7\nA8\nA9\nA10\nA11\nA12\nA13\nA14\nA15\nA16\n\n\n\n\n0\n5314\n1787\n1993-07-05\n96396\n12\n8033.0\nB\nContract finised, loan was not paid\n1\n30\n...\n8\n2\n10\n81.8\n9650\n3.38\n3.67\n100\n2985\n2804\n\n\n1\n5316\n1801\n1993-07-11\n165960\n36\n4610.0\nA\nContract finished, no problem\n-1\n46\n...\n7\n3\n10\n73.5\n8369\n1.79\n2.31\n117\n2854\n2618\n\n\n2\n6863\n9188\n1993-07-28\n127080\n60\n2118.0\nA\nContract finished, no problem\n-1\n45\n...\n6\n1\n5\n53.5\n8390\n2.28\n2.89\n132\n2080\n2122\n\n\n\n\n3 rows × 28 columns\n\n\n\n\n#number of duplicated accounts in loans_acc_df. I was checking if several loans can be tied to the same account ? \ntest_df = pd.DataFrame (loans_acc_df ['account_id'].duplicated ())\nprint (test_df [test_df ['account_id'] == True])\n\nEmpty DataFrame\nColumns: [account_id]\nIndex: []\n\n\nThe answer is “no” multiple loans cannot be tied to the same account\n\n# enriching disposition dataframe with clients_df then I will have a df which will contain loan, district, \n#disposition and client info\n\ndispos_client_df = dispos_df.copy ()\ndispos_client_df = dispos_client_df.merge (clients_df, left_on = 'client_id', right_on = 'client_id', \n                                           how = 'left', validate = 'many_to_one') \ndisplay (dispos_df.head (n=3))\ndisplay (dispos_client_df.head (n=3))   \n\n\n\n\n\n\n\n\ndisp_id\nclient_id\naccount_id\ntype\n\n\n\n\n0\n1\n1\n1\nOWNER\n\n\n1\n2\n2\n2\nOWNER\n\n\n2\n3\n3\n2\nDISPONENT\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndisp_id\nclient_id\naccount_id\ntype\nbirth_number\ndistrict_id\nbirth_date\ngender\nage\n\n\n\n\n0\n1\n1\n1\nOWNER\n706213\n18\n1970-12-13\n0.0\n28\n\n\n1\n2\n2\n2\nOWNER\n450204\n1\n1945-02-04\n1.0\n53\n\n\n2\n3\n3\n2\nDISPONENT\n406009\n1\n1940-10-09\n0.0\n58\n\n\n\n\n\n\n\n\n#number of duplicated accounts in dispos_client_df. checking if the same account can be tied to several clients\ntest2_df = pd.DataFrame (dispos_client_df ['account_id'].duplicated ())\n\nprint (dispos_client_df.shape) #number of disposition - clients \nprint (test2_df [test2_df ['account_id'] == True].shape) #number of duplicates\n\nprint ('this means that same accounts can be owned by different clients where one of them can be owner and the second one disponent') \nprint ('Cannot merge enrich loans_acc_distr_df with some customer related data and see if some personal factors affects loan status as same account can be owned by different persons')\n\n(5369, 9)\n(869, 1)\nthis means that same accounts can be owned by different clients where one of them can be owner and the second one disponent\nCannot merge enrich loans_acc_distr_df with some customer related data and see if some personal factors affects loan status as same account can be owned by different persons\n\n\n\n#heatmap for correlation between districts and loan statuses\nloans_acc_distr_df.head (n=3)\n\n\n\n\n\n\n\n\nloan_id\naccount_id\ndate_x\namount\nduration\npayments\nstatus\nstatus_desc\nstatus_numeric\ndistrict_id\n...\nA7\nA8\nA9\nA10\nA11\nA12\nA13\nA14\nA15\nA16\n\n\n\n\n0\n5314\n1787\n1993-07-05\n96396\n12\n8033.0\nB\nContract finised, loan was not paid\n1\n30\n...\n8\n2\n10\n81.8\n9650\n3.38\n3.67\n100\n2985\n2804\n\n\n1\n5316\n1801\n1993-07-11\n165960\n36\n4610.0\nA\nContract finished, no problem\n-1\n46\n...\n7\n3\n10\n73.5\n8369\n1.79\n2.31\n117\n2854\n2618\n\n\n2\n6863\n9188\n1993-07-28\n127080\n60\n2118.0\nA\nContract finished, no problem\n-1\n45\n...\n6\n1\n5\n53.5\n8390\n2.28\n2.89\n132\n2080\n2122\n\n\n\n\n3 rows × 28 columns\n\n\n\n\n## Using the pandas.groupby() method to produce a pivot table:\n\ndist_loans_df = loans_acc_distr_df.groupby(by=['A2']).status_desc.value_counts().sort_index()\ndist_loans_df.head (n=3)\n\nA2       status_desc                  \nBenesov  Contract finished, no problem    1\n         Runing contract, OK so far       5\nBeroun   Contract finished, no problem    1\nName: status_desc, dtype: int64\n\n\n\n## Manipulating the data:\ndist_loans_df = dist_loans_df.unstack()  #Converting the groupby object into a dataset\n#dist_loans_df.head (n=10)\ndist_loans_df.fillna(value=0.0, inplace=True)  #Replacing NaN values by zero\ndist_loans_df = df_row_normalize(dist_loans_df)  #Normalizing its values by the total of each row\n## Converting the normalized float values to percentual int values:\ndist_loans_df = (dist_loans_df*100).astype(float).applymap('{:,.2f}'.format)#\ndist_loans_df.head (n=5)\n\n\n\n\n\n\n\nstatus_desc\nContract finised, loan was not paid\nContract finished, no problem\nRuning contract, OK so far\nRuning contract, client in debt\n\n\nA2\n\n\n\n\n\n\n\n\nBenesov\n0.00\n16.67\n83.33\n0.00\n\n\nBeroun\n0.00\n16.67\n50.00\n33.33\n\n\nBlansko\n0.00\n42.86\n42.86\n14.29\n\n\nBreclav\n14.29\n14.29\n71.43\n0.00\n\n\nBrno - mesto\n8.33\n16.67\n62.50\n12.50\n\n\n\n\n\n\n\n\n#converting df columns dtype into float16 so to make it visible in the heatmap. \ndist_loans_df = dist_loans_df.astype(np.float16)\ndist_loans_df.dtypes\n\nstatus_desc\nContract finised, loan was not paid    float16\nContract finished, no problem          float16\nRuning contract, OK so far             float16\nRuning contract, client in debt        float16\ndtype: object\n\n\n\n# Drawing a heatmap with the numeric values in each cell\nfig3, ax = plt.subplots(figsize=(10, 25))\nfig3.subplots_adjust(top=.965)\nplt.suptitle('Relative loans status distributed by districts', fontsize=14, fontweight='bold')\n\ncbar_kws = {'orientation':\"horizontal\", 'pad':0.13, 'aspect':50}\nsns.heatmap(dist_loans_df, annot=True, fmt='.2f', linewidths=.3, cmap='RdPu', ax=ax, cbar_kws=cbar_kws );\n\n\n\n\n\n\nBusiness questions part #3 (loans insight) conclusion:\n\nGenerally higher monthly payments tied with unpaid or overdue loans.\nFrom the correlation matrix we can conclude that longer duration slightly increases probability for a loan to be delinquent.\nWe can see that higher amount of monthly payment correlated with overdue/unpaid debts at the same time lower amount in most of the cases stands for diligent case.\n\nCases in branches of following districts should be investigated for subject of potential scam or not duly managed credit risks: Strakonice, Sokolov, Opava, Kutna Hora, Klatovy, Domazlice, Bruntal, Beroun. In those districts number of problematic debts is more than 30%.\n\n\n\nBusiness questions part #4: transactions\n\nTransaction types distribution in the dataset\n\nPossible trends and patterns in transactions distribution within given period (1993-1998). Is there any type of trx that was less popular and it’s number growed within time?\n\nCloser look into funds transfer from and to another banks. Is the overall balance positive? Are there any trends with the balance in the given period?\n\n\n#take a look to the subset where operation values are missing\n\ntrans_opermis_df = trans_df [trans_df ['operation'].isnull ()] [trans_df.columns]\ntrans_opermis_df.head (n=3)\n\n\n\n\n\n\n\n\ntrans_id\naccount_id\ndate\ntype\noperation\namount\nbalance\nk_symbol\nbank\naccount\n\n\n\n\n137\n3591421\n1844\n1993-01-31\nPRIJEM\nNaN\n12.6\n3754.6\nUROK\nNaN\nNaN\n\n\n138\n3617490\n2632\n1993-01-31\nPRIJEM\nNaN\n47.5\n15411.5\nUROK\nNaN\nNaN\n\n\n139\n3579543\n1493\n1993-01-31\nPRIJEM\nNaN\n13.3\n5222.3\nUROK\nNaN\nNaN\n\n\n\n\n\n\n\n\n#interpretation why it is missing\n\ntrans_opermis_df.groupby ('type', axis =0).count ()\n\n\n\n\n\n\n\n\ntrans_id\naccount_id\ndate\noperation\namount\nbalance\nk_symbol\nbank\naccount\n\n\ntype\n\n\n\n\n\n\n\n\n\n\n\n\n\nPRIJEM\n183114\n183114\n183114\n0\n183114\n183114\n183114\n0\n0\n\n\n\n\n\n\n\n\ntrans_opermis_df.groupby ('k_symbol', axis =0).count ()\n\n\n\n\n\n\n\n\ntrans_id\naccount_id\ndate\ntype\noperation\namount\nbalance\nbank\naccount\n\n\nk_symbol\n\n\n\n\n\n\n\n\n\n\n\n\n\nUROK\n183114\n183114\n183114\n183114\n0\n183114\n183114\n0\n0\n\n\n\n\n\n\n\nIt appeared that the only possible value in k_symbol for the subset is “UROK” which stands for “interest credited”. We can conclude that operation values are missing since those are transactions “interest credited”, it cannot be filled with any operation type like case withdrawal, remittance or anything else. We can substitute missing values in “operation” with new value “interest_credit”.\n\n#substituting missing values in \"operation\"\ntrans_df ['operation'].fillna ('INTEREST_CREDIT', inplace = True)\n\n#filling all missing values as np.NaN\ntrans_df.fillna (np.NaN, inplace = True)\n\ntrans_df.isnull ().sum ()\n\ntrans_id           0\naccount_id         0\ndate               0\ntype               0\noperation          0\namount             0\nbalance            0\nk_symbol      481881\nbank          782812\naccount       760931\ndtype: int64\n\n\n\n#transactions popularity for the whole reported period \n\ntrans_count = Counter (trans_df ['operation'])\n\ntrans_counts = pd.DataFrame.from_dict (trans_count, orient = 'index')\ntrans_counts.columns = ['Percentage_ratio']\ntrans_counts = trans_counts / (trans_counts ['Percentage_ratio'].sum ()) * 100\n\n#renaming indices \ntrans_counts.rename (index = {'VKLAD':'Credit in cash', 'PREVOD Z UCTU':'Collection from another bank', 'VYBER':'Withdrawal in cash',\n                             'INTEREST_CREDIT':'Interest credit', 'PREVOD NA UCET':'Remittance to another bank', \n                             'VYBER KARTOU':'Withdrawal from credit card'}, inplace = True)#, , , \n                            #'Remittance to another bank', ], inplace = True)\n\ntrans_counts.plot (kind = 'bar',\n                    title = 'Transaction types distribution',\n                    width = 0.75,\n                    figsize = (8, 5.8),\n                    subplots = True, \n                    color = [plt.cm.Paired(np.arange(len(region_counts)))]\n                    )\ntrans_counts #pd.DataFrame (list(trans_count.items ())) #/trans_df.shape [0]*100\n\n\n\n\n\n\n\n\nPercentage_ratio\n\n\n\n\nCredit in cash\n14.838591\n\n\nCollection from another bank\n6.174833\n\n\nWithdrawal in cash\n41.172940\n\n\nInterest credit\n17.335088\n\n\nRemittance to another bank\n19.717794\n\n\nWithdrawal from credit card\n0.760754\n\n\n\n\n\n\n\n\n\n\n\n#adding a column indicating a year when a transaction was performed \ntrans_df ['tran_year'] = trans_df ['date']\ntrans_df ['tran_year'] = trans_df ['tran_year'].apply (year_extract)\n    \ntrans_df.head (n=3)\n\n\n\n\n\n\n\n\ntrans_id\naccount_id\ndate\ntype\noperation\namount\nbalance\nk_symbol\nbank\naccount\ntran_year\n\n\n\n\n0\n695247\n2378\n1993-01-01\nPRIJEM\nVKLAD\n700.0\n700.0\nNaN\nNaN\nNaN\n1993\n\n\n1\n171812\n576\n1993-01-01\nPRIJEM\nVKLAD\n900.0\n900.0\nNaN\nNaN\nNaN\n1993\n\n\n2\n207264\n704\n1993-01-01\nPRIJEM\nVKLAD\n1000.0\n1000.0\nNaN\nNaN\nNaN\n1993\n\n\n\n\n\n\n\n\n#grouping by tran_year and operation, then counting number of transactions  \nyear_trantype_df = pd.DataFrame (trans_df.groupby (['tran_year', 'operation'], axis = 0) ['operation'].count ())\n\n#naming column as 'count' \nyear_trantype_df.columns = ['count']\n\n#making columns out of multi-index of tran_year and operation so to make it easier to operate with values\nyear_trantype_df.reset_index (level= ['operation', 'tran_year'], inplace = True)\n\ndisplay (year_trantype_df .head (n=3))\n\n\n\n\n\n\n\n\ntran_year\noperation\ncount\n\n\n\n\n0\n1993\nINTEREST_CREDIT\n6065\n\n\n1\n1993\nPREVOD NA UCET\n3231\n\n\n2\n1993\nPREVOD Z UCTU\n2462\n\n\n\n\n\n\n\n\n#no values for VYBER KARTOU in 1993 \n#adding as zero\n\nlist_of_zer1 = [pd.Series ([1993, 'VYBER KARTOU', 0], index = year_trantype_df.columns)]\nyear_trantype_df = year_trantype_df.append (list_of_zer1, ignore_index = True)\n\nyear_trantype_df.sort_values (['tran_year', 'operation'], inplace= True)\nyear_trantype_df.reset_index (inplace = True, drop = True) #making index start from 0 again\ndisplay (year_trantype_df .head (n=6))\n\n\n\n\n\n\n\n\ntran_year\noperation\ncount\n\n\n\n\n0\n1993\nINTEREST_CREDIT\n6065\n\n\n1\n1993\nPREVOD NA UCET\n3231\n\n\n2\n1993\nPREVOD Z UCTU\n2462\n\n\n3\n1993\nVKLAD\n6685\n\n\n4\n1993\nVYBER\n9762\n\n\n5\n1993\nVYBER KARTOU\n0\n\n\n\n\n\n\n\n\n#plotting number of transactions per year \n\nx = list (year_trantype_df ['tran_year'].unique()) #years \ny =  list (year_trantype_df [year_trantype_df ['operation'] == 'INTEREST_CREDIT'] ['count']) \ny2 = list (year_trantype_df [year_trantype_df ['operation'] == 'PREVOD NA UCET'] ['count']) \ny3 = list (year_trantype_df [year_trantype_df ['operation'] == 'PREVOD Z UCTU'] ['count']) \ny4 = list (year_trantype_df [year_trantype_df ['operation'] == 'VKLAD'] ['count']) \ny5 = list (year_trantype_df [year_trantype_df ['operation'] == 'VYBER'] ['count']) \ny6 = list (year_trantype_df [year_trantype_df ['operation'] == 'VYBER KARTOU'] ['count']) \n\nbar1 = go.Bar(\n    x=x,\n    y=y,\n    text= y,\n    name = 'Interest credit',\n    textposition = 'auto',\n    marker=dict(\n        color='rgb(158,202,225)',\n        line=dict(\n            color='rgb(8,48,107)',\n            width=1.5),\n        ),\n    opacity=0.6\n)\n\nbar2 = go.Bar(\n    x=x,\n    y=y2,\n    text=y2,\n    name = 'Remittance to another bank',\n    textposition = 'auto',\n    marker=dict(\n        color='rgb(0,100,0)',\n        line=dict(\n            color='rgb(8,48,107)',\n            width=1.5),\n        ),\n    opacity=0.6\n)\n\nbar3 = go.Bar(\n    x=x,\n    y=y3,\n    text=y3,\n    name = 'Collection from another bank',\n    textposition = 'auto',\n    marker=dict(\n        color='rgb(128,0,128)',\n        line=dict(\n            color='rgb(8,48,107)',\n            width=1.5),\n        ),\n    opacity=0.6\n)\n\nbar4 = go.Bar(\n    x=x,\n    y=y4,\n    text=y4,\n    name = 'Credit in cash',\n    textposition = 'auto',\n    marker=dict(\n        color='rgb(0,139,139)',\n        line=dict(\n            color='rgb(8,48,107)',\n            width=1.5),\n        ),\n    opacity=0.6\n)\n\nbar5 = go.Bar(\n    x=x,\n    y=y5,\n    text=y5,\n    name = 'Withdrawal in cash',\n    textposition = 'auto',\n    marker=dict(\n        color='rgb(255,215,0)',\n        line=dict(\n            color='rgb(8,48,107)',\n            width=1.5),\n        ),\n    opacity=0.6\n)\n\nbar6 = go.Bar(\n    x=x,\n    y=y6,\n    text=y6,\n    name = 'Withdrawal from credit card',\n    textposition = 'auto',\n    marker=dict(\n        color='rgb(255,0,0)',\n        line=dict(\n            color='rgb(8,48,107)',\n            width=1.5),\n        ),\n    opacity=0.6\n)\n\ndata = [bar1, bar2, bar3, bar4, bar5, bar6]\n\npy.iplot(data, filename='grouped-bar-direct-labels')\n\n\n\n\n\n#investigating transactions marked as \"collection from another bank\" \n\ntrans_col_df = trans_df [trans_df ['operation'] == 'PREVOD Z UCTU']\n\n#unique values in the column k_symbol which will describe possible reason for collection\ntrans_col_df ['k_symbol'].unique ()\n\narray(['DUCHOD', nan], dtype=object)\n\n\n\n#checking proportion of each possible value: DUCHOD ( old-age pension) and NaNs\npd.DataFrame(Counter (trans_col_df ['k_symbol']), index = [0]).transpose () / trans_col_df.shape [0] * 100\n\n\n\n\n\n\n\n\n0\n\n\n\n\nDUCHOD\n46.512127\n\n\nNaN\n53.487873\n\n\n\n\n\n\n\nFrom the above we can conclude that 46.5% of “collection from another bank” transactions stands for old-age pension and 53.5% transactions were not specifically identified which may related to funds transfers between clients.\n\ntrans_col_df [trans_col_df ['k_symbol'] == 'DUCHOD'] ['amount'].sum ()\n\n167472118.0\n\n\n\ntrans_col_df [trans_col_df ['k_symbol'] != 'DUCHOD'] ['amount'].sum ()\n\n614007835.0\n\n\n\n#investigating transactions marked as \"remittance to another bank\"\ntrans_rem_df = trans_df [trans_df ['operation'] == 'PREVOD NA UCET']\n\n#unique values in the column k_symbol which will describe possible reason for collection\ntrans_rem_df ['k_symbol'].unique ()\n\narray(['SIPO', ' ', 'POJISTNE', nan, 'UVER'], dtype=object)\n\n\n\n#filling spaces in k_symbol with NaN\ntrans_rem_df = trans_rem_df.replace(r'^\\s*$', np.nan, regex=True)\n\n\n#verifying result \ntrans_rem_df ['k_symbol'].unique ()\n\narray(['SIPO', nan, 'POJISTNE', 'UVER'], dtype=object)\n\n\n\n#checking proportion of each possible value for remittance transactions\npd.DataFrame(Counter (trans_rem_df ['k_symbol']), index = [0]).transpose () / trans_rem_df.shape [0] * 100\n\n\n\n\n\n\n\n\n0\n\n\n\n\nSIPO\n55.335289\n\n\nNaN\n29.273633\n\n\nPOJISTNE\n8.871103\n\n\nUVER\n6.519975\n\n\n\n\n\n\n\nFrom the above we see that majority (55.3%) of remittance transactions related to household payments, on the second place (29.3%) payment reason unknown, on the third (8.9%) - insurance payments, on the last place (6.5%) - loan payments.\n\n#verifying positive/negative balance between collection from another banks and remittance to another banks\ntrans_colrem_df = trans_col_df.append (trans_rem_df)\n\n#overall balance of incoming (PREVOD Z UCTU) and outgoing (PREVOD NA UCET)\ndisplay (pd.DataFrame (trans_colrem_df.groupby (['operation']).sum () ['amount'].sort_values ()))\n\n#proportion \ndisplay (pd.DataFrame (trans_colrem_df.groupby (['operation']).sum () ['amount'].sort_values ())/trans_colrem_df ['amount'].sum () *100)\n\n\n\n\n\n\n\n\namount\n\n\noperation\n\n\n\n\n\nPREVOD NA UCET\n672637786.3\n\n\nPREVOD Z UCTU\n781479953.0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\namount\n\n\noperation\n\n\n\n\n\nPREVOD NA UCET\n46.25745\n\n\nPREVOD Z UCTU\n53.74255\n\n\n\n\n\n\n\n\n#mean of remittance and collection \ndisplay (pd.DataFrame (trans_colrem_df.groupby (['operation']).mean () ['amount'].sort_values ()))\n\n\n\n\n\n\n\n\namount\n\n\noperation\n\n\n\n\n\nPREVOD NA UCET\n3229.441607\n\n\nPREVOD Z UCTU\n11981.111106\n\n\n\n\n\n\n\nFrom 1993 to 1998 overall balance is positive: received amount is bigger for 7.5% comparing to outgoing.\n\n#now let's take a look if there is any trend from 1993 to 1998 in the amounts for remittance and collection\nyear_transfer_df = pd.DataFrame (trans_colrem_df.groupby (['operation','tran_year'], axis = 0) ['amount'] .sum ().sort_values ())\n\n#naming column as 'sum' \nyear_transfer_df.columns = ['sum']\n\n#making columns out of multi-index so to make it easier to operate with values\nyear_transfer_df.reset_index (inplace = True)\n\n#renaming values in operation so to have better visibility\ndict11 =  {'PREVOD NA UCET':'Remittance to another bank', \n      'PREVOD Z UCTU':'Collection from another bank'\n     }\nyear_transfer_df.operation = year_transfer_df.operation.replace (dict11)\n\ndisplay (year_transfer_df .head (n=30))\n\n\n\n\n\n\n\n\noperation\ntran_year\nsum\n\n\n\n\n0\nRemittance to another bank\n1993\n10159410.7\n\n\n1\nCollection from another bank\n1993\n29809129.0\n\n\n2\nRemittance to another bank\n1994\n53841549.1\n\n\n3\nCollection from another bank\n1994\n68925764.0\n\n\n4\nRemittance to another bank\n1995\n81845324.4\n\n\n5\nCollection from another bank\n1995\n99242619.0\n\n\n6\nRemittance to another bank\n1996\n119563522.2\n\n\n7\nCollection from another bank\n1996\n146015899.0\n\n\n8\nRemittance to another bank\n1997\n182443823.2\n\n\n9\nCollection from another bank\n1997\n207339926.0\n\n\n10\nRemittance to another bank\n1998\n224784156.7\n\n\n11\nCollection from another bank\n1998\n230146616.0\n\n\n\n\n\n\n\n\n#plotting sum of remittance and collection per years \nx = list (year_transfer_df ['tran_year'].unique()) #years \ny =  list (year_transfer_df [year_transfer_df ['operation'] == 'Remittance to another bank'] ['sum']) \ny2 = list (year_transfer_df [year_transfer_df ['operation'] == 'Collection from another bank'] ['sum']) \n\nbar2 = go.Bar(\n    x=x,\n    y=y,\n    text=y,\n    name = 'Remittance to another bank',\n    textposition = 'auto',\n    marker=dict(\n        color='rgb(0,100,0)',\n        line=dict(\n            color='rgb(8,48,107)',\n            width=1.5),\n        ),\n    opacity=0.6\n)\nbar5 = go.Bar(\n    x=x,\n    y=y2,\n    text=y2,\n    name = 'Collection from another bank',\n    textposition = 'auto',\n    marker=dict(\n        color='rgb(255,215,0)',\n        line=dict(\n            color='rgb(8,48,107)',\n            width=1.5),\n        ),\n    opacity=0.6\n)\ndata = [bar2, bar5]\npy.iplot(data, filename='grouped-bar-direct-labels')\n\n\n\n\n\n#a dataframe for plotting remittance and colletion by months distribution\ntrans_remcol_df = trans_colrem_df.copy ()\n\n#columns with amounts that affects balance positively - collection from another banks \ntrans_remcol_df = trans_remcol_df.assign (pos_sum = np.nan)\n\n#columns with amounts that affects balance negatively - remittance to another banks\ntrans_remcol_df = trans_remcol_df.assign (neg_sum = np.nan)\n\n#filling columns pos_sum and neg_sum based on operation type. pos_sum stands for inflows\ntrans_remcol_df.pos_sum = trans_remcol_df.amount.where (trans_remcol_df.operation == 'PREVOD Z UCTU', trans_remcol_df.pos_sum)\n\n#neg_sum stands for outflows\ntrans_remcol_df.neg_sum = trans_remcol_df.amount.where (trans_remcol_df.operation == 'PREVOD NA UCET', trans_remcol_df.neg_sum )\n\ndisplay (trans_remcol_df.head (n=3))\ntrans_remcol_df [trans_remcol_df ['operation'] == 'PREVOD Z UCTU'].tail (n=3)\n\n\n\n\n\n\n\n\ntrans_id\naccount_id\ndate\ntype\noperation\namount\nbalance\nk_symbol\nbank\naccount\ntran_year\npos_sum\nneg_sum\n\n\n\n\n15\n637742\n2177\n1993-01-05\nPRIJEM\nPREVOD Z UCTU\n5123.0\n5923.0\nDUCHOD\nYZ\n62457513.0\n1993\n5123.0\nNaN\n\n\n17\n232961\n793\n1993-01-05\nPRIJEM\nPREVOD Z UCTU\n3401.0\n4201.0\nNaN\nIJ\n6149286.0\n1993\n3401.0\nNaN\n\n\n21\n542216\n1844\n1993-01-07\nPRIJEM\nPREVOD Z UCTU\n3242.0\n3742.0\nNaN\nST\n42988401.0\n1993\n3242.0\nNaN\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntrans_id\naccount_id\ndate\ntype\noperation\namount\nbalance\nk_symbol\nbank\naccount\ntran_year\npos_sum\nneg_sum\n\n\n\n\n1047122\n546460\n1860\n1998-12-14\nPRIJEM\nPREVOD Z UCTU\n3255.0\n20916.3\nNaN\nMN\n61854898.0\n1998\n3255.0\nNaN\n\n\n1047127\n605757\n2062\n1998-12-14\nPRIJEM\nPREVOD Z UCTU\n64642.0\n97552.6\nNaN\nEF\n10179949.0\n1998\n64642.0\nNaN\n\n\n1047137\n519257\n1773\n1998-12-14\nPRIJEM\nPREVOD Z UCTU\n4316.0\n17215.9\nDUCHOD\nCD\n77385341.0\n1998\n4316.0\nNaN\n\n\n\n\n\n\n\n\n#we need a pandas.DateTimeIndex for resampling\ntrans_remcol_df.set_index (trans_remcol_df ['date'], inplace = True)\ntrans_remcol_df.head (n=3)\n\n\n\n\n\n\n\n\ntrans_id\naccount_id\ndate\ntype\noperation\namount\nbalance\nk_symbol\nbank\naccount\ntran_year\npos_sum\nneg_sum\n\n\ndate\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1993-01-05\n637742\n2177\n1993-01-05\nPRIJEM\nPREVOD Z UCTU\n5123.0\n5923.0\nDUCHOD\nYZ\n62457513.0\n1993\n5123.0\nNaN\n\n\n1993-01-05\n232961\n793\n1993-01-05\nPRIJEM\nPREVOD Z UCTU\n3401.0\n4201.0\nNaN\nIJ\n6149286.0\n1993\n3401.0\nNaN\n\n\n1993-01-07\n542216\n1844\n1993-01-07\nPRIJEM\nPREVOD Z UCTU\n3242.0\n3742.0\nNaN\nST\n42988401.0\n1993\n3242.0\nNaN\n\n\n\n\n\n\n\n\n#plotting results with aggregation on the level of years\ninflows = trans_remcol_df['pos_sum'].resample('Y').sum ()\noutflows = trans_remcol_df['neg_sum'].resample('Y').sum ()\n\nplt.style.use('fivethirtyeight')\nx = outflows.index\nfig, ax = plt.subplots(figsize = (10,6))\ny1 = list (inflows / 1000000) \ny2 = list (outflows / 1000000)\n\nax.set_ylabel('Millions')\nax.plot(x, y1, label = 'Collection')\nax.plot(x, y2, label = 'Remittance' )#, figsize = (8,8))\nax.set_title(\"Collection from and remittance to another banks (years)\")\nplt.legend (loc = 'best')\nplt.show()\n\n\n\n\n\n#plotting results with aggregation on the level of months\ninflows = trans_remcol_df['pos_sum'].resample('M').sum ()\noutflows = trans_remcol_df['neg_sum'].resample('M').sum ()\n\nplt.style.use('fivethirtyeight')\nx = outflows.index\nfig, ax = plt.subplots(figsize = (10,6))\ny1 = list (inflows / 1000000) \ny2 = list (outflows / 1000000)\n\nax.set_ylabel('Millions')\nax.plot(x, y1, label = 'Collection')\nax.plot(x, y2, label = 'Remittance' )#, figsize = (8,8))\nax.set_title(\"Collection from and remittance to another banks (months)\")\nplt.legend (loc = 'best')\nplt.show()\n\n\n\n\n\n#complementing trans_df with account data and district data\ntrans_acc_df = trans_df.copy ()\ntrans_acc_df = trans_acc_df.merge (accounts_df, left_on = 'account_id', right_on = 'account_id', \n                                           how = 'left', validate = 'many_to_one') \ntrans_acc_dist_df = trans_acc_df.merge (district_df, left_on = 'district_id', right_on = 'A1', \n                                           how = 'left', validate = 'many_to_one')\ndisplay (trans_df.head (n=3))\ndisplay (trans_acc_df.head (n=3)) \ndisplay (trans_acc_dist_df.head (n=3))\n\n\n\n\n\n\n\n\ntrans_id\naccount_id\ndate\ntype\noperation\namount\nbalance\nk_symbol\nbank\naccount\ntran_year\n\n\n\n\n0\n695247\n2378\n1993-01-01\nPRIJEM\nVKLAD\n700.0\n700.0\nNaN\nNaN\nNaN\n1993\n\n\n1\n171812\n576\n1993-01-01\nPRIJEM\nVKLAD\n900.0\n900.0\nNaN\nNaN\nNaN\n1993\n\n\n2\n207264\n704\n1993-01-01\nPRIJEM\nVKLAD\n1000.0\n1000.0\nNaN\nNaN\nNaN\n1993\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntrans_id\naccount_id\ndate_x\ntype\noperation\namount\nbalance\nk_symbol\nbank\naccount\ntran_year\ndistrict_id\nfrequency\ndate_y\n\n\n\n\n0\n695247\n2378\n1993-01-01\nPRIJEM\nVKLAD\n700.0\n700.0\nNaN\nNaN\nNaN\n1993\n16\nPOPLATEK MESICNE\n1993-01-01\n\n\n1\n171812\n576\n1993-01-01\nPRIJEM\nVKLAD\n900.0\n900.0\nNaN\nNaN\nNaN\n1993\n55\nPOPLATEK MESICNE\n1993-01-01\n\n\n2\n207264\n704\n1993-01-01\nPRIJEM\nVKLAD\n1000.0\n1000.0\nNaN\nNaN\nNaN\n1993\n55\nPOPLATEK MESICNE\n1993-01-01\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntrans_id\naccount_id\ndate_x\ntype\noperation\namount\nbalance\nk_symbol\nbank\naccount\n...\nA7\nA8\nA9\nA10\nA11\nA12\nA13\nA14\nA15\nA16\n\n\n\n\n0\n695247\n2378\n1993-01-01\nPRIJEM\nVKLAD\n700.0\n700.0\nNaN\nNaN\nNaN\n...\n10\n1\n8\n56.9\n8427\n1.12\n1.54\n107\n1874\n1913\n\n\n1\n171812\n576\n1993-01-01\nPRIJEM\nVKLAD\n900.0\n900.0\nNaN\nNaN\nNaN\n...\n18\n0\n9\n33.9\n8743\n1.88\n2.43\n111\n3659\n3894\n\n\n2\n207264\n704\n1993-01-01\nPRIJEM\nVKLAD\n1000.0\n1000.0\nNaN\nNaN\nNaN\n...\n18\n0\n9\n33.9\n8743\n1.88\n2.43\n111\n3659\n3894\n\n\n\n\n3 rows × 30 columns\n\n\n\n\ntrans_bal_df = trans_df.copy ()\n\n#column with amounts that affects balance positively \ntrans_bal_df = trans_bal_df.assign (pos_sum = np.nan)\n\n#column with amounts that affects balance negatively \ntrans_bal_df = trans_bal_df.assign (neg_sum = np.nan)\n\n#filling columns pos_sum and neg_sum based on operation type. pos_sum stands for inflows\ntrans_bal_df.pos_sum = trans_bal_df.amount.where (trans_bal_df.operation == 'VKLAD', trans_bal_df.pos_sum )\ntrans_bal_df.pos_sum = trans_bal_df.amount.where (trans_bal_df.operation == 'PREVOD Z UCTU', trans_bal_df.pos_sum)\n\n#neg_sum stands for outflows\ntrans_bal_df.neg_sum = trans_bal_df.amount.where (trans_bal_df.operation == 'VYBER', trans_bal_df.neg_sum )\ntrans_bal_df.neg_sum = trans_bal_df.amount.where (trans_bal_df.operation == 'PREVOD NA UCET', trans_bal_df.neg_sum )\ntrans_bal_df.neg_sum = trans_bal_df.amount.where (trans_bal_df.operation == 'VYBER KARTOU', trans_bal_df.neg_sum )\n\ntrans_bal_df [trans_bal_df ['operation'] == 'VYBER'].tail (n=3)\n\n\n\n\n\n\n\n\ntrans_id\naccount_id\ndate\ntype\noperation\namount\nbalance\nk_symbol\nbank\naccount\ntran_year\npos_sum\nneg_sum\n\n\n\n\n1055639\n3489430\n7520\n1998-12-31\nVYDAJ\nVYBER\n67.2\n-11020.4\nSANKC. UROK\nNaN\nNaN\n1998\nNaN\n67.2\n\n\n1055676\n3488804\n7465\n1998-12-31\nVYDAJ\nVYBER\n0.8\n84975.2\nSANKC. UROK\nNaN\nNaN\n1998\nNaN\n0.8\n\n\n1055731\n3545165\n442\n1998-12-31\nVYDAJ\nVYBER\n0.6\n54376.5\nSANKC. UROK\nNaN\nNaN\n1998\nNaN\n0.6\n\n\n\n\n\n\n\n\n#setting date as index for further visualization\ntrans_bal_df.set_index (trans_bal_df ['date'], inplace = True)\ntrans_bal_df.head (n=3)\n\n\n\n\n\n\n\n\ntrans_id\naccount_id\ndate\ntype\noperation\namount\nbalance\nk_symbol\nbank\naccount\ntran_year\npos_sum\nneg_sum\n\n\ndate\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1993-01-01\n695247\n2378\n1993-01-01\nPRIJEM\nVKLAD\n700.0\n700.0\nNaN\nNaN\nNaN\n1993\n700.0\nNaN\n\n\n1993-01-01\n171812\n576\n1993-01-01\nPRIJEM\nVKLAD\n900.0\n900.0\nNaN\nNaN\nNaN\n1993\n900.0\nNaN\n\n\n1993-01-01\n207264\n704\n1993-01-01\nPRIJEM\nVKLAD\n1000.0\n1000.0\nNaN\nNaN\nNaN\n1993\n1000.0\nNaN\n\n\n\n\n\n\n\n\n#Outflows and inflows trends - years aggregation\ninflows = trans_bal_df['pos_sum'].resample('Y').sum ()\noutflows = trans_bal_df['neg_sum'].resample('Y').sum ()\n\nplt.style.use('fivethirtyeight')\nx = outflows.index\nfig, ax = plt.subplots(figsize = (10,6))\ny1 = list (inflows / 1000000) \ny2 = list (outflows / 1000000)\nax.set_ylabel('Millions')\nax.plot(x, y1, label = 'Inflows')\nax.plot(x, y2, label = 'Outflows' )#, figsize = (8,8))\nax.set_title(\"Outflows and inflows trends (years)\")\nplt.legend (loc = 'best')\nplt.show()\n\n\n\n\n\n#Outflows and inflows trends - months aggregation\ninflows = trans_bal_df['pos_sum'].resample('M').sum ()\noutflows = trans_bal_df['neg_sum'].resample('M').sum ()\n\nplt.style.use('fivethirtyeight')\nx = outflows.index\nfig, ax = plt.subplots(figsize = (10,6))\ny1 = list (inflows / 1000000) \ny2 = list (outflows / 1000000)\nax.set_ylabel('Millions')\nax.plot(x, y1, label = 'Inflows')\nax.plot(x, y2, label = 'Outflows' )#, figsize = (8,8))\nax.set_title(\"Outflows and inflows trends (months)\")\nplt.legend (loc = 'best')\nplt.show()\n\n\n\n\n\n#Outflows and inflows trends - days aggregation\ninflows = trans_bal_df['pos_sum'].resample('D').sum ()\noutflows = trans_bal_df['neg_sum'].resample('D').sum ()\n\nplt.style.use('fivethirtyeight')\nx = outflows.index\nfig, ax = plt.subplots(figsize = (10,6))\ny1 = list (inflows / 1000000) \ny2 = list (outflows / 1000000)\nax.set_ylabel('Millions')\nax.plot(x, y1, label = 'Inflows')\nax.plot(x, y2, label = 'Outflows' )#, figsize = (8,8))\nax.set_title(\"Outflows and inflows trends (days)\")\nplt.legend (loc = 'best')\nplt.show()\n\n\n\n\n\n#verifying overall balance of inflows and outflows for the whole reported period\ndisplay (trans_bal_df['neg_sum'].sum ())\ndisplay (trans_bal_df['pos_sum'].sum ())\n\n3030321135.799999\n\n\n3200001735.0\n\n\n\n#Outflows details investigation\n\ntrans_negflow_df = trans_df.copy ()\n\n#column with amounts that stand for cash withdrawal \ntrans_negflow_df = trans_negflow_df.assign (cw_sum = np.nan)\n\n#column with amounts that stand for withdrawal from credit card \ntrans_negflow_df = trans_negflow_df.assign (cwc_sum = np.nan)\n\n#column with amounts that stand for remittance to another banks \ntrans_negflow_df = trans_negflow_df.assign (rem_sum = np.nan)\n\n#filling columns pos_sum and neg_sum based on operation type. pos_sum stands for inflows\ntrans_negflow_df.cw_sum = trans_negflow_df.amount.where (trans_negflow_df.operation == 'VYBER', trans_negflow_df.cw_sum )\ntrans_negflow_df.cwc_sum = trans_negflow_df.amount.where (trans_negflow_df.operation == 'VYBER KARTOU', trans_negflow_df.cwc_sum)\ntrans_negflow_df.rem_sum = trans_negflow_df.amount.where (trans_negflow_df.operation == 'PREVOD NA UCET', trans_negflow_df.rem_sum)\n\n#trans_negflow_df [trans_negflow_df ['operation'] == 'PREVOD NA UCET'].head (n=5)\n\n\n#setting date as index for further visualization\ntrans_negflow_df.set_index (trans_negflow_df ['date'], inplace = True)\ntrans_negflow_df.head (n=3)\n\n\n\n\n\n\n\n\ntrans_id\naccount_id\ndate\ntype\noperation\namount\nbalance\nk_symbol\nbank\naccount\ntran_year\ncw_sum\ncwc_sum\nrem_sum\n\n\ndate\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1993-01-01\n695247\n2378\n1993-01-01\nPRIJEM\nVKLAD\n700.0\n700.0\nNaN\nNaN\nNaN\n1993\nNaN\nNaN\nNaN\n\n\n1993-01-01\n171812\n576\n1993-01-01\nPRIJEM\nVKLAD\n900.0\n900.0\nNaN\nNaN\nNaN\n1993\nNaN\nNaN\nNaN\n\n\n1993-01-01\n207264\n704\n1993-01-01\nPRIJEM\nVKLAD\n1000.0\n1000.0\nNaN\nNaN\nNaN\n1993\nNaN\nNaN\nNaN\n\n\n\n\n\n\n\n\n#plotting results outflows decomposition (years)\ncashw = trans_negflow_df['cw_sum'].resample('Y').sum ()\ncashw_card = trans_negflow_df['cwc_sum'].resample('Y').sum ()\nremit = trans_negflow_df['rem_sum'].resample('Y').sum ()\n\nplt.style.use('fivethirtyeight')\nx = cashw.index\nfig, ax = plt.subplots(figsize = (10,6))\ny1 = list (cashw / 1000000) \ny2 = list (remit / 1000000)\ny3 = list (cashw_card / 1000000)\nax.set_ylabel('Millions')\nax.plot(x, y1, label = 'Cash withdrawal')\nax.plot(x, y2, label = 'Remittance to another banks' )#, figsize = (8,8))\nax.plot(x, y3, label = 'Cash withdrawal from credit card' )#, figsize = (8,8))\nax.set_title(\"Outflows decomposition (years)\")\nplt.legend (loc = 'best')\nplt.show()\n\n\n\n\n\n#plotting results outflows decomposition (months)\ncashw = trans_negflow_df['cw_sum'].resample('M').sum ()\ncashw_card = trans_negflow_df['cwc_sum'].resample('M').sum ()\nremit = trans_negflow_df['rem_sum'].resample('M').sum ()\n\nplt.style.use('fivethirtyeight')\nx = cashw.index\nfig, ax = plt.subplots(figsize = (10,6))\ny1 = list (cashw / 1000000) \ny2 = list (remit / 1000000)\ny3 = list (cashw_card / 1000000)\nax.set_ylabel('Millions')\nax.plot(x, y1, label = 'Cash withdrawal')\nax.plot(x, y2, label = 'Remittance to another banks' )#, figsize = (8,8))\nax.plot(x, y3, label = 'Cash withdrawal from credit card' )#, figsize = (8,8))\nax.set_title(\"Outflows decomposition (months)\")\nplt.legend (loc = 'best')\nplt.show()\n\n\n\n\n\npd.DataFrame (cashw/1000000).head (n=24)\n\n\n\n\n\n\n\n\ncw_sum\n\n\ndate\n\n\n\n\n\n1993-01-31\n0.034700\n\n\n1993-02-28\n0.271815\n\n\n1993-03-31\n0.852493\n\n\n1993-04-30\n2.026936\n\n\n1993-05-31\n3.897119\n\n\n1993-06-30\n7.180022\n\n\n1993-07-31\n6.676697\n\n\n1993-08-31\n7.994952\n\n\n1993-09-30\n8.471902\n\n\n1993-10-31\n10.318828\n\n\n1993-11-30\n10.782028\n\n\n1993-12-31\n15.199389\n\n\n1994-01-31\n21.031592\n\n\n1994-02-28\n9.571364\n\n\n1994-03-31\n13.809816\n\n\n1994-04-30\n13.718473\n\n\n1994-05-31\n15.815229\n\n\n1994-06-30\n25.609627\n\n\n1994-07-31\n15.466124\n\n\n1994-08-31\n15.846305\n\n\n1994-09-30\n16.992539\n\n\n1994-10-31\n17.482733\n\n\n1994-11-30\n17.032683\n\n\n1994-12-31\n23.894643\n\n\n\n\n\n\n\nPeaks of cashwithdrawal in Jan (rarely Feb) and June.\n\n#plotting results outflows decomposition (days)\ncashw = trans_negflow_df['cw_sum'].resample('D').sum ()\ncashw_card = trans_negflow_df['cwc_sum'].resample('D').sum ()\nremit = trans_negflow_df['rem_sum'].resample('D').sum ()\n\nplt.style.use('fivethirtyeight')\nx = cashw.index\nfig, ax = plt.subplots(figsize = (15,6))\ny1 = list (cashw / 1000000) \ny2 = list (remit / 1000000)\ny3 = list (cashw_card / 1000000)\nax.set_ylabel('Millions')\nax.plot(x, y1, label = 'Cash withdrawal')\nax.plot(x, y2, label = 'Remittance to another banks' )#, figsize = (8,8))\nax.plot(x, y3, label = 'Cash withdrawal from credit card' )#, figsize = (8,8))\nax.set_title(\"Outflows decomposition (days)\")\nplt.legend (loc = 'best')\nplt.show()\n\n\n\n\n\n\nBusiness questions part #4 (transactions) conclusion:\n\nTransaction types distribution in the dataset: traditionally cash withdrawal is the most popular operation (41%), 19% for remittance from another banks, 17% for interest credit, the least used is cash withdrawal from credit card which is reasonable as this transaction is usually chargeable for customers.\n\nTransactions distribution within given period (1993-1998) mostly the same. Despite the fact that number of transaction for remittance to another bank is bigger than number of collection from another banks (in some years more than 2 times), overall sum within year is smaller and the balance is positive.\nInflows prevail over outflows during reported period (year, months) although in some specific days it is different."
  },
  {
    "objectID": "posts/AS4_5.html",
    "href": "posts/AS4_5.html",
    "title": "AS HW4_5",
    "section": "",
    "text": "야구 데이터\n로지스틱을 돌릴만한 데이터가 ,, 야구에 있나?"
  },
  {
    "objectID": "posts/AS4_5.html#model1원본",
    "href": "posts/AS4_5.html#model1원본",
    "title": "AS HW4_5",
    "section": "model1(원본)",
    "text": "model1(원본)\n\nmodel1 &lt;- lm(연봉.2018. ~ ., dt)\nsummary(model1)\n\n\nCall:\nlm(formula = 연봉.2018. ~ ., data = dt)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-46529  -2418    424   2649  47773 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  1.513e+04  1.826e+04   0.829   0.4087    \n승           1.004e+03  5.375e+02   1.869   0.0639 .  \n패          -1.836e+02  5.504e+02  -0.334   0.7392    \n세          -2.112e+01  2.713e+02  -0.078   0.9381    \n홀드        -1.817e+01  3.161e+02  -0.057   0.9542    \n블론         4.535e+02  7.610e+02   0.596   0.5522    \n경기        -1.760e+02  1.456e+02  -1.209   0.2289    \n선발        -6.719e+02  4.616e+02  -1.456   0.1479    \n이닝         7.425e+01  1.156e+02   0.642   0.5217    \n삼진.9      -4.603e+02  2.349e+03  -0.196   0.8449    \n볼넷.9       1.194e+03  2.256e+03   0.529   0.5976    \n홈런.9       4.874e+03  1.413e+04   0.345   0.7306    \nBABIP       -9.997e+03  1.486e+04  -0.673   0.5022    \nLOB.        -4.350e+01  1.299e+02  -0.335   0.7382    \nERA         -7.413e+01  5.693e+02  -0.130   0.8966    \nRA9.WAR     -7.584e+02  1.487e+03  -0.510   0.6109    \nFIP         -6.436e+03  4.477e+04  -0.144   0.8859    \nkFIP         3.805e+03  3.593e+04   0.106   0.9158    \nWAR          8.559e+03  1.789e+03   4.783 4.55e-06 ***\n연봉.2017.   8.755e-01  4.444e-02  19.698  &lt; 2e-16 ***\n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\nResidual standard error: 9198 on 132 degrees of freedom\nMultiple R-squared:  0.9228,    Adjusted R-squared:  0.9116 \nF-statistic: 82.99 on 19 and 132 DF,  p-value: &lt; 2.2e-16\n\n\n\nvif(model1)\n\n승7.69934669638176패5.30032799820878세3.03718565968123홀드3.63605161357374블론2.75956034802692경기14.2011271199637선발36.1601878733279이닝60.3244538179009삼진.978.7161704892493볼넷.950.7257985016107홈런.9368.399308168573BABIP3.11936893312554LOB.4.04602533224649ERA10.0441738232824RA9.WAR13.4198973515016FIP12525.2424059086kFIP9046.04880487446WAR9.99177856816849연봉.2017.2.21186942564398\n\n\n\nthreshold &lt;- 10\n\n\nhigh_vif_vars &lt;- get_high_vif_variables(model1, threshold)\nprint(high_vif_vars)\n\n [1] \"경기\"    \"선발\"    \"이닝\"    \"삼진.9\"  \"볼넷.9\"  \"홈런.9\"  \"ERA\"    \n [8] \"RA9.WAR\" \"FIP\"     \"kFIP\"   \n\n\n\nthreshold &lt;- 15\n\n\nhigh_vif_vars &lt;- get_high_vif_variables(model1, threshold)\nprint(high_vif_vars)\n\n[1] \"선발\"   \"이닝\"   \"삼진.9\" \"볼넷.9\" \"홈런.9\" \"FIP\"    \"kFIP\"  \n\n\n\npairs(dt,panel=panel.smooth)\n\n\n\n\n\n수치형 데이터들끼리의 상관계수 확인..\n\n\ndt_numeric &lt;- dt[, sapply(dt, is.numeric)]\ncor_matrix &lt;- cor(dt_numeric)\nprint(round(cor_matrix,2))\n\n              승    패    세  홀드  블론  경기  선발  이닝 삼진.9 볼넷.9 홈런.9\n승          1.00  0.71  0.05  0.09  0.11  0.40  0.77  0.91   0.08  -0.40  -0.12\n패          0.71  1.00  0.07  0.10  0.12  0.34  0.77  0.83   0.03  -0.39  -0.06\n세          0.05  0.07  1.00  0.11  0.61  0.43 -0.18  0.02   0.17  -0.13  -0.07\n홀드        0.09  0.10  0.11  1.00  0.49  0.72 -0.29  0.02   0.19  -0.15  -0.08\n블론        0.11  0.12  0.61  0.49  1.00  0.63 -0.26  0.01   0.19  -0.14  -0.06\n경기        0.40  0.34  0.43  0.72  0.63  1.00 -0.04  0.38   0.19  -0.36  -0.11\n선발        0.77  0.77 -0.18 -0.29 -0.26 -0.04  1.00  0.89  -0.06  -0.31  -0.06\n이닝        0.91  0.83  0.02  0.02  0.01  0.38  0.89  1.00   0.04  -0.45  -0.11\n삼진.9      0.08  0.03  0.17  0.19  0.19  0.19 -0.06  0.04   1.00   0.11   0.22\n볼넷.9     -0.40 -0.39 -0.13 -0.15 -0.14 -0.36 -0.31 -0.45   0.11   1.00   0.30\n홈런.9     -0.12 -0.06 -0.07 -0.08 -0.06 -0.11 -0.06 -0.11   0.22   0.30   1.00\nBABIP      -0.17 -0.13 -0.09 -0.10 -0.11 -0.24 -0.10 -0.19   0.46   0.28   0.36\nLOB.        0.13 -0.02  0.17  0.05  0.10  0.11  0.04  0.10  -0.07  -0.15  -0.27\nERA        -0.27 -0.19 -0.15 -0.16 -0.16 -0.32 -0.16 -0.29   0.26   0.52   0.63\nRA9.WAR     0.85  0.60  0.17  0.00  0.01  0.28  0.74  0.85   0.10  -0.40  -0.19\nFIP        -0.30 -0.23 -0.20 -0.21 -0.21 -0.35 -0.15 -0.30  -0.15   0.63   0.83\nkFIP       -0.31 -0.24 -0.23 -0.24 -0.24 -0.37 -0.14 -0.30  -0.32   0.61   0.74\nWAR         0.82  0.63  0.08 -0.04 -0.06  0.20  0.76  0.83   0.15  -0.39  -0.21\n연봉.2018.  0.71  0.47  0.21 -0.02  0.10  0.21  0.56  0.66   0.10  -0.33  -0.12\n연봉.2017.  0.63  0.43  0.26  0.00  0.15  0.23  0.49  0.59   0.10  -0.33  -0.10\n           BABIP  LOB.   ERA RA9.WAR   FIP  kFIP   WAR 연봉.2018. 연봉.2017.\n승         -0.17  0.13 -0.27    0.85 -0.30 -0.31  0.82       0.71       0.63\n패         -0.13 -0.02 -0.19    0.60 -0.23 -0.24  0.63       0.47       0.43\n세         -0.09  0.17 -0.15    0.17 -0.20 -0.23  0.08       0.21       0.26\n홀드       -0.10  0.05 -0.16    0.00 -0.21 -0.24 -0.04      -0.02       0.00\n블론       -0.11  0.10 -0.16    0.01 -0.21 -0.24 -0.06       0.10       0.15\n경기       -0.24  0.11 -0.32    0.28 -0.35 -0.37  0.20       0.21       0.23\n선발       -0.10  0.04 -0.16    0.74 -0.15 -0.14  0.76       0.56       0.49\n이닝       -0.19  0.10 -0.29    0.85 -0.30 -0.30  0.83       0.66       0.59\n삼진.9      0.46 -0.07  0.26    0.10 -0.15 -0.32  0.15       0.10       0.10\n볼넷.9      0.28 -0.15  0.52   -0.40  0.63  0.61 -0.39      -0.33      -0.33\n홈런.9      0.36 -0.27  0.63   -0.19  0.83  0.74 -0.21      -0.12      -0.10\nBABIP       1.00 -0.51  0.73   -0.19  0.25  0.17 -0.08      -0.10      -0.09\nLOB.       -0.51  1.00 -0.72    0.29 -0.29 -0.27  0.14       0.13       0.11\nERA         0.73 -0.72  1.00   -0.34  0.65  0.58 -0.26      -0.22      -0.20\nRA9.WAR    -0.19  0.29 -0.34    1.00 -0.37 -0.38  0.92       0.74       0.64\nFIP         0.25 -0.29  0.65   -0.37  1.00  0.98 -0.39      -0.28      -0.27\nkFIP        0.17 -0.27  0.58   -0.38  0.98  1.00 -0.41      -0.30      -0.28\nWAR        -0.08  0.14 -0.26    0.92 -0.39 -0.41  1.00       0.79       0.68\n연봉.2018. -0.10  0.13 -0.22    0.74 -0.28 -0.30  0.79       1.00       0.93\n연봉.2017. -0.09  0.11 -0.20    0.64 -0.27 -0.28  0.68       0.93       1.00"
  },
  {
    "objectID": "posts/AS4_5.html#model2vif-10-이상인-변수-제거",
    "href": "posts/AS4_5.html#model2vif-10-이상인-변수-제거",
    "title": "AS HW4_5",
    "section": "model2(Vif 10 이상인 변수 제거)",
    "text": "model2(Vif 10 이상인 변수 제거)\n\nmodel2 &lt;- lm(연봉.2018. ~ .-경기-선발-이닝-삼진.9-볼넷.9-홈런.9-ERA-RA9.WAR-FIP-kFIP, dt)\nsummary(model2)\n\n\nCall:\nlm(formula = 연봉.2018. ~ . - 경기 - 선발 - 이닝 - 삼진.9 - \n    볼넷.9 - 홈런.9 - ERA - RA9.WAR - FIP - kFIP, data = dt)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-48657  -1981    511   2303  51073 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  6.432e+03  7.893e+03   0.815   0.4165    \n승           4.770e+02  4.061e+02   1.175   0.2421    \n패          -7.851e+02  3.525e+02  -2.227   0.0275 *  \n세          -1.172e+02  2.150e+02  -0.545   0.5865    \n홀드        -1.229e+02  1.973e+02  -0.623   0.5344    \n블론         6.340e+02  7.188e+02   0.882   0.3792    \nBABIP       -7.810e+03  9.994e+03  -0.781   0.4358    \nLOB.        -4.979e+01  7.793e+01  -0.639   0.5239    \nWAR          7.298e+03  1.169e+03   6.243 4.67e-09 ***\n연봉.2017.   8.846e-01  4.322e-02  20.469  &lt; 2e-16 ***\n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\nResidual standard error: 9149 on 142 degrees of freedom\nMultiple R-squared:  0.9178,    Adjusted R-squared:  0.9126 \nF-statistic: 176.1 on 9 and 142 DF,  p-value: &lt; 2.2e-16\n\n\n\nvif(model2)\n\n승4.44133840452701패2.19787784118271세1.9291576101908홀드1.43155944990414블론2.48814143828698BABIP1.42670331782564LOB.1.47225296358887WAR4.30937396392511연봉.2017.2.11357639082776\n\n\n\nmodel1에서 다중공산성이 높았던 변수들을 제외하고 lm을 돌렸더니, 회귀모형은 유의하게 나왔고 R^2값도 91%로 높게 나왔지만 model1보다는 R^2값이 조금 적게 나왔다.\n다중공산성이 높은 변수를 제외하는 것은 다른 것들도 확인을 해보아야 한다."
  },
  {
    "objectID": "posts/AS4_5.html#정규화",
    "href": "posts/AS4_5.html#정규화",
    "title": "AS HW4_5",
    "section": "정규화",
    "text": "정규화\n\ndt2 &lt;- dt[,1:19]\n\n\nhead(dt)\n\n\nA data.frame: 6 × 20\n\n\n\n승\n패\n세\n홀드\n블론\n경기\n선발\n이닝\n삼진.9\n볼넷.9\n홈런.9\nBABIP\nLOB.\nERA\nRA9.WAR\nFIP\nkFIP\nWAR\n연봉.2018.\n연봉.2017.\n\n\n\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;int&gt;\n&lt;int&gt;\n\n\n\n\n1\n16\n7\n0\n0\n0\n30\n30\n190.0\n8.95\n2.13\n0.76\n0.342\n73.7\n3.60\n6.91\n3.69\n3.44\n6.62\n140000\n85000\n\n\n2\n11\n11\n1\n0\n0\n30\n29\n185.1\n7.43\n1.85\n0.53\n0.319\n67.1\n3.88\n6.80\n3.52\n3.41\n6.08\n120000\n50000\n\n\n3\n20\n6\n0\n0\n0\n31\n31\n193.1\n7.36\n2.09\n0.79\n0.332\n72.1\n3.44\n6.54\n3.94\n3.82\n5.64\n230000\n150000\n\n\n4\n10\n7\n0\n0\n0\n28\n28\n175.2\n8.04\n1.95\n1.02\n0.298\n75.0\n3.43\n6.11\n4.20\n4.03\n4.63\n100000\n100000\n\n\n5\n13\n7\n0\n0\n0\n30\n30\n187.1\n7.49\n2.11\n0.91\n0.323\n74.1\n3.80\n6.13\n4.36\n4.31\n4.38\n111000\n85000\n\n\n6\n8\n10\n0\n0\n0\n26\n26\n160.0\n7.42\n1.74\n1.12\n0.289\n76.1\n3.04\n6.52\n4.42\n4.32\n3.94\n85000\n35000\n\n\n\n\n\n\ndt2\n\n\nA data.frame: 152 × 19\n\n\n\n승\n패\n세\n홀드\n블론\n경기\n선발\n이닝\n삼진.9\n볼넷.9\n홈런.9\nBABIP\nLOB.\nERA\nRA9.WAR\nFIP\nkFIP\nWAR\n연봉.2018.\n\n\n\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;int&gt;\n\n\n\n\n1\n16\n7\n0\n0\n0\n30\n30\n190.0\n8.95\n2.13\n0.76\n0.342\n73.7\n3.60\n6.91\n3.69\n3.44\n6.62\n140000\n\n\n2\n11\n11\n1\n0\n0\n30\n29\n185.1\n7.43\n1.85\n0.53\n0.319\n67.1\n3.88\n6.80\n3.52\n3.41\n6.08\n120000\n\n\n3\n20\n6\n0\n0\n0\n31\n31\n193.1\n7.36\n2.09\n0.79\n0.332\n72.1\n3.44\n6.54\n3.94\n3.82\n5.64\n230000\n\n\n4\n10\n7\n0\n0\n0\n28\n28\n175.2\n8.04\n1.95\n1.02\n0.298\n75.0\n3.43\n6.11\n4.20\n4.03\n4.63\n100000\n\n\n5\n13\n7\n0\n0\n0\n30\n30\n187.1\n7.49\n2.11\n0.91\n0.323\n74.1\n3.80\n6.13\n4.36\n4.31\n4.38\n111000\n\n\n6\n8\n10\n0\n0\n0\n26\n26\n160.0\n7.42\n1.74\n1.12\n0.289\n76.1\n3.04\n6.52\n4.42\n4.32\n3.94\n85000\n\n\n7\n8\n12\n0\n1\n0\n25\n24\n141.2\n7.94\n1.02\n0.83\n0.362\n64.6\n5.08\n2.97\n3.88\n3.78\n3.87\n11500\n\n\n8\n14\n9\n0\n0\n0\n29\n29\n180.1\n6.24\n2.55\n0.60\n0.293\n75.8\n3.14\n7.28\n4.26\n4.35\n3.85\n100000\n\n\n9\n9\n8\n0\n2\n0\n35\n24\n137.1\n9.11\n4.19\n0.52\n0.321\n73.1\n3.67\n4.99\n3.91\n3.67\n3.78\n16000\n\n\n10\n9\n7\n0\n0\n0\n30\n29\n176.0\n7.31\n1.99\n1.12\n0.353\n76.5\n4.14\n5.66\n4.65\n4.61\n3.64\n70000\n\n\n11\n12\n9\n0\n0\n0\n28\n28\n174.1\n6.71\n1.91\n1.14\n0.305\n72.7\n4.28\n5.36\n4.78\n4.80\n3.03\n80000\n\n\n12\n11\n6\n0\n1\n0\n30\n29\n188.2\n5.06\n1.96\n0.95\n0.329\n69.7\n4.53\n4.79\n4.78\n4.97\n2.89\n50000\n\n\n13\n8\n6\n0\n0\n0\n23\n19\n118.1\n5.55\n1.37\n0.68\n0.331\n72.3\n3.65\n4.25\n4.07\n4.19\n2.79\n13000\n\n\n14\n12\n6\n0\n0\n0\n28\n28\n171.1\n6.15\n2.94\n1.10\n0.290\n78.3\n3.68\n5.92\n5.07\n5.14\n2.54\n25000\n\n\n15\n8\n4\n0\n3\n0\n35\n14\n100.2\n8.40\n3.22\n0.98\n0.336\n73.2\n4.38\n3.01\n4.51\n4.34\n2.25\n15500\n\n\n16\n11\n5\n0\n1\n1\n30\n22\n130.1\n7.80\n2.83\n1.31\n0.298\n75.8\n4.21\n4.22\n4.91\n4.77\n2.20\n40000\n\n\n17\n8\n6\n0\n0\n0\n25\n25\n131.1\n7.33\n3.77\n0.69\n0.327\n63.5\n5.35\n1.53\n4.79\n4.84\n2.15\n29000\n\n\n18\n7\n10\n0\n0\n0\n27\n25\n133.0\n6.77\n1.56\n1.29\n0.334\n60.6\n5.21\n1.48\n4.95\n4.98\n2.14\n70000\n\n\n19\n6\n10\n0\n0\n0\n27\n26\n124.1\n8.18\n3.26\n0.87\n0.335\n71.3\n4.63\n3.15\n4.81\n4.79\n2.04\n11500\n\n\n20\n1\n3\n37\n0\n5\n61\n0\n62.0\n9.00\n1.60\n1.02\n0.311\n89.9\n2.18\n3.91\n3.69\n3.37\n1.82\n70000\n\n\n21\n6\n4\n26\n0\n5\n56\n0\n59.0\n11.90\n2.90\n0.76\n0.304\n79.4\n2.75\n2.85\n3.26\n2.69\n1.81\n120000\n\n\n22\n6\n7\n0\n0\n0\n23\n22\n120.0\n6.15\n2.48\n1.20\n0.340\n63.1\n6.00\n1.52\n5.13\n5.22\n1.80\n13000\n\n\n23\n3\n6\n0\n22\n0\n68\n0\n80.0\n7.76\n2.14\n0.45\n0.336\n63.9\n4.39\n2.02\n3.60\n3.52\n1.71\n18500\n\n\n24\n7\n8\n0\n0\n0\n25\n25\n128.0\n6.05\n2.04\n1.20\n0.328\n66.4\n5.06\n2.47\n5.10\n5.21\n1.68\n50000\n\n\n25\n12\n7\n0\n1\n0\n29\n28\n151.1\n6.36\n3.63\n0.95\n0.287\n75.3\n4.10\n4.31\n5.38\n5.55\n1.62\n20000\n\n\n26\n4\n3\n6\n4\n1\n70\n0\n66.0\n7.77\n2.18\n0.55\n0.337\n68.0\n3.95\n2.43\n3.57\n3.45\n1.54\n10000\n\n\n27\n10\n6\n0\n15\n2\n69\n0\n89.2\n9.84\n2.71\n1.30\n0.268\n76.1\n3.61\n3.20\n4.30\n3.92\n1.47\n23000\n\n\n28\n5\n1\n3\n6\n1\n60\n3\n88.2\n8.73\n3.05\n1.02\n0.312\n71.4\n4.06\n1.97\n4.41\n4.22\n1.38\n18800\n\n\n29\n5\n7\n0\n0\n0\n28\n23\n119.0\n8.85\n3.10\n1.66\n0.350\n65.4\n5.67\n1.04\n5.53\n5.35\n1.31\n19000\n\n\n30\n7\n2\n7\n12\n1\n70\n0\n89.0\n7.38\n3.24\n0.61\n0.316\n73.5\n3.44\n2.68\n4.15\n4.09\n1.27\n15000\n\n\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n\n\n123\n0\n2\n0\n1\n1\n14\n0\n21.0\n3.86\n4.71\n0.00\n0.315\n69.2\n3.43\n0.12\n5.16\n5.73\n-0.12\n4000\n\n\n124\n0\n0\n0\n0\n0\n4\n2\n7.1\n6.14\n7.36\n1.23\n0.613\n50.7\n17.18\n-0.52\n7.83\n8.32\n-0.12\n3600\n\n\n125\n1\n0\n0\n0\n0\n24\n2\n30.0\n5.70\n4.20\n1.80\n0.389\n65.3\n5.70\n-0.51\n6.47\n6.57\n-0.16\n5200\n\n\n126\n1\n3\n0\n11\n1\n37\n0\n31.1\n4.88\n2.59\n1.72\n0.311\n67.3\n6.32\n0.04\n6.48\n6.78\n-0.16\n45000\n\n\n127\n0\n0\n0\n0\n0\n21\n1\n32.0\n5.91\n6.47\n1.12\n0.378\n59.6\n8.72\n-0.10\n6.39\n6.59\n-0.17\n3000\n\n\n128\n1\n1\n0\n0\n0\n11\n2\n17.0\n3.71\n5.82\n1.06\n0.300\n53.0\n7.94\n-0.52\n6.73\n7.22\n-0.18\n9000\n\n\n129\n3\n6\n0\n0\n0\n16\n12\n59.0\n4.42\n3.97\n1.83\n0.330\n64.3\n7.17\n0.05\n6.77\n7.04\n-0.18\n7300\n\n\n130\n2\n6\n0\n0\n1\n14\n8\n36.1\n5.20\n5.70\n1.24\n0.376\n65.3\n7.93\n-0.35\n6.68\n7.04\n-0.19\n6000\n\n\n131\n3\n3\n0\n0\n0\n20\n3\n35.2\n6.56\n4.04\n2.02\n0.333\n78.7\n5.55\n0.59\n6.79\n6.86\n-0.19\n4200\n\n\n132\n0\n0\n0\n0\n0\n7\n0\n11.1\n6.35\n5.56\n3.18\n0.361\n68.6\n9.53\n-0.26\n9.29\n9.48\n-0.20\n2900\n\n\n133\n0\n1\n0\n0\n0\n20\n1\n36.0\n4.00\n5.25\n0.25\n0.381\n67.0\n6.75\n-0.07\n5.46\n5.96\n-0.20\n6000\n\n\n134\n7\n4\n11\n6\n6\n65\n1\n80.1\n7.39\n5.27\n1.12\n0.267\n75.0\n4.59\n1.89\n5.54\n5.53\n-0.20\n15000\n\n\n135\n1\n4\n0\n0\n0\n9\n7\n32.0\n7.59\n3.94\n2.53\n0.359\n52.5\n9.84\n-0.43\n7.39\n7.36\n-0.20\n5700\n\n\n136\n5\n6\n0\n15\n6\n63\n0\n73.1\n7.85\n4.05\n1.60\n0.303\n61.0\n6.63\n-0.74\n5.68\n5.59\n-0.22\n24000\n\n\n137\n0\n1\n0\n0\n0\n3\n1\n8.0\n4.50\n6.75\n4.50\n0.346\n82.1\n9.00\n-0.16\n11.48\n11.73\n-0.23\n3000\n\n\n138\n3\n1\n0\n2\n1\n24\n0\n26.2\n3.71\n2.70\n1.69\n0.307\n71.4\n5.40\n0.19\n6.47\n6.83\n-0.24\n10000\n\n\n139\n1\n2\n1\n5\n2\n35\n0\n38.0\n7.34\n3.55\n2.13\n0.273\n71.1\n5.68\n-0.05\n6.76\n6.77\n-0.24\n6500\n\n\n140\n2\n0\n0\n0\n0\n16\n0\n19.0\n4.74\n4.74\n1.42\n0.324\n67.0\n6.63\n-0.09\n7.10\n7.57\n-0.25\n3200\n\n\n141\n0\n2\n0\n1\n0\n37\n1\n39.0\n5.08\n3.92\n1.62\n0.294\n68.3\n6.23\n0.25\n6.40\n6.63\n-0.28\n6800\n\n\n142\n0\n0\n0\n0\n0\n9\n0\n8.0\n2.25\n6.75\n3.38\n0.281\n47.3\n10.12\n-0.61\n10.73\n11.37\n-0.30\n3000\n\n\n143\n4\n3\n3\n8\n4\n53\n0\n53.0\n6.62\n2.38\n2.21\n0.280\n73.9\n5.09\n0.28\n6.30\n6.27\n-0.38\n9000\n\n\n144\n5\n3\n0\n0\n0\n39\n13\n89.2\n5.12\n3.41\n1.51\n0.320\n57.4\n7.33\n-0.27\n6.31\n6.58\n-0.41\n12500\n\n\n145\n0\n4\n0\n0\n0\n15\n5\n31.0\n4.65\n6.97\n2.03\n0.340\n66.4\n8.71\n-0.42\n8.15\n8.52\n-0.42\n3600\n\n\n146\n2\n0\n0\n0\n0\n30\n0\n31.2\n7.11\n6.25\n2.56\n0.276\n81.9\n5.12\n0.31\n8.03\n8.03\n-0.44\n7000\n\n\n147\n5\n6\n1\n3\n2\n39\n12\n81.2\n5.18\n3.42\n1.65\n0.314\n63.2\n6.61\n-0.02\n6.33\n6.54\n-0.46\n7600\n\n\n148\n2\n5\n0\n0\n2\n33\n5\n62.2\n4.31\n3.30\n1.58\n0.355\n56.9\n7.76\n-1.21\n6.21\n6.48\n-0.47\n7100\n\n\n149\n3\n2\n0\n0\n0\n25\n11\n59.1\n4.85\n5.61\n1.06\n0.263\n65.4\n5.92\n0.39\n6.41\n6.77\n-0.49\n7500\n\n\n150\n0\n2\n0\n0\n0\n9\n2\n14.2\n4.91\n4.91\n2.45\n0.382\n52.8\n11.66\n-0.83\n8.03\n8.29\n-0.61\n10000\n\n\n151\n0\n3\n0\n1\n0\n41\n0\n43.2\n7.63\n7.01\n1.44\n0.341\n73.9\n5.77\n-0.40\n6.87\n6.95\n-0.70\n4000\n\n\n152\n4\n4\n0\n0\n0\n24\n14\n81.0\n5.78\n5.89\n2.00\n0.312\n65.3\n7.67\n-0.68\n7.60\n7.81\n-1.01\n4000"
  },
  {
    "objectID": "posts/AS4_5.html#변수선택",
    "href": "posts/AS4_5.html#변수선택",
    "title": "AS HW4_5",
    "section": "변수선택",
    "text": "변수선택\n\nmodel3(AIC)\n- AIC(Step)\n\nm0 = lm(연봉.2018. ~ 1, data = dt)\n\n\nmodel3 = step(\n m0,\n scope = 연봉.2018. ~연봉.2017.+승+패+세+홀드+블론+경기+선발+이닝+삼진.9+볼넷.9+홈런.9+BABIP+LOB.+ERA+RA9.WAR+FIP+kFIP+WAR,\n direction = \"both\")\n\nStart:  AIC=3144.3\n연봉.2018. ~ 1\n\n             Df  Sum of Sq        RSS    AIC\n+ 연봉.2017.  1 1.2511e+11 1.9445e+10 2841.4\n+ WAR         1 9.0535e+10 5.4022e+10 2996.7\n+ RA9.WAR     1 7.9230e+10 6.5326e+10 3025.6\n+ 승          1 7.3377e+10 7.1179e+10 3038.6\n+ 이닝        1 6.2759e+10 8.1797e+10 3059.8\n+ 선발        1 4.5409e+10 9.9147e+10 3089.0\n+ 패          1 3.1910e+10 1.1265e+11 3108.4\n+ 볼넷.9      1 1.5661e+10 1.2890e+11 3128.9\n+ kFIP        1 1.2591e+10 1.3197e+11 3132.4\n+ FIP         1 1.1403e+10 1.3315e+11 3133.8\n+ ERA         1 6.7332e+09 1.3782e+11 3139.1\n+ 세          1 6.4461e+09 1.3811e+11 3139.4\n+ 경기        1 6.3714e+09 1.3819e+11 3139.4\n+ LOB.        1 2.2831e+09 1.4227e+11 3143.9\n+ 홈런.9      1 1.9575e+09 1.4260e+11 3144.2\n&lt;none&gt;                     1.4456e+11 3144.3\n+ 삼진.9      1 1.5567e+09 1.4300e+11 3144.7\n+ BABIP       1 1.5139e+09 1.4304e+11 3144.7\n+ 블론        1 1.3815e+09 1.4318e+11 3144.8\n+ 홀드        1 4.3499e+07 1.4451e+11 3146.3\n\nStep:  AIC=2841.38\n연봉.2018. ~ 연봉.2017.\n\n             Df  Sum of Sq        RSS    AIC\n+ WAR         1 7.0421e+09 1.2403e+10 2775.0\n+ RA9.WAR     1 4.9589e+09 1.4486e+10 2798.6\n+ 승          1 3.8414e+09 1.5604e+10 2809.9\n+ 이닝        1 2.8118e+09 1.6633e+10 2819.6\n+ 선발        1 2.1318e+09 1.7313e+10 2825.7\n+ 패          1 8.8114e+08 1.8564e+10 2836.3\n&lt;none&gt;                     1.9445e+10 2841.4\n+ 블론        1 2.2022e+08 1.9225e+10 2841.7\n+ 세          1 1.7105e+08 1.9274e+10 2842.0\n+ kFIP        1 1.6254e+08 1.9283e+10 2842.1\n+ FIP         1 1.5483e+08 1.9290e+10 2842.2\n+ ERA         1 1.0735e+08 1.9338e+10 2842.5\n+ LOB.        1 7.7049e+07 1.9368e+10 2842.8\n+ 홈런.9      1 7.3957e+07 1.9371e+10 2842.8\n+ 볼넷.9      1 6.4565e+07 1.9381e+10 2842.9\n+ BABIP       1 5.6938e+07 1.9388e+10 2842.9\n+ 홀드        1 3.8024e+07 1.9407e+10 2843.1\n+ 삼진.9      1 5.5081e+06 1.9440e+10 2843.3\n+ 경기        1 1.2651e+04 1.9445e+10 2843.4\n- 연봉.2017.  1 1.2511e+11 1.4456e+11 3144.3\n\nStep:  AIC=2775.03\n연봉.2018. ~ 연봉.2017. + WAR\n\n             Df  Sum of Sq        RSS    AIC\n+ 패          1 2.1336e+08 1.2190e+10 2774.4\n+ kFIP        1 1.8769e+08 1.2215e+10 2774.7\n+ 선발        1 1.7153e+08 1.2232e+10 2774.9\n+ FIP         1 1.6877e+08 1.2234e+10 2774.9\n+ 볼넷.9      1 1.6419e+08 1.2239e+10 2775.0\n&lt;none&gt;                     1.2403e+10 2775.0\n+ 이닝        1 1.4704e+08 1.2256e+10 2775.2\n+ 홈런.9      1 5.1612e+07 1.2351e+10 2776.4\n+ 삼진.9      1 4.8349e+07 1.2355e+10 2776.4\n+ 승          1 3.0076e+07 1.2373e+10 2776.7\n+ 경기        1 2.7246e+07 1.2376e+10 2776.7\n+ BABIP       1 2.4182e+07 1.2379e+10 2776.7\n+ ERA         1 1.7077e+07 1.2386e+10 2776.8\n+ 블론        1 1.1153e+07 1.2392e+10 2776.9\n+ RA9.WAR     1 6.6509e+06 1.2396e+10 2776.9\n+ 세          1 4.3325e+06 1.2399e+10 2777.0\n+ 홀드        1 3.4824e+06 1.2400e+10 2777.0\n+ LOB.        1 6.6018e+05 1.2402e+10 2777.0\n- WAR         1 7.0421e+09 1.9445e+10 2841.4\n- 연봉.2017.  1 4.1619e+10 5.4022e+10 2996.7\n\nStep:  AIC=2774.4\n연봉.2018. ~ 연봉.2017. + WAR + 패\n\n             Df  Sum of Sq        RSS    AIC\n+ kFIP        1 1.9738e+08 1.1992e+10 2773.9\n+ 승          1 1.8072e+08 1.2009e+10 2774.1\n+ FIP         1 1.7496e+08 1.2015e+10 2774.2\n&lt;none&gt;                     1.2190e+10 2774.4\n- 패          1 2.1336e+08 1.2403e+10 2775.0\n+ 볼넷.9      1 1.0330e+08 1.2086e+10 2775.1\n+ 홈런.9      1 7.1015e+07 1.2119e+10 2775.5\n+ 삼진.9      1 6.6895e+07 1.2123e+10 2775.6\n+ 블론        1 4.2173e+07 1.2148e+10 2775.9\n+ BABIP       1 4.1954e+07 1.2148e+10 2775.9\n+ 선발        1 3.1474e+07 1.2158e+10 2776.0\n+ ERA         1 1.3441e+07 1.2176e+10 2776.2\n+ 이닝        1 5.8966e+06 1.2184e+10 2776.3\n+ 세          1 3.4705e+06 1.2186e+10 2776.3\n+ RA9.WAR     1 2.4143e+06 1.2187e+10 2776.4\n+ LOB.        1 1.7129e+06 1.2188e+10 2776.4\n+ 경기        1 1.1252e+06 1.2189e+10 2776.4\n+ 홀드        1 1.8992e+05 1.2190e+10 2776.4\n- WAR         1 6.3743e+09 1.8564e+10 2836.3\n- 연봉.2017.  1 4.1680e+10 5.3870e+10 2998.3\n\nStep:  AIC=2773.92\n연봉.2018. ~ 연봉.2017. + WAR + 패 + kFIP\n\n             Df  Sum of Sq        RSS    AIC\n+ 승          1 1.6741e+08 1.1825e+10 2773.8\n&lt;none&gt;                     1.1992e+10 2773.9\n+ 블론        1 1.2836e+08 1.1864e+10 2774.3\n- kFIP        1 1.9738e+08 1.2190e+10 2774.4\n+ 선발        1 1.1764e+08 1.1875e+10 2774.4\n- 패          1 2.2305e+08 1.2215e+10 2774.7\n+ BABIP       1 7.5190e+07 1.1917e+10 2775.0\n+ ERA         1 2.1818e+07 1.1971e+10 2775.6\n+ 홀드        1 2.1404e+07 1.1971e+10 2775.6\n+ 삼진.9      1 1.9275e+07 1.1973e+10 2775.7\n+ 경기        1 1.7028e+07 1.1975e+10 2775.7\n+ 이닝        1 1.3041e+07 1.1979e+10 2775.8\n+ FIP         1 9.3610e+06 1.1983e+10 2775.8\n+ 볼넷.9      1 8.8432e+06 1.1983e+10 2775.8\n+ 홈런.9      1 8.7223e+06 1.1984e+10 2775.8\n+ LOB.        1 4.0316e+06 1.1988e+10 2775.9\n+ RA9.WAR     1 2.0131e+06 1.1990e+10 2775.9\n+ 세          1 1.4454e+06 1.1991e+10 2775.9\n- WAR         1 6.4941e+09 1.8486e+10 2837.7\n- 연봉.2017.  1 4.1735e+10 5.3727e+10 2999.9\n\nStep:  AIC=2773.78\n연봉.2018. ~ 연봉.2017. + WAR + 패 + kFIP + 승\n\n             Df  Sum of Sq        RSS    AIC\n+ 이닝        1 2.1565e+08 1.1609e+10 2773.0\n+ 선발        1 1.9668e+08 1.1628e+10 2773.2\n&lt;none&gt;                     1.1825e+10 2773.8\n- 승          1 1.6741e+08 1.1992e+10 2773.9\n- kFIP        1 1.8408e+08 1.2009e+10 2774.1\n+ 블론        1 8.3012e+07 1.1742e+10 2774.7\n+ RA9.WAR     1 6.3182e+07 1.1762e+10 2775.0\n+ BABIP       1 4.5875e+07 1.1779e+10 2775.2\n+ 볼넷.9      1 1.7921e+07 1.1807e+10 2775.6\n+ 삼진.9      1 1.4564e+07 1.1810e+10 2775.6\n+ 홈런.9      1 1.2160e+07 1.1813e+10 2775.6\n+ ERA         1 8.8026e+06 1.1816e+10 2775.7\n+ FIP         1 8.1221e+06 1.1817e+10 2775.7\n+ 세          1 5.8214e+06 1.1819e+10 2775.7\n+ 홀드        1 5.2671e+06 1.1820e+10 2775.7\n+ LOB.        1 3.9758e+05 1.1825e+10 2775.8\n+ 경기        1 3.3176e+05 1.1825e+10 2775.8\n- 패          1 3.6648e+08 1.2191e+10 2776.4\n- WAR         1 3.6353e+09 1.5460e+10 2812.5\n- 연봉.2017.  1 3.9188e+10 5.1013e+10 2994.0\n\nStep:  AIC=2772.98\n연봉.2018. ~ 연봉.2017. + WAR + 패 + kFIP + 승 + 이닝\n\n             Df  Sum of Sq        RSS    AIC\n- 패          1 3.1923e+07 1.1641e+10 2771.4\n&lt;none&gt;                     1.1609e+10 2773.0\n- kFIP        1 2.1496e+08 1.1824e+10 2773.8\n- 이닝        1 2.1565e+08 1.1825e+10 2773.8\n+ BABIP       1 8.7592e+07 1.1522e+10 2773.8\n+ 선발        1 5.0414e+07 1.1559e+10 2774.3\n+ 블론        1 3.9472e+07 1.1570e+10 2774.5\n+ 삼진.9      1 3.3863e+07 1.1575e+10 2774.5\n+ ERA         1 3.3525e+07 1.1576e+10 2774.5\n+ FIP         1 1.8310e+07 1.1591e+10 2774.7\n+ 홈런.9      1 1.2031e+07 1.1597e+10 2774.8\n+ RA9.WAR     1 1.0398e+07 1.1599e+10 2774.8\n+ LOB.        1 3.1362e+06 1.1606e+10 2774.9\n+ 경기        1 1.9500e+06 1.1607e+10 2775.0\n+ 볼넷.9      1 1.2880e+06 1.1608e+10 2775.0\n+ 세          1 2.2726e+05 1.1609e+10 2775.0\n+ 홀드        1 9.3003e+04 1.1609e+10 2775.0\n- 승          1 3.7002e+08 1.1979e+10 2775.8\n- WAR         1 3.7546e+09 1.5364e+10 2813.6\n- 연봉.2017.  1 3.8723e+10 5.0333e+10 2993.9\n\nStep:  AIC=2771.4\n연봉.2018. ~ 연봉.2017. + WAR + kFIP + 승 + 이닝\n\n             Df  Sum of Sq        RSS    AIC\n&lt;none&gt;                     1.1641e+10 2771.4\n+ BABIP       1 9.5915e+07 1.1545e+10 2772.1\n- kFIP        1 2.2291e+08 1.1864e+10 2772.3\n+ 선발        1 6.0820e+07 1.1580e+10 2772.6\n+ ERA         1 4.1760e+07 1.1599e+10 2772.8\n+ 삼진.9      1 3.6788e+07 1.1604e+10 2772.9\n+ 패          1 3.1923e+07 1.1609e+10 2773.0\n+ 블론        1 2.3680e+07 1.1618e+10 2773.1\n+ FIP         1 2.0239e+07 1.1621e+10 2773.1\n+ 홈런.9      1 1.4166e+07 1.1627e+10 2773.2\n+ LOB.        1 7.7349e+06 1.1633e+10 2773.3\n+ 경기        1 1.5351e+06 1.1640e+10 2773.4\n+ RA9.WAR     1 1.4350e+06 1.1640e+10 2773.4\n+ 볼넷.9      1 1.4095e+06 1.1640e+10 2773.4\n+ 홀드        1 2.5525e+05 1.1641e+10 2773.4\n+ 세          1 6.8181e+04 1.1641e+10 2773.4\n- 승          1 4.0011e+08 1.2041e+10 2774.5\n- 이닝        1 5.5021e+08 1.2191e+10 2776.4\n- WAR         1 3.9604e+09 1.5602e+10 2813.9\n- 연봉.2017.  1 3.8795e+10 5.0436e+10 2992.2\n\n\n\nsummary\n\nsummary(model3)\n\n\nCall:\nlm(formula = 연봉.2018. ~ 연봉.2017. + WAR + kFIP + 승 + \n    이닝, data = dt)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-48717  -2879    204   3083  48961 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -2.691e+03  2.658e+03  -1.012  0.31310    \n연봉.2017.   8.862e-01  4.018e-02  22.058  &lt; 2e-16 ***\nWAR          8.118e+03  1.152e+03   7.048 6.68e-11 ***\nkFIP         6.737e+02  4.029e+02   1.672  0.09666 .  \n승           1.059e+03  4.727e+02   2.240  0.02659 *  \n이닝        -9.701e+01  3.693e+01  -2.627  0.00954 ** \n---\nSignif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\nResidual standard error: 8929 on 146 degrees of freedom\nMultiple R-squared:  0.9195,    Adjusted R-squared:  0.9167 \nF-statistic: 333.4 on 5 and 146 DF,  p-value: &lt; 2.2e-16\n\n\n\nAIC를 이용하면 최종 모형은 “연봉.2018. ~ 연봉.2017. + WAR + kFIP+승+이닝” 이다.\n\n\nvif(model3)\n\n연봉.2017.1.91752518192932WAR4.3927072113068kFIP1.20722030237251승6.3165168650018이닝6.53436057823665"
  },
  {
    "objectID": "posts/AS4_5.html#pca주성분분석",
    "href": "posts/AS4_5.html#pca주성분분석",
    "title": "AS HW4_5",
    "section": "PCA(주성분분석)",
    "text": "PCA(주성분분석)\n\n서로 상관성이 높은 변수들의 선형 결합으로 만들어 기존의 상관성이 높은 변수들을 요약, 축소하는 기법\n\n\nhead(dt, 3)\n\n\nA data.frame: 3 × 20\n\n\n\n승\n패\n세\n홀드\n블론\n경기\n선발\n이닝\n삼진.9\n볼넷.9\n홈런.9\nBABIP\nLOB.\nERA\nRA9.WAR\nFIP\nkFIP\nWAR\n연봉.2018.\n연봉.2017.\n\n\n\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;int&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;dbl&gt;\n&lt;int&gt;\n&lt;int&gt;\n\n\n\n\n1\n16\n7\n0\n0\n0\n30\n30\n190.0\n8.95\n2.13\n0.76\n0.342\n73.7\n3.60\n6.91\n3.69\n3.44\n6.62\n140000\n85000\n\n\n2\n11\n11\n1\n0\n0\n30\n29\n185.1\n7.43\n1.85\n0.53\n0.319\n67.1\n3.88\n6.80\n3.52\n3.41\n6.08\n120000\n50000\n\n\n3\n20\n6\n0\n0\n0\n31\n31\n193.1\n7.36\n2.09\n0.79\n0.332\n72.1\n3.44\n6.54\n3.94\n3.82\n5.64\n230000\n150000\n\n\n\n\n\n\nlog.dt &lt;- log(dt[,1:18])\n\nWarning message in FUN(X[[i]], ...):\n“NaNs produced”\nWarning message in FUN(X[[i]], ...):\n“NaNs produced”\nWarning message in FUN(X[[i]], ...):\n“NaNs produced”\n\n\n\ndt.pca &lt;- princomp(dt, cor=TRUE)\n\n\nsummary(dt.pca)\n\nImportance of components:\n                         Comp.1    Comp.2    Comp.3     Comp.4     Comp.5\nStandard deviation     2.713772 1.8768322 1.5635037 1.27736597 1.12734803\nProportion of Variance 0.368228 0.1761249 0.1222272 0.08158319 0.06354568\nCumulative Proportion  0.368228 0.5443530 0.6665801 0.74816334 0.81170902\n                           Comp.6     Comp.7     Comp.8     Comp.9   Comp.10\nStandard deviation     0.95298765 0.85055438 0.77308613 0.61480657 0.5714560\nProportion of Variance 0.04540927 0.03617214 0.02988311 0.01889936 0.0163281\nCumulative Proportion  0.85711829 0.89329043 0.92317354 0.94207289 0.9584010\n                          Comp.11     Comp.12     Comp.13     Comp.14\nStandard deviation     0.51349415 0.437212718 0.322182613 0.292730349\nProportion of Variance 0.01318381 0.009557748 0.005190082 0.004284553\nCumulative Proportion  0.97158481 0.981142554 0.986332636 0.990617188\n                           Comp.15     Comp.16     Comp.17      Comp.18\nStandard deviation     0.270420989 0.221137542 0.200900792 0.1231832080\nProportion of Variance 0.003656376 0.002445091 0.002018056 0.0007587051\nCumulative Proportion  0.994273564 0.996718655 0.998736711 0.9994954162\n                            Comp.19      Comp.20\nStandard deviation     0.1002308794 6.741405e-03\nProportion of Variance 0.0005023115 2.272327e-06\nCumulative Proportion  0.9999977277 1.000000e+00\n\n\n\n제 1주성분과 제6주성분까지의 누적 분산비율은 대략 85.71%로 6개의 주성분 변수를 활용해 전체 데이터의 85.71%를 설명할 수 있다.\n\n\nscreeplot(dt.pca, npcs=8, type=\"lines\")\n\n\n\n\n\n주성분들에 의해 설명되는 변동 비율\n\n\nloadings(dt.pca)\n\n\nLoadings:\n           Comp.1 Comp.2 Comp.3 Comp.4 Comp.5 Comp.6 Comp.7 Comp.8 Comp.9\n승          0.317  0.170                                            0.103\n패          0.263  0.164         0.106  0.312        -0.335  0.138 -0.402\n세                -0.190  0.302  0.127 -0.455 -0.296 -0.438         0.326\n홀드              -0.251  0.338  0.140  0.369  0.228  0.453         0.105\n블론              -0.264  0.406  0.204        -0.172 -0.165  0.160 -0.333\n경기        0.174 -0.228  0.368  0.220  0.241                       0.214\n선발        0.259  0.312 -0.151         0.138        -0.231        -0.138\n이닝        0.321  0.198                0.183        -0.136              \n삼진.9                    0.390 -0.404 -0.124  0.508 -0.157 -0.113 -0.268\n볼넷.9     -0.232  0.142               -0.212  0.308         0.771       \n홈런.9     -0.152  0.304  0.268  0.267         0.177        -0.526       \nBABIP      -0.125  0.224  0.286 -0.444                              0.179\nLOB.        0.117 -0.219 -0.220  0.230 -0.335  0.559 -0.118 -0.142       \nERA        -0.210  0.318  0.281 -0.157        -0.139                0.123\nRA9.WAR     0.322  0.147               -0.106  0.163                0.440\nFIP        -0.235  0.306         0.378                                   \nkFIP       -0.234  0.288         0.423                                   \nWAR         0.317  0.186                                            0.287\n연봉.2018.  0.284  0.158               -0.327 -0.130  0.397        -0.152\n연봉.2017.  0.263  0.129  0.106        -0.358 -0.204  0.414        -0.315\n           Comp.10 Comp.11 Comp.12 Comp.13 Comp.14 Comp.15 Comp.16 Comp.17\n승                  0.386   0.237   0.472   0.592   0.142           0.107 \n패                 -0.482  -0.139  -0.117   0.314  -0.239   0.233  -0.110 \n세         -0.158  -0.338           0.231           0.199  -0.118         \n홀드        0.126  -0.336  -0.331   0.286           0.225  -0.105         \n블론        0.384   0.520  -0.251  -0.124  -0.149                         \n경기       -0.197           0.555  -0.333          -0.241                 \n선발                                0.205  -0.420   0.277  -0.322         \n이닝                        0.212          -0.350          -0.218         \n삼진.9     -0.370                                                         \n볼넷.9     -0.102           0.129                                         \n홈런.9                                                                    \nBABIP       0.655  -0.181   0.294  -0.157           0.192                 \nLOB.        0.413  -0.160                          -0.336  -0.169   0.133 \nERA                        -0.140   0.334          -0.688  -0.246   0.174 \nRA9.WAR             0.117  -0.242          -0.263  -0.172   0.490  -0.464 \nFIP                                                 0.105                 \nkFIP                                                                      \nWAR                        -0.397  -0.494   0.169          -0.104   0.551 \n연봉.2018.                         -0.158   0.211          -0.490  -0.515 \n연봉.2017.         -0.170   0.199   0.148  -0.250           0.413   0.359 \n           Comp.18 Comp.19 Comp.20\n승                                \n패                                \n세                                \n홀드                              \n블론                              \n경기                0.320         \n선발                0.556         \n이닝               -0.755         \n삼진.9     -0.377                 \n볼넷.9      0.359                 \n홈런.9      0.614          -0.125 \nBABIP                             \nLOB.                              \nERA                               \nRA9.WAR                           \nFIP        -0.322           0.754 \nkFIP       -0.491          -0.641 \nWAR                               \n연봉.2018.                        \n연봉.2017.                        \n\n               Comp.1 Comp.2 Comp.3 Comp.4 Comp.5 Comp.6 Comp.7 Comp.8 Comp.9\nSS loadings      1.00   1.00   1.00   1.00   1.00   1.00   1.00   1.00   1.00\nProportion Var   0.05   0.05   0.05   0.05   0.05   0.05   0.05   0.05   0.05\nCumulative Var   0.05   0.10   0.15   0.20   0.25   0.30   0.35   0.40   0.45\n               Comp.10 Comp.11 Comp.12 Comp.13 Comp.14 Comp.15 Comp.16 Comp.17\nSS loadings       1.00    1.00    1.00    1.00    1.00    1.00    1.00    1.00\nProportion Var    0.05    0.05    0.05    0.05    0.05    0.05    0.05    0.05\nCumulative Var    0.50    0.55    0.60    0.65    0.70    0.75    0.80    0.85\n               Comp.18 Comp.19 Comp.20\nSS loadings       1.00    1.00    1.00\nProportion Var    0.05    0.05    0.05\nCumulative Var    0.90    0.95    1.00"
  },
  {
    "objectID": "posts/AS4_5.html#glm",
    "href": "posts/AS4_5.html#glm",
    "title": "AS HW4_5",
    "section": "glm",
    "text": "glm\n\n\nX &lt;- model.matrix(연봉.2018.~., dt)[,-1] \ny &lt;- dt$연봉.2018.\n\n\nhead(X)\n\n\nA matrix: 6 × 19 of type dbl\n\n\n\n승\n패\n세\n홀드\n블론\n경기\n선발\n이닝\n삼진.9\n볼넷.9\n홈런.9\nBABIP\nLOB.\nERA\nRA9.WAR\nFIP\nkFIP\nWAR\n연봉.2017.\n\n\n\n\n1\n16\n7\n0\n0\n0\n30\n30\n190.0\n8.95\n2.13\n0.76\n0.342\n73.7\n3.60\n6.91\n3.69\n3.44\n6.62\n85000\n\n\n2\n11\n11\n1\n0\n0\n30\n29\n185.1\n7.43\n1.85\n0.53\n0.319\n67.1\n3.88\n6.80\n3.52\n3.41\n6.08\n50000\n\n\n3\n20\n6\n0\n0\n0\n31\n31\n193.1\n7.36\n2.09\n0.79\n0.332\n72.1\n3.44\n6.54\n3.94\n3.82\n5.64\n150000\n\n\n4\n10\n7\n0\n0\n0\n28\n28\n175.2\n8.04\n1.95\n1.02\n0.298\n75.0\n3.43\n6.11\n4.20\n4.03\n4.63\n100000\n\n\n5\n13\n7\n0\n0\n0\n30\n30\n187.1\n7.49\n2.11\n0.91\n0.323\n74.1\n3.80\n6.13\n4.36\n4.31\n4.38\n85000\n\n\n6\n8\n10\n0\n0\n0\n26\n26\n160.0\n7.42\n1.74\n1.12\n0.289\n76.1\n3.04\n6.52\n4.42\n4.32\n3.94\n35000\n\n\n\n\n\n\nridge.fit&lt;-glmnet(X,y,alpha=0, lambda=seq(0,100,10)) ##ridge : alpha=0 \nplot(ridge.fit, label=TRUE)\nabline(h=0, col=\"grey\", lty=2)\n\n\n\n\n\ncv.fit&lt;-cv.glmnet(X,y,alpha=0,nfolds=length(y))\n\nWarning message:\n“Option grouped=FALSE enforced in cv.glmnet, since &lt; 3 observations per fold”\n\n\n\ncv.fit\n\n\nCall:  cv.glmnet(x = X, y = y, nfolds = length(y), alpha = 0) \n\nMeasure: Mean-Squared Error \n\n    Lambda Index   Measure       SE Nonzero\nmin   2869   100 117171048 49876335      19\n1se  12711    84 166795504 78214517      19\n\n\n\nplot(cv.fit)"
  },
  {
    "objectID": "posts/논문.html",
    "href": "posts/논문.html",
    "title": "GNN",
    "section": "",
    "text": "논문\nXia, F., Sun, K., Yu, S., Aziz, A., Wan, L., Pan, S., & Liu, H. (2021). Graph learning: A survey. IEEE Transactions on Artificial Intelligence, 2(2), 109-127.\nNickel, M., Murphy, K., Tresp, V., & Gabrilovich, E. (2015). A review of relational machine learning for knowledge graphs. Proceedings of the IEEE, 104(1), 11-33.\nYang, Z., Cohen, W., & Salakhudinov, R. (2016, June). Revisiting semi-supervised learning with graph embeddings. In International conference on machine learning (pp. 40-48). PMLR.\nhttps://arxiv.org/ftp/arxiv/papers/1812/1812.08434.pdf\nF.Scarselli, M.Gori, “The graph neural network model,” IEEE Transactions on Neural Networks, 2009\nT. N. Kipf and M. Welling, “Semi-supervised classification with graph convolutional networks,” in Proc. of ICLR, 2017.\nZ. Wu, S. Pan, F. Chen, G. Long, C. Zhang, Philip S. Yu, “A Comprehensive Survey on Graph Neural Networks”, arXiv:1901.00596\nD. Xu, Y. Zhu, C. B. Choy, and L. Fei-Fei, “Scene graph generation by iterative message passing,” in Proc. of CVPR, vol. 2, 2017\nJ. Johnson, A. Gupta, and L. Fei-Fei, “Image generation from scene graphs,” in Proc. of CVPR, 2018\nX. Wang, Y. Ye, and A. Gupta, “Zero-shot recognition via semantic embeddings and knowledge graphs,” in CVPR 2018"
  },
  {
    "objectID": "posts/hadamangeo/VAE.html",
    "href": "posts/hadamangeo/VAE.html",
    "title": "VAE(-ing)",
    "section": "",
    "text": "https://arxiv.org/pdf/1312.6114.pdf"
  },
  {
    "objectID": "posts/hadamangeo/VAE.html#오토인코더",
    "href": "posts/hadamangeo/VAE.html#오토인코더",
    "title": "VAE(-ing)",
    "section": "오토인코더",
    "text": "오토인코더\n\nEncoder, Decoder 네트워크로 구성된 모델\n학습 데이터-&gt; encoder에 입력값"
  },
  {
    "objectID": "posts/hadamangeo/Hands-on Graph Neural Networks with PyTorch & PyTorch Geometric.html",
    "href": "posts/hadamangeo/Hands-on Graph Neural Networks with PyTorch & PyTorch Geometric.html",
    "title": "Hands-on Graph Neural Networks with PyTorch & PyTorch Geometric",
    "section": "",
    "text": "Hands-on Graph Neural Networks with PyTorch & PyTorch Geometric"
  },
  {
    "objectID": "posts/hadamangeo/Hands-on Graph Neural Networks with PyTorch & PyTorch Geometric.html#data",
    "href": "posts/hadamangeo/Hands-on Graph Neural Networks with PyTorch & PyTorch Geometric.html#data",
    "title": "Hands-on Graph Neural Networks with PyTorch & PyTorch Geometric",
    "section": "Data",
    "text": "Data\n\nimport torch\nfrom torch_geometric.data import Data\n\n\nx = torch.tensor([[2,1], [5,6], [3,7], [12,0]], dtype=torch.float)\ny = torch.tensor([0, 1, 0, 1], dtype=torch.float)\n\nedge_index = torch.tensor([[0, 2, 1, 0, 3],\n                           [3, 1, 0, 1, 2]], dtype=torch.long)\n\n\ndata = Data(x=x, y=y, edge_index=edge_index)\n\n\ndata\n\nData(x=[4, 2], edge_index=[2, 5], y=[4])"
  },
  {
    "objectID": "posts/hadamangeo/Hands-on Graph Neural Networks with PyTorch & PyTorch Geometric.html#dataset",
    "href": "posts/hadamangeo/Hands-on Graph Neural Networks with PyTorch & PyTorch Geometric.html#dataset",
    "title": "Hands-on Graph Neural Networks with PyTorch & PyTorch Geometric",
    "section": "Dataset",
    "text": "Dataset\n\nInMemoryDataset\n- raw_file_names()\n- processed_file_names()\n- dowanload()\n\n작업 중인 데이터를 self.raw_dir에 지정된 디렉터리로 다운로드, 데이터 다운이 필요 없으면 pass\n\n- process()\n\nself.collate()를 호출하여 DataLoader 객체에서 사용할 슬라이스를 계산\n\nimport torch\nfrom torch_geometric.data import InMemoryDataset\n\n\nclass MyOwnDataset(InMemoryDataset):\n    def __init__(self, root, transform=None, pre_transform=None):\n        super(MyOwnDataset, self).__init__(root, transform, pre_transform)\n        self.data, self.slices = torch.load(self.processed_paths[0])\n\n    @property\n    def raw_file_names(self):\n        return ['some_file_1', 'some_file_2', ...]\n\n    @property\n    def processed_file_names(self):\n        return ['data.pt']\n\n    def download(self):\n        # Download to `self.raw_dir`.\n\n    def process(self):\n        # Read data into huge `Data` list.\n        data_list = [...]\n\n        if self.pre_filter is not None:\n            data_list [data for data in data_list if self.pre_filter(data)]\n\n        if self.pre_transform is not None:\n            data_list = [self.pre_transform(data) for data in data_list]\n\n        data, slices = self.collate(data_list)\n        torch.save((data, slices), self.processed_paths[0])"
  },
  {
    "objectID": "posts/hadamangeo/Hands-on Graph Neural Networks with PyTorch & PyTorch Geometric.html#dataloader",
    "href": "posts/hadamangeo/Hands-on Graph Neural Networks with PyTorch & PyTorch Geometric.html#dataloader",
    "title": "Hands-on Graph Neural Networks with PyTorch & PyTorch Geometric",
    "section": "DataLoader",
    "text": "DataLoader\n\nfrom torch.utils.data import DataLoader\n\n\nloader = DataLoader(dataset, batch_size=512, shuffle=True)\n\n\nfor batch in loader:\n    batch\n    &gt;&gt;&gt; Batch(x=[1024, 21], edge_index=[2, 1568], y=[512], batch=[1024])"
  },
  {
    "objectID": "posts/hadamangeo/ml with python 8.html",
    "href": "posts/hadamangeo/ml with python 8.html",
    "title": "지도 학습",
    "section": "",
    "text": "ref\n\n선형대수와 통계학으로 배우는 머신러닝 with 파이썬\ngithub\n\n\n\nk-최근법 이웃 알고리즘\n\n\n# 데이터 불러오기\nfrom sklearn import datasets\nraw_iris = datasets.load_iris()\n\n# 피쳐/타겟\nX = raw_iris.data\ny = raw_iris.target\n\n# 트레이닝/테스트 데이터 분할\nfrom sklearn.model_selection import train_test_split\nX_tn, X_te, y_tn, y_te=train_test_split(X,y,random_state=0)\n\n#데이터 표준화\nfrom sklearn.preprocessing import StandardScaler\nstd_scale = StandardScaler()\nstd_scale.fit(X_tn)\nX_tn_std = std_scale.transform(X_tn)\nX_te_std  = std_scale.transform(X_te)\n\n\n# 학습\nfrom sklearn.neighbors import KNeighborsClassifier\nclf_knn =  KNeighborsClassifier(n_neighbors=2)\nclf_knn.fit(X_tn_std, y_tn)\n\nKNeighborsClassifier(n_neighbors=2)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.KNeighborsClassifierKNeighborsClassifier(n_neighbors=2)\n\n\n\n# 예측\nknn_pred = clf_knn.predict(X_te_std)\nprint(knn_pred)\n\n[2 1 0 2 0 2 0 1 1 1 1 1 1 1 1 0 1 1 0 0 2 1 0 0 2 0 0 1 1 0 2 1 0 2 2 1 0\n 2]\n\n\n\n# 정확도\nfrom sklearn.metrics import accuracy_score\naccuracy = accuracy_score(y_te, knn_pred)\nprint(accuracy)\n\n0.9473684210526315\n\n\n\n# confusion matrix 확인 \nfrom sklearn.metrics import confusion_matrix\nconf_matrix = confusion_matrix(y_te, knn_pred)\nprint(conf_matrix)\n\n[[13  0  0]\n [ 0 15  1]\n [ 0  1  8]]\n\n\n\n# 분류 레포트 확인\nfrom sklearn.metrics import classification_report\nclass_report = classification_report(y_te, knn_pred)\nprint(class_report)\n\n              precision    recall  f1-score   support\n\n           0       1.00      1.00      1.00        13\n           1       0.94      0.94      0.94        16\n           2       0.89      0.89      0.89         9\n\n    accuracy                           0.95        38\n   macro avg       0.94      0.94      0.94        38\nweighted avg       0.95      0.95      0.95        38\n\n\n\n\n\n선형 회귀 분석\nfrom sklearn import datasets\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.linear_model import Ridge\nfrom sklearn.linear_model import Lasso\nfrom sklearn.linear_model import ElasticNet\nfrom sklearn.linear_model import LinearRegression \n\nfrom sklearn.metrics import r2_score\nfrom sklearn.metrics import mean_squared_error\n\n\n# 데이터 불러오기\nraw_boston = datasets.load_boston()\n\n# 피쳐, 타겟 데이터 지정\nX = raw_boston.data\ny = raw_boston.target\n\n# 트레이닝/테스트 데이터 분할\nX_tn, X_te, y_tn, y_te = train_test_split(X,y,random_state=1)\n\n\n# 데이터 표준화\nstd_scale = StandardScaler()\nstd_scale.fit(X_tn)\nX_tn_std = std_scale.transform(X_tn)\nX_te_std  = std_scale.transform(X_te)\n\n# 선형 회귀분석 학습\nclf_lr =  LinearRegression()\nclf_lr.fit(X_tn_std, y_tn)\n\n# 선형 회귀분석 모형 추정 계수 확인\nprint(clf_lr.coef_)\nprint(clf_lr.intercept_)\n\n# 릿지 회귀분석(L2 제약식 적용)\nclf_ridge = Ridge(alpha=1)\nclf_ridge.fit(X_tn_std, y_tn)\n\n# 릿지 회귀분석 모형 추정 계수 확인\nprint(clf_ridge.coef_)\nprint(clf_ridge.intercept_)\n\n# 라쏘 회귀분석(L1 제약식 적용)\nclf_lasso = Lasso(alpha=0.01)\nclf_lasso.fit(X_tn_std, y_tn)\n\n# 라쏘 회귀분석 모형 추정 계수 확인\nprint(clf_lasso.coef_)\nprint(clf_lasso.intercept_)\n\n# 엘라스틱넷\nclf_elastic = ElasticNet(alpha=0.01, l1_ratio=0.01)\nclf_elastic.fit(X_tn_std, y_tn)\n\n# 엘라스틱넷 모형 추정 계수 확인\nprint(clf_elastic.coef_)\nprint(clf_elastic.intercept_)\n\n# 예측\npred_lr = clf_lr.predict(X_te_std)\npred_ridge = clf_ridge.predict(X_te_std)\npred_lasso = clf_lasso.predict(X_te_std)\npred_elastic = clf_elastic.predict(X_te_std)\n\n# 모형 평가-R제곱값\nprint(r2_score(y_te, pred_lr))\nprint(r2_score(y_te, pred_ridge))\nprint(r2_score(y_te, pred_lasso))\nprint(r2_score(y_te, pred_elastic))\n\n# 모형 평가-MSE\nprint(mean_squared_error(y_te, pred_lr))\nprint(mean_squared_error(y_te, pred_ridge))\nprint(mean_squared_error(y_te, pred_lasso))\nprint(mean_squared_error(y_te, pred_elastic))\n- 회귀분석\n\\[\\hat w = (X^TX)^{-1}X^Ty\\]\n- 릿지 회귀 분석(L2제약식)\n\\[\\hat w^{ridge} = (X^TX+ \\lambda I_p)^{-1}X^Ty\\]\n\n\\(\\lambda\\) 계수의 사이즈 조절, 정규식의 크기 조절, 0에 가까울수록 최소 제곱 추정량에 가까워지며 무한대에 가까워질수록 릿지 해는 0에 가까워짐\n편향(bias)가 존재\n\n- 라쏘 회귀 분석(L1제약식)\n\\[\\hat w^{lasso}=argmin_w \\{(y-Xw)^T(y-Xw)+\\lambda(|w|-t) \\}\\]\n\n\n로지스틱 회귀 분석\n\n# 데이터 불러오기\nfrom sklearn import datasets\nraw_cancer = datasets.load_breast_cancer()\n\n# 피쳐, 타겟 데이터 지정\nX = raw_cancer.data\ny = raw_cancer.target\n\n# 트레이닝/테스트 데이터 분할\nfrom sklearn.model_selection import train_test_split\nX_tn, X_te, y_tn, y_te=train_test_split(X,y,random_state=0)\n\n\n\n\n#데이터 표준화\nfrom sklearn.preprocessing import StandardScaler\nstd_scale = StandardScaler()\nstd_scale.fit(X_tn)\nX_tn_std = std_scale.transform(X_tn)\nX_te_std  = std_scale.transform(X_te)\n\n\n# 로지스틱 회귀분석(L2 제약식 적용)\nfrom sklearn.linear_model import LogisticRegression\nclf_logi_l2 =  LogisticRegression(penalty='l2')\nclf_logi_l2.fit(X_tn_std, y_tn)\n\nLogisticRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LogisticRegressionLogisticRegression()\n\n\n\n# 로지스틱 회귀분석 모형(L2 제약식 적용) 추정 계수\nprint(clf_logi_l2.coef_) # 추정 계수\nprint(clf_logi_l2.intercept_) # 상수항\n\n[[-0.29792942 -0.58056355 -0.3109406  -0.377129   -0.11984232  0.42855478\n  -0.71131106 -0.85371164 -0.46688191  0.11762548 -1.38262136  0.0899184\n  -0.94778563 -0.94686238  0.18575731  0.99305313  0.11090349 -0.3458275\n   0.20290919  0.80470317 -0.91626377 -0.91726667 -0.8159834  -0.86539197\n  -0.45539191  0.10347391 -0.83009341 -0.98445173 -0.5920036  -0.61086989]]\n[0.02713751]\n\n\n\n# 예측\npred_logistic = clf_logi_l2.predict(X_te_std)\nprint(pred_logistic)\n\n[0 1 1 1 1 1 1 1 1 1 1 1 1 0 1 0 1 0 0 0 0 0 1 1 0 1 1 0 1 0 1 0 1 0 1 0 1\n 0 1 0 0 1 0 1 1 0 1 1 1 0 0 0 0 1 1 1 1 1 1 0 0 0 1 1 0 1 0 0 0 1 1 0 1 0\n 0 1 1 1 1 1 0 0 0 1 0 1 1 1 0 0 1 0 0 0 1 1 0 1 1 1 1 1 1 1 0 1 0 1 1 1 1\n 0 0 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 0 1 1 1 1 1 0 0 0 1 1 1 0]\n\n\n\n# 확률값으로 예측\npred_proba = clf_logi_l2.predict_proba(X_te_std)\nprint(pred_proba)\n\n[[9.98638613e-01 1.36138656e-03]\n [3.95544804e-02 9.60445520e-01]\n [1.30896362e-03 9.98691036e-01]\n [1.24473354e-02 9.87552665e-01]\n [2.44132101e-04 9.99755868e-01]\n [4.50491513e-03 9.95495085e-01]\n [1.13985968e-04 9.99886014e-01]\n [1.82475894e-03 9.98175241e-01]\n [9.67965506e-05 9.99903203e-01]\n [1.75222878e-06 9.99998248e-01]\n [1.76572612e-01 8.23427388e-01]\n [8.24119135e-02 9.17588087e-01]\n [9.66067493e-06 9.99990339e-01]\n [5.39343196e-01 4.60656804e-01]\n [3.98187854e-01 6.01812146e-01]\n [9.95762760e-01 4.23724017e-03]\n [2.75612083e-03 9.97243879e-01]\n [9.99997097e-01 2.90271401e-06]\n [9.99926506e-01 7.34935682e-05]\n [9.99999997e-01 2.78313939e-09]\n [9.98738365e-01 1.26163489e-03]\n [9.81405399e-01 1.85946008e-02]\n [1.77902039e-02 9.82209796e-01]\n [9.65876713e-04 9.99034123e-01]\n [9.99464578e-01 5.35421808e-04]\n [6.73385015e-04 9.99326615e-01]\n [5.50833875e-05 9.99944917e-01]\n [9.69828919e-01 3.01710813e-02]\n [1.62119075e-03 9.98378809e-01]\n [9.99997821e-01 2.17867101e-06]\n [6.00571253e-05 9.99939943e-01]\n [9.99954808e-01 4.51921300e-05]\n [1.09252006e-01 8.90747994e-01]\n [9.97255978e-01 2.74402243e-03]\n [4.51047979e-06 9.99995490e-01]\n [9.97449456e-01 2.55054412e-03]\n [1.97830173e-02 9.80216983e-01]\n [9.99571529e-01 4.28470822e-04]\n [8.45566258e-03 9.91544337e-01]\n [9.99487912e-01 5.12087502e-04]\n [9.42409583e-01 5.75904174e-02]\n [8.34700429e-05 9.99916530e-01]\n [9.32505814e-01 6.74941855e-02]\n [8.11944408e-05 9.99918806e-01]\n [6.08911689e-02 9.39108831e-01]\n [9.99999999e-01 1.17373572e-09]\n [1.00967748e-06 9.99998990e-01]\n [1.48182234e-02 9.85181777e-01]\n [6.33630458e-04 9.99366370e-01]\n [9.99927519e-01 7.24813084e-05]\n [9.99989528e-01 1.04724511e-05]\n [8.04262948e-01 1.95737052e-01]\n [9.99965014e-01 3.49860375e-05]\n [1.36691079e-03 9.98633089e-01]\n [1.95330244e-03 9.98046698e-01]\n [5.74609838e-04 9.99425390e-01]\n [1.05063052e-03 9.98949369e-01]\n [7.96089471e-03 9.92039105e-01]\n [1.00288029e-02 9.89971197e-01]\n [9.99999999e-01 1.44073341e-09]\n [9.97609027e-01 2.39097260e-03]\n [9.99257870e-01 7.42129950e-04]\n [3.14309030e-05 9.99968569e-01]\n [4.40044150e-03 9.95599559e-01]\n [9.99897373e-01 1.02627439e-04]\n [1.52976144e-01 8.47023856e-01]\n [1.00000000e+00 2.39185116e-13]\n [9.99998777e-01 1.22317020e-06]\n [9.99999046e-01 9.53579837e-07]\n [7.96239235e-04 9.99203761e-01]\n [3.87033734e-01 6.12966266e-01]\n [9.99993469e-01 6.53125942e-06]\n [2.97085842e-03 9.97029142e-01]\n [8.09412134e-01 1.90587866e-01]\n [9.99996998e-01 3.00240009e-06]\n [1.75950117e-02 9.82404988e-01]\n [4.94325863e-05 9.99950567e-01]\n [3.51047770e-02 9.64895223e-01]\n [4.25841119e-04 9.99574159e-01]\n [2.09232609e-05 9.99979077e-01]\n [9.82374564e-01 1.76254356e-02]\n [1.00000000e+00 3.57855006e-10]\n [9.99988747e-01 1.12526453e-05]\n [5.94724730e-05 9.99940528e-01]\n [9.62731634e-01 3.72683662e-02]\n [1.69452548e-03 9.98305475e-01]\n [6.14966533e-05 9.99938503e-01]\n [6.36886875e-06 9.99993631e-01]\n [9.99902779e-01 9.72205364e-05]\n [1.00000000e+00 8.14423797e-11]\n [3.47458432e-05 9.99965254e-01]\n [5.53589378e-01 4.46410622e-01]\n [6.91462937e-01 3.08537063e-01]\n [9.99996851e-01 3.14924112e-06]\n [2.01951834e-03 9.97980482e-01]\n [2.39759190e-03 9.97602408e-01]\n [9.99999992e-01 7.92006333e-09]\n [1.03400237e-02 9.89659976e-01]\n [9.23218910e-03 9.90767811e-01]\n [9.80048490e-04 9.99019952e-01]\n [5.45753731e-09 9.99999995e-01]\n [3.09034901e-03 9.96909651e-01]\n [6.22819445e-03 9.93771806e-01]\n [1.49494565e-01 8.50505435e-01]\n [9.99994787e-01 5.21292981e-06]\n [6.02188244e-04 9.99397812e-01]\n [9.99995658e-01 4.34219020e-06]\n [9.49795077e-02 9.05020492e-01]\n [3.27428663e-01 6.72571337e-01]\n [1.72350019e-02 9.82764998e-01]\n [3.75686888e-02 9.62431311e-01]\n [9.99975711e-01 2.42887910e-05]\n [9.99911399e-01 8.86014791e-05]\n [8.65663331e-02 9.13433667e-01]\n [8.21398481e-04 9.99178602e-01]\n [2.45946373e-02 9.75405363e-01]\n [1.43898490e-01 8.56101510e-01]\n [1.58128486e-03 9.98418715e-01]\n [1.79682971e-02 9.82031703e-01]\n [1.18803803e-03 9.98811962e-01]\n [1.55728346e-02 9.84427165e-01]\n [1.43822197e-03 9.98561778e-01]\n [3.86829219e-01 6.13170781e-01]\n [2.65232841e-02 9.73476716e-01]\n [9.99999918e-01 8.17382381e-08]\n [1.28424726e-01 8.71575274e-01]\n [4.67709202e-01 5.32290798e-01]\n [2.58725940e-04 9.99741274e-01]\n [3.25269018e-05 9.99967473e-01]\n [4.00075207e-05 9.99959992e-01]\n [9.99901036e-01 9.89636008e-05]\n [1.27248974e-04 9.99872751e-01]\n [2.66411581e-04 9.99733588e-01]\n [2.13163719e-01 7.86836281e-01]\n [2.92511631e-02 9.70748837e-01]\n [2.37309476e-05 9.99976269e-01]\n [5.09465728e-01 4.90534272e-01]\n [6.17881971e-01 3.82118029e-01]\n [1.00000000e+00 1.46648090e-12]\n [8.41453252e-05 9.99915855e-01]\n [1.58701592e-03 9.98412984e-01]\n [1.26424968e-03 9.98735750e-01]\n [9.99999994e-01 5.81805301e-09]]\n\n\n\n# 정밀도\nfrom sklearn.metrics import precision_score\nprecision = precision_score(y_te, pred_logistic)\nprint(precision)\n\n0.9666666666666667\n\n\n\n# confusion matrix 확인 \nfrom sklearn.metrics import confusion_matrix\nconf_matrix = confusion_matrix(y_te, pred_logistic)\nprint(conf_matrix)\n\n[[50  3]\n [ 3 87]]\n\n\n\n# 분류 레포트 확인\nfrom sklearn.metrics import classification_report\nclass_report = classification_report(y_te, pred_logistic)\nprint(class_report)\n\n              precision    recall  f1-score   support\n\n           0       0.94      0.94      0.94        53\n           1       0.97      0.97      0.97        90\n\n    accuracy                           0.96       143\n   macro avg       0.96      0.96      0.96       143\nweighted avg       0.96      0.96      0.96       143\n\n\n\n\n\n나이브 베이즈(추후 다시)\nfrom sklearn import datasets\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.naive_bayes import GaussianNB\n\nfrom sklearn.metrics import recall_score\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report\n\n\n# 데이터 불러오기\nraw_wine = datasets.load_wine()\n\n# 피쳐, 타겟 데이터 지정\nX = raw_wine.data\ny = raw_wine.target\n\n# 트레이닝/테스트 데이터 분할\nX_tn, X_te, y_tn, y_te=train_test_split(X,y,random_state=0)\n\n# 데이터 표준화\nstd_scale = StandardScaler()\nstd_scale.fit(X_tn)\nX_tn_std = std_scale.transform(X_tn)\nX_te_std  = std_scale.transform(X_te)\n\n# 나이브 베이즈 학습\nclf_gnb = GaussianNB()\nclf_gnb.fit(X_tn_std, y_tn)\n\n# 예측\npred_gnb = clf_gnb.predict(X_te_std)\nprint(pred_gnb)\n\n# 리콜\nrecall = recall_score(y_te, pred_gnb, average='macro')\nprint(recall)\n\n# confusion matrix 확인 \nconf_matrix = confusion_matrix(y_te, pred_gnb)\nprint(conf_matrix)\n\n# 분류 레포트 확인\nclass_report = classification_report(y_te, pred_gnb)\nprint(class_report)\n\n\n의사결정나무(추후 다시)\n\n테스트 성능 평가는 엔트로피 이용\n엔트로피는 불순도(노드에 서로 다른 데이터가 얼마나 섞여 있는지) 정도를 측정하며 낮을수록 좋다.\n\n\\[Entropy(d) = - \\sum p(x) log P(x)\\]\n\\[= - \\sum_{i=1}^k p(i|d)log_2(p(i|d))\\]\n\n# 데이터 불러오기\nfrom sklearn import datasets\nraw_wine = datasets.load_wine()\n\n# 피쳐, 타겟 데이터 지정\nX = raw_wine.data\ny = raw_wine.target\n\n# 트레이닝/테스트 데이터 분할\nfrom sklearn.model_selection import train_test_split\nX_tn, X_te, y_tn, y_te=train_test_split(X,y,random_state=0)\n\n# 데이터 표준화\nfrom sklearn.preprocessing import StandardScaler\nstd_scale = StandardScaler()\nstd_scale.fit(X_tn)\nX_tn_std = std_scale.transform(X_tn)\nX_te_std  = std_scale.transform(X_te)\n\n\n# 의사결정나무 학습\nfrom sklearn import tree \nclf_tree = tree.DecisionTreeClassifier(random_state=0)\nclf_tree.fit(X_tn_std, y_tn)\n\n\nDecisionTreeClassifier(random_state=0)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.DecisionTreeClassifierDecisionTreeClassifier(random_state=0)\n\n\n\n# 예측\npred_tree = clf_tree.predict(X_te_std)\nprint(pred_tree)\n\n\n[0 2 1 0 1 1 0 2 1 1 2 2 0 1 2 1 0 0 2 0 1 0 1 1 1 1 1 1 1 2 0 0 1 0 0 0 2\n 1 1 2 1 0 1 1 1]\n\n\n\n# f1 score\nfrom sklearn.metrics import f1_score\nf1 = f1_score(y_te, pred_tree, average='macro')\nprint(f1)\n\n0.9349141206870346\n\n\n\n# confusion matrix 확인 \nfrom sklearn.metrics import confusion_matrix\nconf_matrix = confusion_matrix(y_te, pred_tree)\nprint(conf_matrix)\n\n[[14  2  0]\n [ 0 20  1]\n [ 0  0  8]]\n\n\n\n# 분류 레포트 확인\nfrom sklearn.metrics import classification_report\nclass_report = classification_report(y_te, pred_tree)\nprint(class_report)\n\n              precision    recall  f1-score   support\n\n           0       1.00      0.88      0.93        16\n           1       0.91      0.95      0.93        21\n           2       0.89      1.00      0.94         8\n\n    accuracy                           0.93        45\n   macro avg       0.93      0.94      0.93        45\nweighted avg       0.94      0.93      0.93        45\n\n\n\n\n\n서포트 벡터 머신(추후 다시)\n\n\n크로스 밸리데이션(추후 다시)"
  },
  {
    "objectID": "posts/학회.html",
    "href": "posts/학회.html",
    "title": "2023년 학회 발표 준비",
    "section": "",
    "text": "일정:23. 6.29(목) ~ 7.1 (토)\n장소: 부경대학교(부산)\n발표신청 및 초록제출: 3.20.(월) ~ 4.20.(목)\n발표요약본제출(석사과정) : ~4.20.(목)\n포스터파일제출: ~ 5.19.(금)"
  },
  {
    "objectID": "posts/학회.html#한국통계학회",
    "href": "posts/학회.html#한국통계학회",
    "title": "2023년 학회 발표 준비",
    "section": "",
    "text": "일정:23. 6.29(목) ~ 7.1 (토)\n장소: 부경대학교(부산)\n발표신청 및 초록제출: 3.20.(월) ~ 4.20.(목)\n발표요약본제출(석사과정) : ~4.20.(목)\n포스터파일제출: ~ 5.19.(금)"
  },
  {
    "objectID": "posts/학회.html#자료분석학회",
    "href": "posts/학회.html#자료분석학회",
    "title": "2023년 학회 발표 준비",
    "section": "자료분석학회",
    "text": "자료분석학회\n\n하계: 2023. 7.6.(목) ~ 7.7.(금)\n고려대학교\n발표신청 및 사전등록: 23.5.29.(월)\n초록 또는 논문제출: ~5.31.(수)"
  },
  {
    "objectID": "posts/학회.html#한국데이터마이닝학회",
    "href": "posts/학회.html#한국데이터마이닝학회",
    "title": "2023년 학회 발표 준비",
    "section": "한국데이터마이닝학회",
    "text": "한국데이터마이닝학회\n\n일정: 6.23.(금) ~ 6.24.(토)\n장소: 강릉원주대학교\n발표논문 초록 제출: ~5.5.(금)\n발표 논문 제출: ~6.16.(금)\n학생논문: ~6.9.(금)"
  },
  {
    "objectID": "posts/GAN논문정리.html",
    "href": "posts/GAN논문정리.html",
    "title": "[GAN] 논문 정리 (~ing)",
    "section": "",
    "text": "Generative Adversarial Nets\n\nhttps://proceedings.neurips.cc/paper_files/paper/2014/file/5ca3e9b122f61f8f06494c97b1afccf3-Paper.pdf\n\nlan Goodfellow(2014)\n\n\n\n가장 최초로 GAN을 제안한 논문으로 새로운 이미지를 생성하는 생성자 Generator와 샘플 데이터와 생성자가 생성한 이미지를 구분하는 구별자 Discriminator 두 네트워크 구조 제안\n\n\n\nUnsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks\nhttps://arxiv.org/pdf/1511.06434.pdf%C3\n:Ranford(2015)\n\nGAN + CNN : DCGAN(Deep Convolutinal Generative Adversarial Networks)\n\n\n\nInfoGAN: Interpretable Representation Learning by Information Maximizing Generative Adversarial Nets\nhttps://proceedings.neurips.cc/paper_files/paper/2016/file/7c9d0b1f96aebd7b5eca8c3edaa19ebb-Paper.pdf\n:Xi Chen(2016)\n\n기존 GAN 모델에서 생성자 생성 이미지와 잠재 벡터의 상호 정보량을 높이 가지는 term을 추가하여 실제 벡터 공간의 이동이나 조작이 유의미한 생성 이미지의 변화를 가져오도록 설계\n\n\n\nImproved training of wasserstein gans\nhttps://proceedings.neurips.cc/paper_files/paper/2017/file/892c3b1c6dccd52936e27cbd0ff683d6-Paper.pdf\n:Ishaan Gulrajani(2017)\n\nWassertein Distance 도입\n\n\n\nLeast Squares Generative Adversarial Networks\nhttps://openaccess.thecvf.com/content_ICCV_2017/papers/Mao_Least_Squares_Generative_ICCV_2017_paper.pdf\n:Xudong Mao(2017):LSGANs\n$min_D V_{LSGAN}(D) = {x P{data}(x)} [ (D(x) - b)^2 ] + _{z p_x(z)} [(D(G(z)) - a)^2 ] $\n\\(min_G V_{LSGAN}(G) = \\frac{1}{2} \\mathbb{E}_{z~p_z(z)} [(D(G(z))-c)^2]\\)\n\n(a): fake label\n(b): real label\n\n\n\nEnergy-based Generative Adversarial Network\n\n\nBEGAN: Boundary Equilibrium Generative Adversarial Networks\n\n\nConditional Generative Adversarial Nets\nhttps://arxiv.org/pdf/1411.1784.pdf%EF%BC%88CGAN%EF%BC%89\n\n\nref\nhttps://ysbsb.github.io/gan/2020/06/17/GAN-newbie-guide.html\nhttps://di-bigdata-study.tistory.com/12"
  },
  {
    "objectID": "posts/사기거래/낙서.html",
    "href": "posts/사기거래/낙서.html",
    "title": "Scribbling",
    "section": "",
    "text": "import numpy as np\n\n# 구한 행렬\nmatrix = np.array([[0, 5, 10, 15, 20],\n                   [5, 0, 5, 10, 15],\n                   [10, 5, 0, 5, 10],\n                   [15, 10, 5, 0, 5],\n                   [20, 15, 10, 5, 0]])\n\n# exp(-a_ij) 계산\nresult = np.exp(-matrix)\n\n# 결과 확인\nprint(result)\n\n[[1.00000000e+00 6.73794700e-03 4.53999298e-05 3.05902321e-07\n  2.06115362e-09]\n [6.73794700e-03 1.00000000e+00 6.73794700e-03 4.53999298e-05\n  3.05902321e-07]\n [4.53999298e-05 6.73794700e-03 1.00000000e+00 6.73794700e-03\n  4.53999298e-05]\n [3.05902321e-07 4.53999298e-05 6.73794700e-03 1.00000000e+00\n  6.73794700e-03]\n [2.06115362e-09 3.05902321e-07 4.53999298e-05 6.73794700e-03\n  1.00000000e+00]]\n\n\n\nimport numpy as np\n\n# 구한 행렬\nmatrix = np.array([[0, 5, 10, 15, 20],\n                   [5, 0, 5, 10, 15],\n                   [10, 5, 0, 5, 10],\n                   [15, 10, 5, 0, 5],\n                   [20, 15, 10, 5, 0]])\n\n# 행렬의 상관 계수 행렬 구하기\ncorr_matrix = np.corrcoef(matrix)\n\n# 각 행렬의 값을 corr로 나누기\nresult = matrix / corr_matrix\nresult = np.exp(-result)\n# 결과 확인\nprint(result)\n\n[[1.00000000e+00 2.45592422e-03 0.00000000e+00 6.75079804e+07\n  4.85165195e+08]\n [2.45592422e-03 1.00000000e+00 6.62778420e-06 1.16270342e+08\n  6.75079804e+07]\n [0.00000000e+00 6.62778420e-06 1.00000000e+00 6.62778420e-06\n  0.00000000e+00]\n [6.75079804e+07 1.16270342e+08 6.62778420e-06 1.00000000e+00\n  2.45592422e-03]\n [4.85165195e+08 6.75079804e+07 0.00000000e+00 2.45592422e-03\n  1.00000000e+00]]\n\n\n/tmp/ipykernel_3252510/2210747788.py:14: RuntimeWarning: divide by zero encountered in true_divide\n  result = matrix / corr_matrix\n\n\n\nimport numpy as np\n\n# 데이터 준비\nx = np.array([[1, 2, 3],\n              [4, 5, 6],\n              [7, 8, 9],\n              [10, 11, 12],\n              [13, 14, 15]])\n\nW = np.array([[0.1],\n              [0.2],\n              [0.3]])\n\n# 예측 모델 구성\ny = np.dot(x, W)\n\n# 결과 출력\nprint(y)\n\n[[1.4]\n [3.2]\n [5. ]\n [6.8]\n [8.6]]"
  },
  {
    "objectID": "posts/사기거래/9999.html",
    "href": "posts/사기거래/9999.html",
    "title": "사기거래(교수님과 같이 정리중)",
    "section": "",
    "text": "import numpy as np\nimport pandas as pd\nimport networkx as nx\nimport sklearn\n\n# split \nfrom sklearn.model_selection import train_test_split\n\n# embedding \nfrom node2vec import Node2Vec\nfrom node2vec.edges import HadamardEmbedder, AverageEmbedder, WeightedL1Embedder, WeightedL2Embedder\n\n# models \nfrom sklearn.ensemble import RandomForestClassifier \n\n# 평가 \nfrom sklearn import metrics \n\n\ndef build_graph_bipartite(df_input, graph_type=nx.Graph()):\n    df=df_input.copy()\n    mapping={x:node_id for node_id, x in enumerate(set(df[\"cc_num\"].values.tolist()+\\\n                                                      df[\"merchant\"].values.tolist()))}\n    \n    df[\"from\"]=df[\"cc_num\"].apply(lambda x:mapping[x])  #엣지의 출발점\n    df[\"to\"]=df[\"merchant\"].apply(lambda x:mapping[x])  #엣지의 도착점\n    \n    df = df[['from', 'to', \"amt\", \"is_fraud\"]].groupby(['from','to']).agg({\"is_fraud\":\"sum\",\"amt\":\"sum\"}).reset_index()\n    df[\"is_fraud\"]=df[\"is_fraud\"].apply(lambda x:1 if x&gt;0 else 0)\n    \n    G=nx.from_edgelist(df[[\"from\",\"to\"]].values, create_using=graph_type)\n    \n    nx.set_edge_attributes(G, {(int(x[\"from\"]),int(x[\"to\"])):x[\"is_fraud\"] for idx, x in df[[\"from\",\"to\",\"is_fraud\"]].iterrows()}, \"label\")  #엣지 속성 설정,각 속성의 사기 여부부 \n    \n    nx.set_edge_attributes(G,{(int(x[\"from\"]),int(x[\"to\"])):x[\"amt\"] for idx,x in df[[\"from\",\"to\",\"amt\"]].iterrows()}, \"weight\") # 엣지 속성 설정, 각 엣지의 거래 금액\n\n    return G\n\ndef down_sample_textbook(df):\n    df = df[df[\"is_fraud\"]==0].sample(frac=0.20, random_state=42).append(df[df[\"is_fraud\"] == 1])\n    df_majority = df[df.is_fraud==0]\n    df_minority = df[df.is_fraud==1]\n    df_maj_dowsampled = sklearn.utils.resample(df_majority, n_samples=len(df_minority), random_state=42)\n    df_downsampled = pd.concat([df_minority, df_maj_dowsampled])\n    return df_downsampled\n\ndef split(Graph,test_size=0.20,random_state=42):\n    edg = list(range(len(Graph.edges))) \n    edg_att = list(nx.get_edge_attributes(Graph, \"label\").values())\n    return train_test_split(edg,edg_att,test_size=test_size,random_state=random_state) \n\ndef embedding(Graph):\n    _edgs = list(Graph.edges)\n    _train_edges, _test_edges, y, yy = split(Graph)\n    _train_graph = Graph.edge_subgraph([_edgs[x] for x in _train_edges]).copy()\n    _train_graph.add_nodes_from(list(set(Graph.nodes) - set(_train_graph.nodes)))\n    _embedded = AverageEmbedder(Node2Vec(_train_graph, weight_key='weight').fit(window=10).wv)\n    X = [_embedded[str(_edgs[x][0]), str(_edgs[x][1])] for x in _train_edges]\n    XX = [_embedded[str(_edgs[x][0]), str(_edgs[x][1])] for x in _test_edges]\n    return X,XX,y,yy \n\ndef evaluate(lrnr,XX,yy):\n    yyhat = lrnr.predict(XX)\n    df = pd.DataFrame({'pre':[sklearn.metrics.precision_score(yy,yyhat)], \n                  'rec':[sklearn.metrics.recall_score(yy,yyhat)],\n                  'f1':[sklearn.metrics.f1_score(yy,yyhat)]})\n    return df \n\ndef anal(df,n_estimators=10):\n    Graph = build_graph_bipartite(df)\n    X,XX,y,yy = embedding(Graph)\n    lrnr = RandomForestClassifier(n_estimators=n_estimators, random_state=42) \n    lrnr.fit(X,y)\n    return lrnr, XX,yy, evaluate(lrnr,XX,yy)\n\ndef our_sampling1(df):\n    cus_list = set(df.query('is_fraud==1').cc_num.tolist())\n    return df.query(\"cc_num in @ cus_list\")"
  },
  {
    "objectID": "posts/사기거래/9999.html#imports",
    "href": "posts/사기거래/9999.html#imports",
    "title": "사기거래(교수님과 같이 정리중)",
    "section": "",
    "text": "import numpy as np\nimport pandas as pd\nimport networkx as nx\nimport sklearn\n\n# split \nfrom sklearn.model_selection import train_test_split\n\n# embedding \nfrom node2vec import Node2Vec\nfrom node2vec.edges import HadamardEmbedder, AverageEmbedder, WeightedL1Embedder, WeightedL2Embedder\n\n# models \nfrom sklearn.ensemble import RandomForestClassifier \n\n# 평가 \nfrom sklearn import metrics \n\n\ndef build_graph_bipartite(df_input, graph_type=nx.Graph()):\n    df=df_input.copy()\n    mapping={x:node_id for node_id, x in enumerate(set(df[\"cc_num\"].values.tolist()+\\\n                                                      df[\"merchant\"].values.tolist()))}\n    \n    df[\"from\"]=df[\"cc_num\"].apply(lambda x:mapping[x])  #엣지의 출발점\n    df[\"to\"]=df[\"merchant\"].apply(lambda x:mapping[x])  #엣지의 도착점\n    \n    df = df[['from', 'to', \"amt\", \"is_fraud\"]].groupby(['from','to']).agg({\"is_fraud\":\"sum\",\"amt\":\"sum\"}).reset_index()\n    df[\"is_fraud\"]=df[\"is_fraud\"].apply(lambda x:1 if x&gt;0 else 0)\n    \n    G=nx.from_edgelist(df[[\"from\",\"to\"]].values, create_using=graph_type)\n    \n    nx.set_edge_attributes(G, {(int(x[\"from\"]),int(x[\"to\"])):x[\"is_fraud\"] for idx, x in df[[\"from\",\"to\",\"is_fraud\"]].iterrows()}, \"label\")  #엣지 속성 설정,각 속성의 사기 여부부 \n    \n    nx.set_edge_attributes(G,{(int(x[\"from\"]),int(x[\"to\"])):x[\"amt\"] for idx,x in df[[\"from\",\"to\",\"amt\"]].iterrows()}, \"weight\") # 엣지 속성 설정, 각 엣지의 거래 금액\n\n    return G\n\ndef down_sample_textbook(df):\n    df = df[df[\"is_fraud\"]==0].sample(frac=0.20, random_state=42).append(df[df[\"is_fraud\"] == 1])\n    df_majority = df[df.is_fraud==0]\n    df_minority = df[df.is_fraud==1]\n    df_maj_dowsampled = sklearn.utils.resample(df_majority, n_samples=len(df_minority), random_state=42)\n    df_downsampled = pd.concat([df_minority, df_maj_dowsampled])\n    return df_downsampled\n\ndef split(Graph,test_size=0.20,random_state=42):\n    edg = list(range(len(Graph.edges))) \n    edg_att = list(nx.get_edge_attributes(Graph, \"label\").values())\n    return train_test_split(edg,edg_att,test_size=test_size,random_state=random_state) \n\ndef embedding(Graph):\n    _edgs = list(Graph.edges)\n    _train_edges, _test_edges, y, yy = split(Graph)\n    _train_graph = Graph.edge_subgraph([_edgs[x] for x in _train_edges]).copy()\n    _train_graph.add_nodes_from(list(set(Graph.nodes) - set(_train_graph.nodes)))\n    _embedded = AverageEmbedder(Node2Vec(_train_graph, weight_key='weight').fit(window=10).wv)\n    X = [_embedded[str(_edgs[x][0]), str(_edgs[x][1])] for x in _train_edges]\n    XX = [_embedded[str(_edgs[x][0]), str(_edgs[x][1])] for x in _test_edges]\n    return X,XX,y,yy \n\ndef evaluate(lrnr,XX,yy):\n    yyhat = lrnr.predict(XX)\n    df = pd.DataFrame({'pre':[sklearn.metrics.precision_score(yy,yyhat)], \n                  'rec':[sklearn.metrics.recall_score(yy,yyhat)],\n                  'f1':[sklearn.metrics.f1_score(yy,yyhat)]})\n    return df \n\ndef anal(df,n_estimators=10):\n    Graph = build_graph_bipartite(df)\n    X,XX,y,yy = embedding(Graph)\n    lrnr = RandomForestClassifier(n_estimators=n_estimators, random_state=42) \n    lrnr.fit(X,y)\n    return lrnr, XX,yy, evaluate(lrnr,XX,yy)\n\ndef our_sampling1(df):\n    cus_list = set(df.query('is_fraud==1').cc_num.tolist())\n    return df.query(\"cc_num in @ cus_list\")"
  },
  {
    "objectID": "posts/사기거래/9999.html#read-and-define-data",
    "href": "posts/사기거래/9999.html#read-and-define-data",
    "title": "사기거래(교수님과 같이 정리중)",
    "section": "read and define data",
    "text": "read and define data\n\ndf = pd.read_csv(\"~/Desktop/fraudTrain.csv\")\n\n\n\n\n\n\n\n\nUnnamed: 0\ntrans_date_trans_time\ncc_num\nmerchant\ncategory\namt\nfirst\nlast\ngender\nstreet\n...\nlat\nlong\ncity_pop\njob\ndob\ntrans_num\nunix_time\nmerch_lat\nmerch_long\nis_fraud\n\n\n\n\n669418\n669418\n2019-10-12 18:21\n4.089100e+18\nfraud_Haley, Jewess and Bechtelar\nshopping_pos\n7.53\nDebra\nStark\nF\n686 Linda Rest\n...\n32.3836\n-94.8653\n24536\nMultimedia programmer\n1983-10-14\nd313353fa30233e5fab5468e852d22fc\n1350066071\n32.202008\n-94.371865\n0\n\n\n32567\n32567\n2019-01-20 13:06\n4.247920e+12\nfraud_Turner LLC\ntravel\n3.79\nJudith\nMoss\nF\n46297 Benjamin Plains Suite 703\n...\n39.5370\n-83.4550\n22305\nTelevision floor manager\n1939-03-09\n88c65b4e1585934d578511e627fe3589\n1327064760\n39.156673\n-82.930503\n0\n\n\n156587\n156587\n2019-03-24 18:09\n4.026220e+12\nfraud_Klein Group\nentertainment\n59.07\nDebbie\nPayne\nF\n204 Ashley Neck Apt. 169\n...\n41.5224\n-71.9934\n4720\nBroadcast presenter\n1977-05-18\n3bd9ede04b5c093143d5e5292940b670\n1332612553\n41.657152\n-72.595751\n0\n\n\n1020243\n1020243\n2020-02-25 15:12\n4.957920e+12\nfraud_Monahan-Morar\npersonal_care\n25.58\nAlan\nParsons\nM\n0547 Russell Ford Suite 574\n...\n39.6171\n-102.4776\n207\nNetwork engineer\n1955-12-04\n19e16ee7a01d229e750359098365e321\n1361805120\n39.080346\n-103.213452\n0\n\n\n116272\n116272\n2019-03-06 23:19\n4.178100e+15\nfraud_Kozey-Kuhlman\npersonal_care\n84.96\nJill\nFlores\nF\n639 Cruz Islands\n...\n41.9488\n-86.4913\n3104\nHorticulturist, commercial\n1981-03-29\na0c8641ca1f5d6e243ed5a2246e66176\n1331075954\n42.502065\n-86.732664\n0\n\n\n\n\n5 rows × 23 columns\n\n\n\n\n# df_downsampled = down_sample_textbook(df)"
  },
  {
    "objectID": "posts/사기거래/9999.html#embedding",
    "href": "posts/사기거래/9999.html#embedding",
    "title": "사기거래(교수님과 같이 정리중)",
    "section": "embedding",
    "text": "embedding\n\n#G_down = build_graph_bipartite(df_downsampled)\n\n\n# X,XX,y,yy = embedding(G_down)"
  },
  {
    "objectID": "posts/사기거래/9999.html#learn",
    "href": "posts/사기거래/9999.html#learn",
    "title": "사기거래(교수님과 같이 정리중)",
    "section": "learn",
    "text": "learn\n\n# lrnr = RandomForestClassifier(n_estimators=10, random_state=42) \n# lrnr.fit(X,y)"
  },
  {
    "objectID": "posts/사기거래/9999.html#evaluate",
    "href": "posts/사기거래/9999.html#evaluate",
    "title": "사기거래(교수님과 같이 정리중)",
    "section": "evaluate",
    "text": "evaluate\n\n# evaluate(lrnr,XX,yy)"
  },
  {
    "objectID": "posts/사기거래/9999.html#read-and-define-data-1",
    "href": "posts/사기거래/9999.html#read-and-define-data-1",
    "title": "사기거래(교수님과 같이 정리중)",
    "section": "read and define data",
    "text": "read and define data\n\ndf = pd.read_csv(\"~/Desktop/fraudTrain.csv\")\nlrnr2, _,_,_ = anal(down_sample_textbook(our_sampling1(df)),n_estimators=100)\n\n\n\n\nGenerating walks (CPU: 1):   0%|          | 0/10 [00:00&lt;?, ?it/s]Generating walks (CPU: 1): 100%|██████████| 10/10 [00:03&lt;00:00,  3.01it/s]"
  },
  {
    "objectID": "posts/사기거래/2023-05-24-try2변형.html",
    "href": "posts/사기거래/2023-05-24-try2변형.html",
    "title": "사기거래(교수님 코드-변형)",
    "section": "",
    "text": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt \nimport networkx as nx\nimport sklearn\n\n# sklearn\nfrom sklearn import model_selection # split함수이용\nfrom sklearn import ensemble # RF,GBM\nfrom sklearn import metrics \n\n# embedding \nfrom node2vec import Node2Vec\nfrom node2vec.edges import HadamardEmbedder, AverageEmbedder, WeightedL1Embedder, WeightedL2Embedder\n\n\ndef build_graph_bipartite(df_input, graph_type=nx.Graph()):\n    df=df_input.copy()\n    mapping={x:node_id for node_id, x in enumerate(set(df[\"cc_num\"].values.tolist()+\\\n                                                      df[\"merchant\"].values.tolist()))}\n    \n    df[\"from\"]=df[\"cc_num\"].apply(lambda x:mapping[x])  #엣지의 출발점\n    df[\"to\"]=df[\"merchant\"].apply(lambda x:mapping[x])  #엣지의 도착점\n    \n    df = df[['from', 'to', \"amt\", \"is_fraud\"]].groupby(['from','to']).agg({\"is_fraud\":\"sum\",\"amt\":\"sum\"}).reset_index()\n    df[\"is_fraud\"]=df[\"is_fraud\"].apply(lambda x:1 if x&gt;0 else 0)\n    \n    G=nx.from_edgelist(df[[\"from\",\"to\"]].values, create_using=graph_type)\n    \n    nx.set_edge_attributes(G,{(int(x[\"from\"]),int(x[\"to\"])):x[\"is_fraud\"] for idx, x in df[[\"from\",\"to\",\"is_fraud\"]].iterrows()}, \"label\")  #엣지 속성 설정,각 속성의 사기 여부부     \n    nx.set_edge_attributes(G,{(int(x[\"from\"]),int(x[\"to\"])):x[\"amt\"] for idx,x in df[[\"from\",\"to\",\"amt\"]].iterrows()}, \"weight\") # 엣지 속성 설정, 각 엣지의 거래 금액\n\n    return G\n\n\ndef build_graph_tripartite(df_input, graph_type=nx.Graph()):\n    df=df_input.copy()\n    mapping={x:node_id for node_id, x in enumerate(set(df.index.values.tolist() + \n                                                       df[\"cc_num\"].values.tolist() +\n                                                       df[\"merchant\"].values.tolist()))}\n    df[\"in_node\"]= df[\"cc_num\"].apply(lambda x: mapping[x])\n    df[\"out_node\"]=df[\"merchant\"].apply(lambda x:mapping[x])\n    \n        \n    G=nx.from_edgelist([(x[\"in_node\"], mapping[idx]) for idx, x in df.iterrows()] +\\\n                        [(x[\"out_node\"], mapping[idx]) for idx, x in df.iterrows()], create_using=graph_type)\n    \n    nx.set_edge_attributes(G,{(x[\"in_node\"], mapping[idx]):x[\"is_fraud\"] for idx, x in df.iterrows()}, \"label\")     \n    nx.set_edge_attributes(G,{(x[\"out_node\"], mapping[idx]):x[\"is_fraud\"] for idx, x in df.iterrows()}, \"label\")   \n    nx.set_edge_attributes(G,{(x[\"in_node\"], mapping[idx]):x[\"amt\"] for idx, x in df.iterrows()}, \"weight\")  \n    nx.set_edge_attributes(G,{(x[\"out_node\"], mapping[idx]):x[\"amt\"] for idx, x in df.iterrows()}, \"weight\")\n\n    return G\n    \n    \ndef down_sample_textbook(df):\n    df_majority = df[df.is_fraud==0].copy()\n    df_minority = df[df.is_fraud==1].copy()\n    df_maj_dowsampled = sklearn.utils.resample(df_majority, n_samples=len(df_minority), replace=False, random_state=42)\n    df_downsampled = pd.concat([df_minority, df_maj_dowsampled])\n    return df_downsampled\n\ndef embedding(Graph):\n    # Graph -&gt; X (feature)\n    _edgs = list(Graph.edges)\n    subGraph = Graph.edge_subgraph([_edgs[x] for x in range(len(Graph.edges))]).copy()\n    subGraph.add_nodes_from(list(set(Graph.nodes) - set(subGraph.nodes)))    \n    embedded = AverageEmbedder(Node2Vec(subGraph, weight_key='weight').fit(window=10).wv)\n    X = [embedded[str(_edgs[x][0]), str(_edgs[x][1])] for x in range(len(Graph.edges))]\n    # Graph -&gt; y (label)\n    y = np.array(list(nx.get_edge_attributes(Graph, \"label\").values()))\n    return X,y \n\ndef anal(df):\n    Graph = build_graph_bipartite(df)\n    X,XX,y,yy = embedding(Graph)\n    lrnr = RandomForestClassifier(n_estimators=100, random_state=42) \n    lrnr.fit(X,y)\n    yyhat = lrnr.predict(XX)\n    df = pd.DataFrame({\n        'acc':[sklearn.metrics.accuracy_score(yy,yyhat)], \n        'pre':[sklearn.metrics.precision_score(yy,yyhat)], \n        'rec':[sklearn.metrics.recall_score(yy,yyhat)],\n        'f1':[sklearn.metrics.f1_score(yy,yyhat)]}\n    )    \n    return df\n\ndef our_sampling1(df):\n    cus_list = set(df.query('is_fraud==1').cc_num.tolist())\n    return df.query(\"cc_num in @ cus_list\")"
  },
  {
    "objectID": "posts/사기거래/2023-05-24-try2변형.html#데이터-종류",
    "href": "posts/사기거래/2023-05-24-try2변형.html#데이터-종류",
    "title": "사기거래(교수님 코드-변형)",
    "section": "데이터 종류",
    "text": "데이터 종류\n\nfraudTrain.csv: (1048575, 23), 기본데이터\ndf02: (214520, 23), is_fraud==0 에서는 20퍼의 샘플만, is_fraud==1 에서는 모든 샘플을 뽑아서 정리한 새로운 자료\ndf50 = (12012, 23), df20에서 is_fraud==0 와 is_fraud==1 의 비율을 맞추어서 샘플을 뽑은 것\n\n\n\n\n\n\n\n\n\n\n데이터\nshape\n사기거래빈도\n설명\n\n\n\n\nfraudTrain\n(1048575, 22)\n0.00573\n원래자료\n\n\ndf02\n(214520, 22)\n0.028\nis_fraud==0 에서는 20퍼의 샘플만, is_fraud==1 에서는 모든 샘플을 뽑아서 정리한 새로운 자료\n\n\ndf50\n(12012, 22)\n0.5\ndf02에서 사기비율을 50퍼로 맞추어 샘플링한 자료\n\n\ndf50_tr\n(9009, 22)\n0.49828\ndf50에서 랜덤으로 train/test를 분리하여 얻은 train dataset\n\n\ndf50_test\n(3003, 22)\n0.50516\ndf50에서 랜덤으로 train/test를 분리하여 얻은 test dataset\n\n\ndf02_tr\n(211517, 22)\n0.02122\ndf02에서 df50_test에 해당하는 인덱스를 제외\n\n\nfraudTrain_tr\n(1045572, 22)\n0.00429\nfraudTrain에서 df50_test에 해당하는 인덱스를 제외\n\n\n\n- fraudTrain\n\nfraudTrain = pd.read_csv(\"~/Desktop/fraudTrain.csv\").iloc[:,1:]\n\n\nfraudTrain.is_fraud.mean().round(5)\n\n0.00573\n\n\n- df20\n\n_df1 = fraudTrain[fraudTrain[\"is_fraud\"] == 0].sample(frac=0.20, random_state=42)\n_df2 = fraudTrain[fraudTrain[\"is_fraud\"] == 1]\ndf02 = pd.concat([_df1,_df2])\ndf02.shape\n\n(214520, 22)\n\n\n\ndf02.is_fraud.mean().round(5)\n\n0.028\n\n\n- df50\n\ndf50 = down_sample_textbook(df02)\ndf50.shape\n\n(12012, 22)\n\n\n\ndf50\n\n\n\n\n\n\n\n\ntrans_date_trans_time\ncc_num\nmerchant\ncategory\namt\nfirst\nlast\ngender\nstreet\ncity\n...\nlat\nlong\ncity_pop\njob\ndob\ntrans_num\nunix_time\nmerch_lat\nmerch_long\nis_fraud\n\n\n\n\n2449\n2019-01-02 1:06\n4.613310e+12\nfraud_Rutherford-Mertz\ngrocery_pos\n281.06\nJason\nMurphy\nM\n542 Steve Curve Suite 011\nCollettsville\n...\n35.9946\n-81.7266\n885\nSoil scientist\n1988-09-15\ne8a81877ae9a0a7f883e15cb39dc4022\n1325466397\n36.430124\n-81.179483\n1\n\n\n2472\n2019-01-02 1:47\n3.401870e+14\nfraud_Jenkins, Hauck and Friesen\ngas_transport\n11.52\nMisty\nHart\nF\n27954 Hall Mill Suite 575\nSan Antonio\n...\n29.4400\n-98.4590\n1595797\nHorticultural consultant\n1960-10-28\nbc7d41c41103877b03232f03f1f8d3f5\n1325468849\n29.819364\n-99.142791\n1\n\n\n2523\n2019-01-02 3:05\n3.401870e+14\nfraud_Goodwin-Nitzsche\ngrocery_pos\n276.31\nMisty\nHart\nF\n27954 Hall Mill Suite 575\nSan Antonio\n...\n29.4400\n-98.4590\n1595797\nHorticultural consultant\n1960-10-28\nb98f12f4168391b2203238813df5aa8c\n1325473523\n29.273085\n-98.836360\n1\n\n\n2546\n2019-01-02 3:38\n4.613310e+12\nfraud_Erdman-Kertzmann\ngas_transport\n7.03\nJason\nMurphy\nM\n542 Steve Curve Suite 011\nCollettsville\n...\n35.9946\n-81.7266\n885\nSoil scientist\n1988-09-15\n397894a5c4c02e3c61c784001f0f14e4\n1325475483\n35.909292\n-82.091010\n1\n\n\n2553\n2019-01-02 3:55\n3.401870e+14\nfraud_Koepp-Parker\ngrocery_pos\n275.73\nMisty\nHart\nF\n27954 Hall Mill Suite 575\nSan Antonio\n...\n29.4400\n-98.4590\n1595797\nHorticultural consultant\n1960-10-28\n7863235a750d73a244c07f1fb7f0185a\n1325476547\n29.786426\n-98.683410\n1\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n363827\n2019-06-17 19:30\n2.475090e+15\nfraud_Frami Group\nentertainment\n81.13\nJohn\nMiller\nM\n153 Mccullough Springs Apt. 857\nLamberton\n...\n44.2378\n-95.2739\n1507\nLand/geomatics surveyor\n1993-10-12\nc66cb411019c7dfd4d89f42a1ba4765f\n1339961448\n44.212695\n-95.661879\n0\n\n\n140154\n2019-03-17 14:33\n2.131550e+14\nfraud_Bahringer-Streich\nfood_dining\n55.00\nChristopher\nSheppard\nM\n39218 Baker Shoals\nBristow\n...\n38.1981\n-86.6821\n965\nHorticultural therapist\n1982-02-10\n316b9d25b9fa7d08a6831b7dab6634cd\n1331994839\n38.394240\n-86.413557\n0\n\n\n860597\n2019-12-17 12:31\n2.280870e+15\nfraud_Lubowitz-Walter\nkids_pets\n8.12\nKatherine\nCooper\nF\n3854 Lauren Springs Suite 648\nOakford\n...\n40.0994\n-89.9601\n530\nTransport planner\n1967-09-23\nd92e9e63d9b24c3ccb92d05cba4cac54\n1355747517\n39.695248\n-89.853063\n0\n\n\n29341\n2019-01-18 9:20\n4.878360e+15\nfraud_Denesik and Sons\nshopping_pos\n3.52\nTina\nAlvarez\nF\n1976 Tyler Underpass\nEarly\n...\n42.4483\n-95.1726\n885\nPilot, airline\n1949-08-14\n8390ce51cfb8482b618ebc4ac370bcf7\n1326878457\n42.633204\n-95.598143\n0\n\n\n529797\n2019-08-16 13:17\n4.450830e+15\nfraud_Beier and Sons\nhome\n84.15\nDonna\nDavis\nF\n6760 Donovan Lakes\nClayton\n...\n34.5906\n-95.3800\n1760\nOccupational psychologist\n1972-01-20\n04e1be9bcb18ea8b96048659bd02177b\n1345123058\n33.885236\n-95.885110\n0\n\n\n\n\n12012 rows × 22 columns\n\n\n\n\ndf50.is_fraud.mean().round(5)\n\n0.5\n\n\n- df50_tr, df50_test\n\ndf50_tr,df50_test = sklearn.model_selection.train_test_split(df50, random_state=42)\n\n\ndf50_tr.is_fraud.mean().round(5), df50_test.is_fraud.mean().round(5)\n\n(0.49828, 0.50516)\n\n\n- df02_tr, fraudTrain_tr\n\ndf02_tr = df02.loc[[i not in df50_test.index for i in df02.index],:].copy()\nfraudTrain_tr = fraudTrain.loc[[i not in df50_test.index for i in fraudTrain.index],:].copy()\n\n\ndf02_tr.shape, fraudTrain_tr.shape\n\n((211517, 22), (1045572, 22))\n\n\n\ndf02_tr.is_fraud.mean().round(5), fraudTrain_tr.is_fraud.mean().round(5)\n\n(0.02122, 0.00429)"
  },
  {
    "objectID": "posts/사기거래/교수님.html",
    "href": "posts/사기거래/교수님.html",
    "title": "분석",
    "section": "",
    "text": "import numpy as np\nimport pandas as pd\nimport networkx as nx\nimport sklearn\n\n# sklearn\nfrom sklearn import model_selection # split함수이용\nfrom sklearn import ensemble # RF \nfrom sklearn import metrics \n\n# embedding \nfrom node2vec import Node2Vec\nfrom node2vec.edges import HadamardEmbedder, AverageEmbedder, WeightedL1Embedder, WeightedL2Embedder\n\n\ndef build_graph_bipartite(df_input, graph_type=nx.Graph()):\n    df=df_input.copy()\n    mapping={x:node_id for node_id, x in enumerate(set(df[\"cc_num\"].values.tolist()+\\\n                                                      df[\"merchant\"].values.tolist()))}\n    \n    df[\"from\"]=df[\"cc_num\"].apply(lambda x:mapping[x])  #엣지의 출발점\n    df[\"to\"]=df[\"merchant\"].apply(lambda x:mapping[x])  #엣지의 도착점\n    \n    df = df[['from', 'to', \"amt\", \"is_fraud\"]].groupby(['from','to']).agg({\"is_fraud\":\"sum\",\"amt\":\"sum\"}).reset_index()\n    df[\"is_fraud\"]=df[\"is_fraud\"].apply(lambda x:1 if x&gt;0 else 0)\n    \n    G=nx.from_edgelist(df[[\"from\",\"to\"]].values, create_using=graph_type)\n    \n    nx.set_edge_attributes(G,{(int(x[\"from\"]),int(x[\"to\"])):x[\"is_fraud\"] for idx, x in df[[\"from\",\"to\",\"is_fraud\"]].iterrows()}, \"label\")  #엣지 속성 설정,각 속성의 사기 여부부     \n    nx.set_edge_attributes(G,{(int(x[\"from\"]),int(x[\"to\"])):x[\"amt\"] for idx,x in df[[\"from\",\"to\",\"amt\"]].iterrows()}, \"weight\") # 엣지 속성 설정, 각 엣지의 거래 금액\n\n    return G\n\ndef build_graph_tripartite(df_input, graph_type=nx.Graph()):\n    df=df_input.copy()\n    mapping={x:node_id for node_id, x in enumerate(set(df.index.values.tolist() + \n                                                       df[\"cc_num\"].values.tolist() +\n                                                       df[\"merchant\"].values.tolist()))}\n    df[\"in_node\"]= df[\"cc_num\"].apply(lambda x: mapping[x])\n    df[\"out_node\"]=df[\"merchant\"].apply(lambda x:mapping[x])\n    \n        \n    G=nx.from_edgelist([(x[\"in_node\"], mapping[idx]) for idx, x in df.iterrows()] +\\\n                        [(x[\"out_node\"], mapping[idx]) for idx, x in df.iterrows()], create_using=graph_type)\n    \n    nx.set_edge_attributes(G,{(x[\"in_node\"], mapping[idx]):x[\"is_fraud\"] for idx, x in df.iterrows()}, \"label\")     \n    nx.set_edge_attributes(G,{(x[\"out_node\"], mapping[idx]):x[\"is_fraud\"] for idx, x in df.iterrows()}, \"label\")   \n    nx.set_edge_attributes(G,{(x[\"in_node\"], mapping[idx]):x[\"amt\"] for idx, x in df.iterrows()}, \"weight\")  \n    nx.set_edge_attributes(G,{(x[\"out_node\"], mapping[idx]):x[\"amt\"] for idx, x in df.iterrows()}, \"weight\")\n\n    return G\n    \n    \ndef down_sample_textbook(df):\n    df_majority = df[df.is_fraud==0]\n    df_minority = df[df.is_fraud==1]\n    df_maj_dowsampled = sklearn.utils.resample(df_majority, n_samples=len(df_minority), random_state=42)\n    df_downsampled = pd.concat([df_minority, df_maj_dowsampled]).reset_index(drop=True)\n    df_downsampled = df_downsampled.reindex(np.random.permutation(range(df_downsampled.shape[0]))).reset_index(drop=True)\n    return df_downsampled\n\ndef embedding(Graph):\n    # Graph -&gt; X (feature)\n    _edgs = list(Graph.edges)\n    subGraph = Graph.edge_subgraph([_edgs[x] for x in range(len(Graph.edges))]).copy()\n    subGraph.add_nodes_from(list(set(Graph.nodes) - set(subGraph.nodes)))    \n    embedded = AverageEmbedder(Node2Vec(subGraph, weight_key='weight').fit(window=10).wv)\n    X = [embedded[str(_edgs[x][0]), str(_edgs[x][1])] for x in range(len(Graph.edges))]\n    # Graph -&gt; y (label)\n    y = np.array(list(nx.get_edge_attributes(Graph, \"label\").values()))\n    return X,y \n\ndef anal(df):\n    Graph = build_graph_bipartite(df)\n    X,XX,y,yy = embedding(Graph)\n    lrnr = RandomForestClassifier(n_estimators=100, random_state=42) \n    lrnr.fit(X,y)\n    yyhat = lrnr.predict(XX)\n    df = pd.DataFrame({\n        'acc':[sklearn.metrics.accuracy_score(yy,yyhat)], \n        'pre':[sklearn.metrics.precision_score(yy,yyhat)], \n        'rec':[sklearn.metrics.recall_score(yy,yyhat)],\n        'f1':[sklearn.metrics.f1_score(yy,yyhat)]}\n    )    \n    return df\n\ndef our_sampling1(df):\n    cus_list = set(df.query('is_fraud==1').cc_num.tolist())\n    return df.query(\"cc_num in @ cus_list\")\n\n- 원본데이터\n\nfraudTrain = pd.read_csv(\"~/Desktop/fraudTrain.csv\").iloc[:,1:]\nfraudTrain.shape\n\n(1048575, 22)\n\n\n\nfraudTrain.is_fraud.mean().round(5)\n\n0.00573\n\n\n\n총 1048575의 데이터 중 사기거래는 0.00573\n\n- 정상거래의 20퍼 랜덤샘플링\n\n_df1 = fraudTrain[fraudTrain[\"is_fraud\"] == 0].sample(frac=0.20, random_state=42)\n_df2 = fraudTrain[fraudTrain[\"is_fraud\"] == 1]\ndf02 = pd.concat([_df1,_df2])\n\n\ndf02 = pd.concat([_df1,_df2]).reset_index(drop=True)\ndf02.shape\n\n(214520, 22)\n\n\n\ndf02.is_fraud.mean().round(5)\n\n0.028\n\n\n\n총 214,502의 데이터 중 사기거래는 0.028에 해당\n\n- downsample\n\ndf50 = down_sample_textbook(df02)\ndf50.shape\n\n(12012, 22)\n\n\n\ndf50.is_fraud.mean().round(5)\n\n0.5\n\n\n\n기존 파일에서 사기거래=1인 데이터의 숫자에 맞춰 downsampling함.\n12,012의 데이터중 0.5가 사기거래 비율\n\n\ndf02_tr,df02_test = sklearn.model_selection.train_test_split(df02)\ndf50_tr,df50_test = sklearn.model_selection.train_test_split(df50)\n\n\nprint(df02_tr.shape)\nprint(df02_test.shape)\nprint(df50_tr.shape)\nprint(df50_test.shape)\n\n(160890, 22)\n(53630, 22)\n(9009, 22)\n(3003, 22)\n\n\n0.75/0.25로 나눴다.\n\nX=np.array(df50_tr.amt).reshape(-1,1)\nXX = np.array(df50_test.amt).reshape(-1,1)\ny = np.array(df50_tr.is_fraud)\nyy = np.array(df50_test.is_fraud)\n\n\nlrnr = sklearn.linear_model.LogisticRegression()\n\n\nlrnr.fit(X,y)\n\nLogisticRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LogisticRegressionLogisticRegression()\n\n\n\nyyhat = lrnr.predict(XX) \n\n\nyyhat\n\narray([1, 1, 1, ..., 0, 1, 0])\n\n\n\nmetrics = [sklearn.metrics.accuracy_score,\n           sklearn.metrics.precision_score,\n           sklearn.metrics.recall_score,\n           sklearn.metrics.f1_score]\n\n\n{m.__name__:m(yy,yyhat).round(6) for m in metrics}\n\n{'accuracy_score': 0.867799,\n 'precision_score': 0.952188,\n 'recall_score': 0.776603,\n 'f1_score': 0.855479}\n\n\n\nGtr = build_graph_tripartite(df50_tr)\nGtest = build_graph_tripartite(df50_test)\n\n\n    df=df50.copy();graph_type=nx.Graph()\n    mapping={x:node_id for node_id, x in enumerate(set(df.index.values.tolist() + \n                                                       df[\"cc_num\"].values.tolist() +\n                                                       df[\"merchant\"].values.tolist()))}\n    df[\"in_node\"]= df[\"cc_num\"].apply(lambda x: mapping[x])\n    df[\"out_node\"]=df[\"merchant\"].apply(lambda x:mapping[x])\n    \n        \n    G=nx.from_edgelist([(x[\"in_node\"], mapping[idx]) for idx, x in df.iterrows()] +\\\n                        [(x[\"out_node\"], mapping[idx]) for idx, x in df.iterrows()], create_using=graph_type)\n    \n    nx.set_edge_attributes(G,{(x[\"in_node\"], mapping[idx]):x[\"is_fraud\"] for idx, x in df.iterrows()}, \"label\")\n     \n    nx.set_edge_attributes(G,{(x[\"out_node\"], mapping[idx]):x[\"is_fraud\"] for idx, x in df.iterrows()}, \"label\")\n    \n    nx.set_edge_attributes(G,{(x[\"in_node\"], mapping[idx]):x[\"amt\"] for idx, x in df.iterrows()}, \"weight\")\n    \n    nx.set_edge_attributes(G,{(x[\"out_node\"], mapping[idx]):x[\"amt\"] for idx, x in df.iterrows()}, \"weight\")\n    \n\n\n_G = nx.from_edgelist([[0,1],[0,2]])\n\n\nlen([(x[\"in_node\"], mapping[idx]) for idx, x in df.iterrows()] + [(x[\"out_node\"], mapping[idx]) for idx, x in df.iterrows()])\n\n24024\n\n\n\nlen(df50)\n\n12012\n\n\n\ndf[(df.is_fraud==1)&(df.cc_num==3.598900e+15)]\n\n\n\n\n\n\n\n\ntrans_date_trans_time\ncc_num\nmerchant\ncategory\namt\nfirst\nlast\ngender\nstreet\ncity\n...\ncity_pop\njob\ndob\ntrans_num\nunix_time\nmerch_lat\nmerch_long\nis_fraud\nin_node\nout_node\n\n\n\n\n686\n2019-11-25 22:26\n3.598900e+15\nfraud_Kilback LLC\ngrocery_pos\n320.26\nKenneth\nRichards\nM\n994 Livingston Extensions Suite 918\nNorwalk\n...\n23805\nCopy\n1970-02-22\nde391d925e1aa1f86cc77cf0e8f0e21b\n1353882366\n41.042223\n-82.308471\n1\n12668\n12614\n\n\n925\n2019-11-25 23:48\n3.598900e+15\nfraud_Kassulke PLC\nshopping_net\n948.05\nKenneth\nRichards\nM\n994 Livingston Extensions Suite 918\nNorwalk\n...\n23805\nCopy\n1970-02-22\n61df1730a076116730249af6e92c3ffb\n1353887312\n41.157798\n-82.108063\n1\n12668\n12241\n\n\n1352\n2019-11-25 3:38\n3.598900e+15\nfraud_Kuvalis Ltd\ngas_transport\n12.63\nKenneth\nRichards\nM\n994 Livingston Extensions Suite 918\nNorwalk\n...\n23805\nCopy\n1970-02-22\n4681eb64a8a2f95281afdcb7ddb43c4f\n1353814696\n41.082232\n-82.864767\n1\n12668\n12783\n\n\n3113\n2019-11-24 3:51\n3.598900e+15\nfraud_Koepp-Parker\ngrocery_pos\n308.82\nKenneth\nRichards\nM\n994 Livingston Extensions Suite 918\nNorwalk\n...\n23805\nCopy\n1970-02-22\n726d491f80a2c4db905fefe75290cdc8\n1353729072\n40.523117\n-82.073365\n1\n12668\n13061\n\n\n3219\n2019-11-25 22:41\n3.598900e+15\nfraud_Bechtelar-Rippin\nfood_dining\n127.81\nKenneth\nRichards\nM\n994 Livingston Extensions Suite 918\nNorwalk\n...\n23805\nCopy\n1970-02-22\n94f80338f56e539310f84c10c9c11fa8\n1353883286\n41.226756\n-83.475536\n1\n12668\n12392\n\n\n7647\n2019-11-25 22:44\n3.598900e+15\nfraud_Kihn, Abernathy and Douglas\nshopping_net\n968.41\nKenneth\nRichards\nM\n994 Livingston Extensions Suite 918\nNorwalk\n...\n23805\nCopy\n1970-02-22\n2a1a6053c916e39ed512338017bed155\n1353883476\n41.394215\n-82.910453\n1\n12668\n13072\n\n\n9546\n2019-11-25 22:21\n3.598900e+15\nfraud_Heathcote, Yost and Kertzmann\nshopping_net\n1074.33\nKenneth\nRichards\nM\n994 Livingston Extensions Suite 918\nNorwalk\n...\n23805\nCopy\n1970-02-22\n11ee1c8458d0d448d55de05c4c8a0fe9\n1353882077\n41.869650\n-82.354703\n1\n12668\n13303\n\n\n11881\n2019-11-24 23:35\n3.598900e+15\nfraud_Schaefer Ltd\nkids_pets\n16.53\nKenneth\nRichards\nM\n994 Livingston Extensions Suite 918\nNorwalk\n...\n23805\nCopy\n1970-02-22\n2a8b2e1497ff931108e47f5131ffbf5d\n1353800139\n41.431914\n-82.661167\n1\n12668\n12681\n\n\n\n\n8 rows × 24 columns\n\n\n\n\nimport matplotlib.pyplot as plt\n\n\nlen(df02.loc[(df02.cc_num==3.598900e+15),'amt'])\n\n95\n\n\n\nlen(df50.loc[(df02.cc_num==3.598900e+15),'amt'])\n\n3\n\n\n\nplt.plot(list(range(95)),df02.loc[(df02.cc_num==3.598900e+15),'amt'])\n\n\n\n\n\n_y= np.array(df02.loc[(df02.cc_num==3.598900e+15),'amt'])\n\n\ns=_y*0\n\n\nfor i in range(1,95):\n    s[i] = s[i-1] + _y[i]-np.mean(_y)\n\n\n_z=(_y-np.median(_y))/np.std(_y)\n\n\nplt.plot(_z)\n\n\n\n\n\n_t=(_y-np.mean(_y))/np.std(_y)\nplt.plot(_t)\n\n\n\n\n\nplt.plot(_z.cumsum())\n\n\n\n\n\nplt.plot(_t.cumsum())\n\n\n\n\n평균내면 -값 나와서.. 그래프가 이렇게\n\n_X,_y = embedding(Gtr)\n_XX,_yy = embedding(Gtest)\n\n\n\n\nGenerating walks (CPU: 1):   0%|          | 0/10 [00:00&lt;?, ?it/s]Generating walks (CPU: 1): 100%|██████████| 10/10 [00:07&lt;00:00,  1.39it/s]\nGenerating walks (CPU: 1):   0%|          | 0/10 [00:00&lt;?, ?it/s]Generating walks (CPU: 1): 100%|██████████| 10/10 [00:07&lt;00:00,  1.36it/s]\n\n\n\n\n\n\n\n\n\n\nprint(len(_X),len(_y), len(_XX),len(_yy))\n\n6006 6006 6006 6006\n\n\n\nprint(df50_tr.shape, len(_X), df50_test.shape, len(_XX))\n\n(9009, 22) 6006 (3003, 22) 6006\n\n\n\nX = np.array(df50_tr.amt).reshape(-1,1)\nXX = np.array(df50_test.amt).reshape(-1,1)\ny = np.array(df50_tr.is_fraud)\nyy = np.array(df50_test.is_fraud)\n\n\nnp.stack(_X).shape\n\n(6006, 128)\n\n\n\nX.shape, np.stack(_X).shape\n\n((9009, 1), (6006, 128))\n\n\n_X는 그래프 임베딩 한건데, 뭘까\n\nlen(_yy),len(_y)\n\n(6006, 6006)\n\n\n\nlrnr = sklearn.linear_model.LogisticRegression()\n\n\nlrnr.fit(_X,_y)\n\nLogisticRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LogisticRegressionLogisticRegression()\n\n\n\n_yyhat = lrnr.predict(_XX) \n\n\nmetrics = [sklearn.metrics.accuracy_score,\n           sklearn.metrics.precision_score,\n           sklearn.metrics.recall_score,\n           sklearn.metrics.f1_score]\n\n\n{m.__name__:m(_yy,_yyhat).round(6) for m in metrics}\n\n{'accuracy_score': 0.511655,\n 'precision_score': 0.577371,\n 'recall_score': 0.114673,\n 'f1_score': 0.191343}\n\n\n교수님것에서는 len(_X)= 2970 나왔는데\n\nG = build_graph_bipartite(df50)\n\n\ndf50.shape\n\n(12012, 22)\n\n\n\nX,XX,y,yy = embedding(G)\n\n\n\n\nGenerating walks (CPU: 1):   0%|          | 0/10 [00:00&lt;?, ?it/s]Generating walks (CPU: 1): 100%|██████████| 10/10 [00:04&lt;00:00,  2.16it/s]\n\n\n\n\n\nValueError: not enough values to unpack (expected 4, got 2)\n\n\n\nnp.stack(X).shape, np.stack(XX).shape\n\n((9009, 1), (3003, 1))\n\n\n\nnp.array(y).shape,np.array(yy).shape\n\n((9009,), (3003,))\n\n\n\nfrom sklearn.ensemble import RandomForestClassifier\n\n\nlrnr = RandomForestClassifier(n_estimators=100, random_state=42) \nlrnr.fit(np.stack(X),np.array(y))\n\nRandomForestClassifier(random_state=42)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.RandomForestClassifierRandomForestClassifier(random_state=42)\n\n\n\nnp.mean(lrnr.predict(XX) == yy)\n\n0.8608058608058609\n\n\n\nanal(down_sample_textbook(df))\n\n\n\n\nGenerating walks (CPU: 1):   0%|          | 0/10 [00:00&lt;?, ?it/s]Generating walks (CPU: 1): 100%|██████████| 10/10 [00:04&lt;00:00,  2.45it/s]\n\n\n\n\n\nValueError: not enough values to unpack (expected 4, got 2)\n\n\n\nanal(down_sample_textbook(our_sampling1(df)))"
  },
  {
    "objectID": "posts/사기거래/교수님 data(time12).html",
    "href": "posts/사기거래/교수님 data(time12).html",
    "title": "사기거래(설명변수 time추가)",
    "section": "",
    "text": "신용카드 거래 사기탐지 TRY1"
  },
  {
    "objectID": "posts/사기거래/교수님 data(time12).html#분석",
    "href": "posts/사기거래/교수님 data(time12).html#분석",
    "title": "사기거래(설명변수 time추가)",
    "section": "분석",
    "text": "분석\n\n분석1\n- step1: data\n\nX = np.array(df50_tr.loc[:,['amt']])\nXX = np.array(df50_test.loc[:,['amt']])\ny = np.array(df50_tr.is_fraud)\nyy = np.array(df50_test.is_fraud)\n\n- step2: lrnr생성\n\nlrnr = sklearn.linear_model.LogisticRegression()\n\n- step3: fit\n\nlrnr.fit(X,y)\n\nLogisticRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LogisticRegressionLogisticRegression()\n\n\n- step4: evaluate\n\nthresh = df50_tr.is_fraud.mean()\nyyhat = (lrnr.predict_proba(XX)&gt;thresh)[:,-1]\n\n\nmetrics = [sklearn.metrics.accuracy_score,\n           sklearn.metrics.precision_score,\n           sklearn.metrics.recall_score,\n           sklearn.metrics.f1_score]\n\n\n_results1 = pd.DataFrame({m.__name__:[m(yy,yyhat).round(6)] for m in metrics}, index=['분석1'])\n_results1\n\n\n\n\n\n\n\n\naccuracy_score\nprecision_score\nrecall_score\nf1_score\n\n\n\n\n분석1\n0.849484\n0.933279\n0.756098\n0.835397\n\n\n\n\n\n\n\n\n\n분석2\n- step1: data\n\nX = np.array(fraudTrain_tr.loc[:,['amt']])\nXX = np.array(df50_test.loc[:,['amt']])\ny = np.array(fraudTrain_tr.is_fraud)\nyy = np.array(df50_test.is_fraud)\n\n- step2: lrnr생성\n\nlrnr = sklearn.linear_model.LogisticRegression()\n\n- step3: fit\n\nlrnr.fit(X,y)\n\nLogisticRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LogisticRegressionLogisticRegression()\n\n\n\nthresh = fraudTrain_tr.is_fraud.mean()\nyyhat = (lrnr.predict_proba(XX)&gt;thresh)[:,-1]\n\n\n_results2 = pd.DataFrame({m.__name__:[m(yy,yyhat).round(6)] for m in metrics}, index=['분석2'])\n_results2\n\n\n\n\n\n\n\n\naccuracy_score\nprecision_score\nrecall_score\nf1_score\n\n\n\n\n분석2\n0.829171\n0.885561\n0.760053\n0.818021\n\n\n\n\n\n\n\n\n\n분석3\n- 함수\n\ndef amtano1(df_train):\n    df = df_train.copy()\n    df = df.assign(amtano=0)\n    normalize = lambda arr: (arr-np.median(arr))/np.std(arr) if np.std(arr)!=0 else arr*0\n    for cc_num, sub_df in df.groupby('cc_num'):\n        df.loc[df.cc_num == cc_num,['amtano']] = normalize(sub_df.amt).cumsum()\n        return df\n\n\ndef amtano2(dt_train, df_test):\n    df = pd.concat([df_train, df_test])\n    df_amtano = amtano_train(df)\n    return df_test.assign(amtano = df_amtano.loc[[i in df_test.index for i in df_amtano.index],'amtano'])\n\n\n# amtano2함수정확이 뭔지 헷갈려\n\n- step1: data\n\nX = np.array(amtano1(df50_tr).loc[:,['amt','amtano']])\nXX = np.array(amtano1(df50_test).loc[:,['amt','amtano']])\ny = np.array(df50_tr.is_fraud)\nyy = np.array(df50_test.is_fraud)\n\n- step2: lrnr생성\n\nlrnr = sklearn.linear_model.LogisticRegression()\n\n- step3: fit\n\nlrnr.fit(X,y)\n\nLogisticRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LogisticRegressionLogisticRegression()\n\n\n- step4: evaluate\n\nthresh = df50_tr.is_fraud.mean()\nyyhat = (lrnr.predict_proba(XX)&gt;thresh)[:,-1]\n\n\n_results3= pd.DataFrame({m.__name__:[m(yy,yyhat).round(6)] for m in metrics},index=['분석3'])\n_results3\n\n\n\n\n\n\n\n\naccuracy_score\nprecision_score\nrecall_score\nf1_score\n\n\n\n\n분석3\n0.849484\n0.933279\n0.756098\n0.835397\n\n\n\n\n\n\n\n\n\n분석4\n- step1: data\n\nX = np.array(amtano1(fraudTrain_tr).loc[:,['amt','amtano']])\nXX = np.array(amtano1(df50_test).loc[:,['amt','amtano']])\ny = np.array(fraudTrain_tr.is_fraud)\nyy = np.array(df50_test.is_fraud)\n\n- step2: lrnr\n\nlrnr = sklearn.linear_model.LogisticRegression()\n\n- step3: fit\n\nlrnr.fit(X,y)\n\nLogisticRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LogisticRegressionLogisticRegression()\n\n\n- step4: evaluate\n\nthresh = fraudTrain_tr.is_fraud.mean()\nyyhat = (lrnr.predict_proba(XX)&gt;thresh)[:,-1]\n\n\n_results4= pd.DataFrame({m.__name__:[m(yy,yyhat).round(6)] for m in metrics},index=['분석4'])\n_results4\n\n\n\n\n\n\n\n\naccuracy_score\nprecision_score\nrecall_score\nf1_score\n\n\n\n\n분석4\n0.828505\n0.884202\n0.760053\n0.817441\n\n\n\n\n\n\n\n\n\n분석5\n- step1: data\n\nGtr = build_graph_bipartite(df50_tr)\nGtest = build_graph_bipartite(df50_test)\nX,y = embedding(Gtr)\nXX, yy = embedding(Gtest)\n\n\n\n\nGenerating walks (CPU: 1): 100%|██████████| 10/10 [00:03&lt;00:00,  3.33it/s]\nGenerating walks (CPU: 1): 100%|██████████| 10/10 [00:02&lt;00:00,  3.38it/s]\n\n\n\n\n\n- step2: lrnr생성\n\nlrnr = sklearn.linear_model.LogisticRegression()\n\n- step3: fit\n\nlrnr.fit(X,y)\n\nLogisticRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LogisticRegressionLogisticRegression()\n\n\n- step4: evaluate\n\nyyhat = lrnr.predict(XX)\n\n\n_results5= pd.DataFrame({m.__name__:[m(yy,yyhat).round(6)] for m in metrics},index=['분석5'])\n_results5\n\n\n\n\n\n\n\n\naccuracy_score\nprecision_score\nrecall_score\nf1_score\n\n\n\n\n분석5\n0.509529\n0.50731\n0.988079\n0.670411\n\n\n\n\n\n\n\n\n\n분석6\n- step1: data\n\nX = np.array(amtano1(df50_tr).loc[:,['amt','amtano','timee']])\nXX = np.array(amtano1(df50_test).loc[:,['amt','amtano','timee']])\ny = np.array(df50_tr.is_fraud)\nyy = np.array(df50_test.is_fraud)\n\n- step2: lrnr생성\n\nlrnr = sklearn.linear_model.LogisticRegression()\n\n- step3: fit\n\nlrnr.fit(X,y)\n\nLogisticRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LogisticRegressionLogisticRegression()\n\n\n- step4: evaluate\n\nthresh = df50_tr.is_fraud.mean()\nyyhat = (lrnr.predict_proba(XX)&gt;thresh)[:,-1]\n\n\n_results6= pd.DataFrame({m.__name__:[m(yy,yyhat).round(6)] for m in metrics},index=['분석6'])\n_results6\n\n\n\n\n\n\n\n\naccuracy_score\nprecision_score\nrecall_score\nf1_score\n\n\n\n\n분석6\n0.834166\n0.911884\n0.743573\n0.819172\n\n\n\n\n\n\n\n\n\n분석7\n- step1: data\n\ny\n\narray([1, 1, 0, ..., 1, 1, 0])\n\n\n\nX = np.array(amtano1(fraudTrain_tr).loc[:,['amt','amtano','timee']])\nXX = np.array(amtano1(df50_test).loc[:,['amt','amtano','timee']])\ny = np.array(fraudTrain_tr.is_fraud)\nyy = np.array(df50_test.is_fraud)\n\n- step2: lrnr\n\nlrnr = sklearn.linear_model.LogisticRegression()\n\n- step3: fit\n\nlrnr.fit(X,y)\n\nLogisticRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LogisticRegressionLogisticRegression()\n\n\n- step4: evaluate\n\nthresh = fraudTrain_tr.is_fraud.mean()\nyyhat = (lrnr.predict_proba(XX)&gt;thresh)[:,-1]\n\n\n_results7= pd.DataFrame({m.__name__:[m(yy,yyhat).round(6)] for m in metrics},index=['분석7'])\n_results7\n\n\n\n\n\n\n\n\naccuracy_score\nprecision_score\nrecall_score\nf1_score\n\n\n\n\n분석7\n0.707293\n0.648787\n0.916941\n0.759902"
  },
  {
    "objectID": "posts/사기거래/교수님 data(time12).html#분석-정리",
    "href": "posts/사기거래/교수님 data(time12).html#분석-정리",
    "title": "사기거래(설명변수 time추가)",
    "section": "분석 정리",
    "text": "분석 정리\n\n\n\n\nTrain\nTest\n모형\n설명변수\n그래프임베딩\n\n\n\n\n분석1\ndf50train\ndf50test\n로지스틱\namt\nX\n\n\n분석2\ndf02train\ndf50test\n로지스틱\namt\nX\n\n\n분석3\ndf50train\ndf50test\n로지스틱\namt,amtano\nX\n\n\n분석4\ndf02train\ndf50test\n로지스틱\namt,amtano\nX\n\n\n분석6\ndf50train\ndf50test\n로지스틱\namt,amtano,timee\nX\n\n\n분석7\ndf02train\ndf50test\n로지스틱\namt,amtano,timee\nX\n\n\n\n\npd.concat([_results1,_results2,_results3,_results4,_results5,_results6,_results7])\n\n\n\n\n\n\n\n\naccuracy_score\nprecision_score\nrecall_score\nf1_score\n\n\n\n\n분석1\n0.849484\n0.933279\n0.756098\n0.835397\n\n\n분석2\n0.829171\n0.885561\n0.760053\n0.818021\n\n\n분석3\n0.849484\n0.933279\n0.756098\n0.835397\n\n\n분석4\n0.828505\n0.884202\n0.760053\n0.817441\n\n\n분석5\n0.509529\n0.507310\n0.988079\n0.670411\n\n\n분석6\n0.834166\n0.911884\n0.743573\n0.819172\n\n\n분석7\n0.707293\n0.648787\n0.916941\n0.759902"
  },
  {
    "objectID": "posts/사기거래/교수님 data(time12).html#분석-1",
    "href": "posts/사기거래/교수님 data(time12).html#분석-1",
    "title": "사기거래(설명변수 time추가)",
    "section": "분석",
    "text": "분석\n\n분석1\n- step1: data\n\nX = np.array(df50_tr.loc[:,['amt']])\nXX = np.array(df50_test.loc[:,['amt']])\ny = np.array(df50_tr.is_fraud)\nyy = np.array(df50_test.is_fraud)\n\n- step2: lrnr생성\n\nlrnr = sklearn.linear_model.LogisticRegression()\n\n- step3: fit\n\nlrnr.fit(X,y)\n\nLogisticRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LogisticRegressionLogisticRegression()\n\n\n- step4: evaluate\n\nyyhat = lrnr.predict(XX)\n\n\n_results1= pd.DataFrame({m.__name__:[m(yy,yyhat).round(6)] for m in metrics},index=['분석1'])\n_results1\n\n\n\n\n\n\n\n\naccuracy_score\nprecision_score\nrecall_score\nf1_score\n\n\n\n\n분석1\n0.849484\n0.933279\n0.756098\n0.835397\n\n\n\n\n\n\n\n\n\n분석2\n- step1: data\n\nX = np.array(df50_tr.loc[:,['amt']])\nXX = np.array(df50_test.loc[:,['amt']])\ny = np.array(df50_tr.is_fraud)\nyy = np.array(df50_test.is_fraud)\n\n- step2: lrnr생성\n\nlrnr = ensemble.GradientBoostingClassifier()\n\n- step3: fit\n\nlrnr.fit(X,y)\n\nGradientBoostingClassifier()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.GradientBoostingClassifierGradientBoostingClassifier()\n\n\n- step4: evaluate\n\nyyhat = lrnr.predict(XX)\n\n\n_results2= pd.DataFrame({m.__name__:[m(yy,yyhat).round(6)] for m in metrics},index=['분석2'])\n_results2\n\n\n\n\n\n\n\n\naccuracy_score\nprecision_score\nrecall_score\nf1_score\n\n\n\n\n분석2\n0.885781\n0.908206\n0.86091\n0.883926\n\n\n\n\n\n\n\n\n\n분석3\n- step1: data\n\nX = np.array(amtano1(df50_tr).loc[:,['amt','amtano']])\nXX = np.array(amtano1(df50_test).loc[:,['amt','amtano']])\ny = np.array(df50_tr.is_fraud)\nyy = np.array(df50_test.is_fraud)\n\n- step2: lrnr생성\n\nlrnr = ensemble.GradientBoostingClassifier()\n\n- step3: fit\n\nlrnr.fit(X,y)\n\nGradientBoostingClassifier()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.GradientBoostingClassifierGradientBoostingClassifier()\n\n\n- step4: evaluate\n\nyyhat = lrnr.predict(XX)\n\n\n_results3= pd.DataFrame({m.__name__:[m(yy,yyhat).round(6)] for m in metrics},index=['분석3'])\n_results3\n\n\n\n\n\n\n\n\naccuracy_score\nprecision_score\nrecall_score\nf1_score\n\n\n\n\n분석3\n0.887113\n0.90846\n0.863546\n0.885434\n\n\n\n\n\n\n\n\n\n분석4\n- step1: data\n\nX = np.array(amtano1(df02_tr).loc[:,['amt','amtano']])\nXX = np.array(amtano1(df50_test).loc[:,['amt','amtano']])\ny = np.array(df02_tr.is_fraud)\nyy = np.array(df50_test.is_fraud)\n\n- step2: lrnr생성\n\nlrnr = ensemble.GradientBoostingClassifier()\n\n- step3: fit\n\nlrnr.fit(X,y)\n\nGradientBoostingClassifier()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.GradientBoostingClassifierGradientBoostingClassifier()\n\n\n- step4: evaluate\n\nthresh = y.mean()\nyyhat = (lrnr.predict_proba(XX)&gt; thresh)[:,-1]\n\n\n_results4= pd.DataFrame({m.__name__:[m(yy,yyhat).round(6)] for m in metrics},index=['분석4'])\n_results4\n\n\n\n\n\n\n\n\naccuracy_score\nprecision_score\nrecall_score\nf1_score\n\n\n\n\n분석4\n0.88678\n0.910677\n0.86025\n0.884746\n\n\n\n\n\n\n\n\n\n분석5\n- step1: data\n\nX = np.array(amtano1(fraudTrain_tr).loc[:,['amt','amtano']])\nXX = np.array(amtano1(df50_test).loc[:,['amt','amtano']])\ny = np.array(fraudTrain_tr.is_fraud)\nyy = np.array(df50_test.is_fraud)\n\n- step2: lrnr생성\n\nlrnr = ensemble.GradientBoostingClassifier()\n\n- step3: fit\n\nlrnr.fit(X,y)\n\nGradientBoostingClassifier()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.GradientBoostingClassifierGradientBoostingClassifier()\n\n\n- step4: evaluate\n\nthresh = y.mean()\nyyhat = (lrnr.predict_proba(XX)&gt; thresh)[:,-1]\n\n\n_results5= pd.DataFrame({m.__name__:[m(yy,yyhat).round(6)] for m in metrics},index=['분석5'])\n_results5\n\n\n\n\n\n\n\n\naccuracy_score\nprecision_score\nrecall_score\nf1_score\n\n\n\n\n분석5\n0.874792\n0.878065\n0.873434\n0.875744\n\n\n\n\n\n\n\n\n\n분석6 timee\n- step1: data\n\nX = np.array(amtano1(df50_tr).loc[:,['amt','amtano','timee']])\nXX = np.array(amtano1(df50_test).loc[:,['amt','amtano','timee']])\ny = np.array(df50_tr.is_fraud)\nyy = np.array(df50_test.is_fraud)\n\n- step2: lrnr생성\n\nlrnr = sklearn.linear_model.LogisticRegression()\n\n- step3: fit\n\nlrnr.fit(X,y)\n\nLogisticRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LogisticRegressionLogisticRegression()\n\n\n- step4: evaluate\n\nthresh = df50_tr.is_fraud.mean()\nyyhat = (lrnr.predict_proba(XX)&gt;thresh)[:,-1]\n\n\n_results6= pd.DataFrame({m.__name__:[m(yy,yyhat).round(6)] for m in metrics},index=['분석6'])\n_results6\n\n\n\n\n\n\n\n\naccuracy_score\nprecision_score\nrecall_score\nf1_score\n\n\n\n\n분석6\n0.834166\n0.911884\n0.743573\n0.819172\n\n\n\n\n\n\n\n\n\n분석 7\n- step1: data\n\nX = np.array(amtano1(df02_tr).loc[:,['amt','amtano','timee']])\nXX = np.array(amtano1(df50_test).loc[:,['amt','amtano','timee']])\ny = np.array(df02_tr.is_fraud)\nyy = np.array(df50_test.is_fraud)\n\n- step2: lrnr생성\n\nlrnr = ensemble.GradientBoostingClassifier()\n\n- step3: fit\n\nlrnr.fit(X,y)\n\nGradientBoostingClassifier()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.GradientBoostingClassifierGradientBoostingClassifier()\n\n\n- step4: evaluate\n\nthresh = y.mean()\nyyhat = (lrnr.predict_proba(XX)&gt; thresh)[:,-1]\n\n\n_results7= pd.DataFrame({m.__name__:[m(yy,yyhat).round(6)] for m in metrics},index=['분석7'])\n_results7\n\n\n\n\n\n\n\n\naccuracy_score\nprecision_score\nrecall_score\nf1_score\n\n\n\n\n분석7\n0.899434\n0.897839\n0.903757\n0.900788\n\n\n\n\n\n\n\n\n\n분석8\n- step1: data\n\nX = np.array(amtano1(fraudTrain_tr).loc[:,['amt','amtano','timee']])\nXX = np.array(amtano1(df50_test).loc[:,['amt','amtano','timee']])\ny = np.array(fraudTrain_tr.is_fraud)\nyy = np.array(df50_test.is_fraud)\n\n- step2: lrnr생성\n\nlrnr = ensemble.GradientBoostingClassifier()\n\n- step3: fit\n\nlrnr.fit(X,y)\n\nGradientBoostingClassifier()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.GradientBoostingClassifierGradientBoostingClassifier()\n\n\n- step4: evaluate\n\nthresh = y.mean()\nyyhat = (lrnr.predict_proba(XX)&gt; thresh)[:,-1]\n\n\n_results8= pd.DataFrame({m.__name__:[m(yy,yyhat).round(6)] for m in metrics},index=['분석8'])\n_results8\n\n\n\n\n\n\n\n\naccuracy_score\nprecision_score\nrecall_score\nf1_score\n\n\n\n\n분석8\n0.89344\n0.892459\n0.897165\n0.894806"
  },
  {
    "objectID": "posts/사기거래/교수님 data(time12).html#분석정리",
    "href": "posts/사기거래/교수님 data(time12).html#분석정리",
    "title": "사기거래(설명변수 time추가)",
    "section": "분석정리",
    "text": "분석정리\n\n\n\n\nTrain\nTest\n모형\n설명변수\n비고\n\n\n\n\n분석1\ndf50train\ndf50test\n로지스틱\namt\n\n\n\n분석2\ndf50train\ndf50test\n그레디언트부스팅\namt\nbase\n\n\n분석3\ndf50train\ndf50test\n그레디언트부스팅\namt, amtano\n\n\n\n분석4\ndf02train\ndf50test\n그레디언트부스팅\namt, amtano\n\n\n\n분석5\nfraudTrain_tr\ndf50test\n그레디언트부스팅\namt, amtano\n\n\n\n분석6\ndf50train\ndf50test\n그레디언트부스팅\namt, amtano, timee\n\n\n\n분석7\ndf02train\ndf50test\n그레디언트부스팅\namt, amtano, timee\n\n\n\n\n\nlst = [_results1,_results2,_results3,_results4,_results5,_results6,_results7,_results8]\npd.concat(lst)\n\n\n\n\n\n\n\n\naccuracy_score\nprecision_score\nrecall_score\nf1_score\n\n\n\n\n분석1\n0.849484\n0.933279\n0.756098\n0.835397\n\n\n분석2\n0.885781\n0.908206\n0.860910\n0.883926\n\n\n분석3\n0.887113\n0.908460\n0.863546\n0.885434\n\n\n분석4\n0.886780\n0.910677\n0.860250\n0.884746\n\n\n분석5\n0.874792\n0.878065\n0.873434\n0.875744\n\n\n분석6\n0.834166\n0.911884\n0.743573\n0.819172\n\n\n분석7\n0.899434\n0.897839\n0.903757\n0.900788\n\n\n분석8\n0.893440\n0.892459\n0.897165\n0.894806"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "끄적끄적 coco 올리기 전 아무거나 막 쓰는 용도"
  }
]