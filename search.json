[
  {
    "objectID": "posts/VAE.html",
    "href": "posts/VAE.html",
    "title": "VAE(-ing)",
    "section": "",
    "text": "https://arxiv.org/pdf/1312.6114.pdf"
  },
  {
    "objectID": "posts/VAE.html#오토인코더",
    "href": "posts/VAE.html#오토인코더",
    "title": "VAE(-ing)",
    "section": "오토인코더",
    "text": "오토인코더\n\nEncoder, Decoder 네트워크로 구성된 모델\n학습 데이터-> encoder에 입력값"
  },
  {
    "objectID": "posts/plot_comparison_under_sampling.html",
    "href": "posts/plot_comparison_under_sampling.html",
    "title": "Scribbling",
    "section": "",
    "text": "%matplotlib inline"
  },
  {
    "objectID": "posts/plot_comparison_under_sampling.html#prototype-generation-under-sampling-by-generating-new-samples",
    "href": "posts/plot_comparison_under_sampling.html#prototype-generation-under-sampling-by-generating-new-samples",
    "title": "Scribbling",
    "section": "Prototype generation: under-sampling by generating new samples",
    "text": "Prototype generation: under-sampling by generating new samples\n:class:~imblearn.under_sampling.ClusterCentroids under-samples by replacing the original samples by the centroids of the cluster found.\n\nimport matplotlib.pyplot as plt\nfrom sklearn.cluster import MiniBatchKMeans\n\nfrom imblearn import FunctionSampler\nfrom imblearn.pipeline import make_pipeline\nfrom imblearn.under_sampling import ClusterCentroids\n\nX, y = create_dataset(n_samples=400, weights=(0.05, 0.15, 0.8), class_sep=0.8)\n\nsamplers = {\n    FunctionSampler(),  # identity resampler\n    ClusterCentroids(\n        estimator=MiniBatchKMeans(n_init=1, random_state=0), random_state=0\n    ),\n}\n\nfig, axs = plt.subplots(nrows=2, ncols=2, figsize=(15, 15))\nfor ax, sampler in zip(axs, samplers):\n    model = make_pipeline(sampler, clf).fit(X, y)\n    plot_decision_function(\n        X, y, model, ax[0], title=f\"Decision function with {sampler.__class__.__name__}\"\n    )\n    plot_resampling(X, y, sampler, ax[1])\n\nfig.tight_layout()"
  },
  {
    "objectID": "posts/plot_comparison_under_sampling.html#prototype-selection-under-sampling-by-selecting-existing-samples",
    "href": "posts/plot_comparison_under_sampling.html#prototype-selection-under-sampling-by-selecting-existing-samples",
    "title": "Scribbling",
    "section": "Prototype selection: under-sampling by selecting existing samples",
    "text": "Prototype selection: under-sampling by selecting existing samples\nThe algorithm performing prototype selection can be subdivided into two groups: (i) the controlled under-sampling methods and (ii) the cleaning under-sampling methods.\nWith the controlled under-sampling methods, the number of samples to be selected can be specified. :class:~imblearn.under_sampling.RandomUnderSampler is the most naive way of performing such selection by randomly selecting a given number of samples by the targetted class.\n\nfrom imblearn.under_sampling import RandomUnderSampler\n\nX, y = create_dataset(n_samples=400, weights=(0.05, 0.15, 0.8), class_sep=0.8)\n\nsamplers = {\n    FunctionSampler(),  # identity resampler\n    RandomUnderSampler(random_state=0),\n}\n\nfig, axs = plt.subplots(nrows=2, ncols=2, figsize=(15, 15))\nfor ax, sampler in zip(axs, samplers):\n    model = make_pipeline(sampler, clf).fit(X, y)\n    plot_decision_function(\n        X, y, model, ax[0], title=f\"Decision function with {sampler.__class__.__name__}\"\n    )\n    plot_resampling(X, y, sampler, ax[1])\n\nfig.tight_layout()\n\n\n\n\n:class:~imblearn.under_sampling.NearMiss algorithms implement some heuristic rules in order to select samples. NearMiss-1 selects samples from the majority class for which the average distance of the \\(k\\)` nearest samples of the minority class is the smallest. NearMiss-2 selects the samples from the majority class for which the average distance to the farthest samples of the negative class is the smallest. NearMiss-3 is a 2-step algorithm: first, for each minority sample, their \\(m\\) nearest-neighbors will be kept; then, the majority samples selected are the on for which the average distance to the \\(k\\) nearest neighbors is the largest.\n\nfrom imblearn.under_sampling import NearMiss\n\nX, y = create_dataset(n_samples=1000, weights=(0.05, 0.15, 0.8), class_sep=1.5)\n\nsamplers = [NearMiss(version=1), NearMiss(version=2), NearMiss(version=3)]\n\nfig, axs = plt.subplots(nrows=3, ncols=2, figsize=(15, 25))\nfor ax, sampler in zip(axs, samplers):\n    model = make_pipeline(sampler, clf).fit(X, y)\n    plot_decision_function(\n        X,\n        y,\n        model,\n        ax[0],\n        title=f\"Decision function for {sampler.__class__.__name__}-{sampler.version}\",\n    )\n    plot_resampling(\n        X,\n        y,\n        sampler,\n        ax[1],\n        title=f\"Resampling using {sampler.__class__.__name__}-{sampler.version}\",\n    )\nfig.tight_layout()\n\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/imblearn/under_sampling/_prototype_selection/_nearmiss.py:203: UserWarning: The number of the samples to be selected is larger than the number of samples available. The balancing ratio cannot be ensure and all samples will be returned.\n  warnings.warn(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/imblearn/under_sampling/_prototype_selection/_nearmiss.py:203: UserWarning: The number of the samples to be selected is larger than the number of samples available. The balancing ratio cannot be ensure and all samples will be returned.\n  warnings.warn(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/imblearn/under_sampling/_prototype_selection/_nearmiss.py:203: UserWarning: The number of the samples to be selected is larger than the number of samples available. The balancing ratio cannot be ensure and all samples will be returned.\n  warnings.warn(\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/imblearn/under_sampling/_prototype_selection/_nearmiss.py:203: UserWarning: The number of the samples to be selected is larger than the number of samples available. The balancing ratio cannot be ensure and all samples will be returned.\n  warnings.warn(\n\n\n\n\n\n:class:~imblearn.under_sampling.EditedNearestNeighbours removes samples of the majority class for which their class differ from the one of their nearest-neighbors. This sieve can be repeated which is the principle of the :class:~imblearn.under_sampling.RepeatedEditedNearestNeighbours. :class:~imblearn.under_sampling.AllKNN is slightly different from the :class:~imblearn.under_sampling.RepeatedEditedNearestNeighbours by changing the \\(k\\) parameter of the internal nearest neighors algorithm, increasing it at each iteration.\n\nfrom imblearn.under_sampling import (\n    AllKNN,\n    EditedNearestNeighbours,\n    RepeatedEditedNearestNeighbours,\n)\n\nX, y = create_dataset(n_samples=500, weights=(0.2, 0.3, 0.5), class_sep=0.8)\n\nsamplers = [\n    EditedNearestNeighbours(),\n    RepeatedEditedNearestNeighbours(),\n    AllKNN(allow_minority=True),\n]\n\nfig, axs = plt.subplots(3, 2, figsize=(15, 25))\nfor ax, sampler in zip(axs, samplers):\n    model = make_pipeline(sampler, clf).fit(X, y)\n    plot_decision_function(\n        X, y, clf, ax[0], title=f\"Decision function for \\n{sampler.__class__.__name__}\"\n    )\n    plot_resampling(\n        X, y, sampler, ax[1], title=f\"Resampling using \\n{sampler.__class__.__name__}\"\n    )\n\nfig.tight_layout()\n\n\n\n\n:class:~imblearn.under_sampling.CondensedNearestNeighbour makes use of a 1-NN to iteratively decide if a sample should be kept in a dataset or not. The issue is that :class:~imblearn.under_sampling.CondensedNearestNeighbour is sensitive to noise by preserving the noisy samples. :class:~imblearn.under_sampling.OneSidedSelection also used the 1-NN and use :class:~imblearn.under_sampling.TomekLinks to remove the samples considered noisy. The :class:~imblearn.under_sampling.NeighbourhoodCleaningRule use a :class:~imblearn.under_sampling.EditedNearestNeighbours to remove some sample. Additionally, they use a 3 nearest-neighbors to remove samples which do not agree with this rule.\n\nfrom imblearn.under_sampling import (\n    CondensedNearestNeighbour,\n    NeighbourhoodCleaningRule,\n    OneSidedSelection,\n)\n\nX, y = create_dataset(n_samples=500, weights=(0.2, 0.3, 0.5), class_sep=0.8)\n\nfig, axs = plt.subplots(nrows=3, ncols=2, figsize=(15, 25))\n\nsamplers = [\n    CondensedNearestNeighbour(random_state=0),\n    OneSidedSelection(random_state=0),\n    NeighbourhoodCleaningRule(),\n]\n\nfor ax, sampler in zip(axs, samplers):\n    model = make_pipeline(sampler, clf).fit(X, y)\n    plot_decision_function(\n        X, y, clf, ax[0], title=f\"Decision function for \\n{sampler.__class__.__name__}\"\n    )\n    plot_resampling(\n        X, y, sampler, ax[1], title=f\"Resampling using \\n{sampler.__class__.__name__}\"\n    )\nfig.tight_layout()\n\n\n\n\n:class:~imblearn.under_sampling.InstanceHardnessThreshold uses the prediction of classifier to exclude samples. All samples which are classified with a low probability will be removed.\n\nfrom imblearn.under_sampling import InstanceHardnessThreshold\n\nsamplers = {\n    FunctionSampler(),  # identity resampler\n    InstanceHardnessThreshold(\n        estimator=LogisticRegression(),\n        random_state=0,\n    ),\n}\n\nfig, axs = plt.subplots(nrows=2, ncols=2, figsize=(15, 15))\nfor ax, sampler in zip(axs, samplers):\n    model = make_pipeline(sampler, clf).fit(X, y)\n    plot_decision_function(\n        X,\n        y,\n        model,\n        ax[0],\n        title=f\"Decision function with \\n{sampler.__class__.__name__}\",\n    )\n    plot_resampling(\n        X, y, sampler, ax[1], title=f\"Resampling using \\n{sampler.__class__.__name__}\"\n    )\n\nfig.tight_layout()\nplt.show()"
  },
  {
    "objectID": "posts/콰트로 블로그 만드는 법.html",
    "href": "posts/콰트로 블로그 만드는 법.html",
    "title": "콰트로 블로그",
    "section": "",
    "text": "git에서 Repositories 생성\n\n\n생성시 Read.Me 체크\n\n\n만들어진 레퍼토리의 HTTPS주소 복사\n\n3.명령 프롬프트에서 Dropbox로 들어가기\ncd Dropbox\n\ngit clone “2번복사한주소”\n콰트로 블로그 설정파일 자동 생성\n\nquarto create-project --type website:blog\n\n유저등록 (최초 한번)\n\ngit config --global user.email \"내 깃허브 이메일\"\n\ngit config --global user.name \"내 깃허브 닉네임\"\n\ngit config credential.helper store  #(아이디와 비번 저장)\n\n\n\ngit add .\ngit commit -m .\ngit push   (실행시 id/password작성)\n\npublish\n\nquarto publish gh-pages --no-prompt --no-bropwer"
  },
  {
    "objectID": "posts/graph4-3.html",
    "href": "posts/graph4-3.html",
    "title": "CH4. 지도 그래프 학습(그래프 정규화 방법)",
    "section": "",
    "text": "그래프 머신러닝\ngithub"
  },
  {
    "objectID": "posts/graph4-3.html#load-dataset",
    "href": "posts/graph4-3.html#load-dataset",
    "title": "CH4. 지도 그래프 학습(그래프 정규화 방법)",
    "section": "Load Dataset",
    "text": "Load Dataset\n- 데이터셋: Cora\n\n7개의 클래스로 라벨링돼 있는 2,708개의 컴퓨터 사이언스 논문\n각 논문은 인용을 기반으로 다른 노드와 연결된 노드\n총 5,429개의 간선\n\n\nfrom stellargraph import datasets\n\n2023-04-06 21:44:50.486139: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n\n\n\ndataset = datasets.Cora()\n\n\n%config Completer.use_jedi = False\n\n\ndataset.download()\n\n\nlabel_index = {\n      'Case_Based': 0,\n      'Genetic_Algorithms': 1,\n      'Neural_Networks': 2,\n      'Probabilistic_Methods': 3,\n      'Reinforcement_Learning': 4,\n      'Rule_Learning': 5,\n      'Theory': 6,\n  }\n\n\nG, labels = dataset.load()\n\n\nG: 네트워크 노드, 간선, BOW표현 설명\nlabea : 논문id와 클래스 중 하나 사이의 매핑\n훈련 샘플: 이웃과 관련된 정보가 포함 -> 훈련을 정규화 하는데 사용\n검증 샘플: 이웃과 관련된 정보 불포함 , 예측된 라벨은 노드 특증, bow표현에만 의존\n\n\nimport numpy as np\nfrom sklearn import preprocessing, feature_extraction, model_selection\n\n\nimport tensorflow as tf\nfrom tensorflow.train import Example, Features, Feature, Int64List, BytesList, FloatList\n\n\nGRAPH_PREFIX=\"NL_nbr\"\n\n\ndef _int64_feature(*value):\n    \"\"\"Returns int64 tf.train.Feature from a bool / enum / int / uint.\"\"\"\n    return Feature(int64_list=Int64List(value=list(value)))\n\ndef _bytes_feature(value):\n    \"\"\"Returns bytes tf.train.Feature from a string.\"\"\"\n    return Feature(\n        bytes_list=BytesList(value=[value.encode('utf-8')])\n    )\n\ndef _float_feature(*value):\n    return Feature(float_list=FloatList(value=list(value)))\n\n\n_int64_feature 함수는 bool, enum, int, uint 데이터 타입을 입력 받아 int64_list 타입의 tf.train.Feature 객체를 반환\n_bytes_feature 함수는 문자열 값을 입력 받아 utf-8로 인코딩하여 bytes_list 타입의 tf.train.Feature 객체를 반환\n_float_feature 함수는 float 데이터 타입을 입력 받아 float_list 타입의 tf.train.Feature 객체를 반환\n\n- 반지도 학습 데이터 셋 만드는 함수 정의\n\nfrom functools import reduce\nfrom typing import List, Tuple\nimport pandas as pd\nimport six\n\ndef addFeatures(x, y):\n    res = Features()\n    res.CopyFrom(x)\n    res.MergeFrom(y)\n    return res\n\ndef neighborFeatures(features: Features, weight: float, prefix: str):  # 객체, 가중치, 접두어 입력으로 받음\n    data = {f\"{prefix}_weight\": _float_feature(weight)}\n    for name, feature in six.iteritems(features.feature):\n        data[f\"{prefix}_{name}\"] = feature \n    return Features(feature=data)\n\ndef neighborsFeatures(neighbors: List[Tuple[Features, float]]):\n    return reduce(\n        addFeatures, \n        [neighborFeatures(sample, weight, f\"{GRAPH_PREFIX}_{ith}\") for ith, (sample, weight) in enumerate(neighbors)],\n        Features()\n    )\n\ndef getNeighbors(idx, adjMatrix, topn=5): #인덱스와 인접행렬 이용하여 이웃 데이터셋 추출 \n    weights = adjMatrix.loc[idx]\n    return weights[weights>0].sort_values(ascending=False).head(topn).to_dict()\n    \n\ndef semisupervisedDataset(G, labels, ratio=0.2, topn=5):  #라벨이 있는 데이터와 없는 데이터 추출\n     #ratio:라벨 유무 비율 설정\n     #topn: 함수에서 추출할 이웃 데이터셋 크기 설정\n    n = int(np.round(len(labels)*ratio)) \n    \n    labelled, unlabelled = model_selection.train_test_split(\n        labels, train_size=n, test_size=None, stratify=labels\n    )\n\n\n1. 노드 특징 df로 구성하고 그래프 인접행렬로 저장\n\nadjMatrix = pd.DataFrame.sparse.from_spmatrix(G.to_adjacency_matrix(), index=G.nodes(), columns=G.nodes())\n    \nfeatures = pd.DataFrame(G.node_features(), index=G.nodes())\n\n\n\n2. adjMatrix사용해 노드ID와 간선 가중치 반환하여 노드의 가장 가까운 TOPN이웃 검색하는 도우미 함수 구현\n\ndef getNeighbors(idx, adjMatrix, topn=5): #인덱스와 인접행렬 이용하여 이웃 데이터셋 추출 \n    weights = adjMatrix.loc[idx]\n    neighbors = weights[weights>0]\\\n        .sort_values(ascending=False)\\\n        .head(topn)\n    return [(k,v) for k, v in neighbors.iteritems()]\n    \n\n\n3. 정보를 단일 df로 병합\n\ndataset = {\n        index: Features(feature = {\n            #\"id\": _bytes_feature(str(index)), \n            \"id\": _int64_feature(index),\n            \"words\": _float_feature(*[float(x) for x in features.loc[index].values]), \n            \"label\": _int64_feature(label_index[label])\n        })\n        for index, label in pd.concat([labelled, unlabelled]).items()\n    }\n\nNameError: name 'labelled' is not defined\n\n\n\nfrom functools import reduce\nfrom typing import List, Tuple\nimport pandas as pd\nimport six\n\ndef addFeatures(x, y):\n    res = Features()\n    res.CopyFrom(x)\n    res.MergeFrom(y)\n    return res\n\ndef neighborFeatures(features: Features, weight: float, prefix: str):\n    data = {f\"{prefix}_weight\": _float_feature(weight)}\n    for name, feature in six.iteritems(features.feature):\n        data[f\"{prefix}_{name}\"] = feature \n    return Features(feature=data)\n\ndef neighborsFeatures(neighbors: List[Tuple[Features, float]]):\n    return reduce(\n        addFeatures, \n        [neighborFeatures(sample, weight, f\"{GRAPH_PREFIX}_{ith}\") for ith, (sample, weight) in enumerate(neighbors)],\n        Features()\n    )\n\ndef getNeighbors(idx, adjMatrix, topn=5):\n    weights = adjMatrix.loc[idx]\n    return weights[weights>0].sort_values(ascending=False).head(topn).to_dict()\n    \n\ndef semisupervisedDataset(G, labels, ratio=0.2, topn=5):\n    n = int(np.round(len(labels)*ratio))\n    \n    labelled, unlabelled = model_selection.train_test_split(\n        labels, train_size=n, test_size=None, stratify=labels\n    )\n    \n    adjMatrix = pd.DataFrame.sparse.from_spmatrix(G.to_adjacency_matrix(), index=G.nodes(), columns=G.nodes())\n    \n    features = pd.DataFrame(G.node_features(), index=G.nodes())\n    \n    dataset = {\n        index: Features(feature = {\n            #\"id\": _bytes_feature(str(index)), \n            \"id\": _int64_feature(index),\n            \"words\": _float_feature(*[float(x) for x in features.loc[index].values]), \n            \"label\": _int64_feature(label_index[label])\n        })\n        for index, label in pd.concat([labelled, unlabelled]).items()\n    }\n    \n    trainingSet = [\n        Example(features=addFeatures(\n            dataset[exampleId], \n            neighborsFeatures(\n                [(dataset[nodeId], weight) for nodeId, weight in getNeighbors(exampleId, adjMatrix, topn).items()]\n            )\n        ))\n        for exampleId in labelled.index\n    ]\n    \n    testSet = [Example(features=dataset[exampleId]) for exampleId in unlabelled.index]\n\n    serializer = lambda _list: [e.SerializeToString() for e in _list]\n    \n    return serializer(trainingSet), serializer(testSet)"
  },
  {
    "objectID": "posts/학회.html",
    "href": "posts/학회.html",
    "title": "Scribbling",
    "section": "",
    "text": "일정:23. 6.29(목) ~ 7.1 (토)\n장소: 부경대학교(부산)\n발표신청 및 초록제출: 3.20.(월) ~ 4.20.(목)\n발표요약본제출(석사과정) : ~4.20.(목)\n포스터파일제출: ~ 5.19.(금)\n\n\n\n\n\n하계: 2023. 7.6.(목) ~ 7.7.(금)\n고려대학교\n발표신청 및 사전등록: 23.5.29.(월)\n초록 또는 논문제출: ~5.31.(수)\n\n\n\n\n\n일정: 6.23.(금) ~ 6.24.(토)\n장소: 강릉원주대학교\n발표논문 초록 제출: ~5.5.(금)\n발표 논문 제출: ~6.16.(금)\n학생논문: ~6.9.(금)"
  },
  {
    "objectID": "posts/AI.html",
    "href": "posts/AI.html",
    "title": "Scribbling",
    "section": "",
    "text": "ALEKS(알렉스)\n아틀라스(아주대)"
  },
  {
    "objectID": "posts/graph8(frac=0.4).html",
    "href": "posts/graph8(frac=0.4).html",
    "title": "CH8. 신용카드 거래에 대한 그래프 분석(frac=0.4)",
    "section": "",
    "text": "그래프 머신러닝\ngithub\nCredit Card Transactions Fraud Detection Dataset\n컬리이미지\nnetworkx"
  },
  {
    "objectID": "posts/graph8(frac=0.4).html#사기-탐지를-위한-지도-및-비지도-임베딩",
    "href": "posts/graph8(frac=0.4).html#사기-탐지를-위한-지도-및-비지도-임베딩",
    "title": "CH8. 신용카드 거래에 대한 그래프 분석(frac=0.4)",
    "section": "사기 탐지를 위한 지도 및 비지도 임베딩",
    "text": "사기 탐지를 위한 지도 및 비지도 임베딩\n\n트랜잭션 간선으로 표기\n각 간선을 올바른 클래스(사기 또는 정상)으로 분류\n\n\n지도학습\n\nfrom sklearn.utils import resample\n\ndf_majority = df[df.is_fraud==0]\ndf_minority = df[df.is_fraud==1]\n\ndf_maj_dowsampled = resample(df_majority,\n                             n_samples=len(df_minority),\n                             random_state=42)\n\ndf_downsampled = pd.concat([df_minority, df_maj_dowsampled])\n\nprint(df_downsampled.is_fraud.value_counts())\nG_down = build_graph_bipartite(df_downsampled)\n\n1    6006\n0    6006\nName: is_fraud, dtype: int64\n\n\n\n무작위 언더샘플링 사용\n소수 클래스(사기거래)이 샘플 수 와 일치시키려고 다수 클래스(정상거래)의 하위 샘플을 가져옴\n데이터 불균형을 처리하기 위해서\n\n\nfrom sklearn.model_selection import train_test_split\n\n\ntrain_edges, test_edges, train_labels, test_labels = train_test_split(list(range(len(G_down.edges))), \n                                                                      list(nx.get_edge_attributes(G_down, \"label\").values()), \n                                                                      test_size=0.20, \n                                                                      random_state=42)\n\n\nedgs = list(G_down.edges)\ntrain_graph = G_down.edge_subgraph([edgs[x] for x in train_edges]).copy()\ntrain_graph.add_nodes_from(list(set(G_down.nodes) - set(train_graph.nodes)))\n\n\n데이터 8:2 비율로 학습 검증\n\n\npip install node2vec\n\nCollecting node2vec\n  Downloading node2vec-0.4.6-py3-none-any.whl (7.0 kB)\nRequirement already satisfied: joblib<2.0.0,>=1.1.0 in /home/coco/anaconda3/envs/py38/lib/python3.8/site-packages (from node2vec) (1.2.0)\nCollecting gensim<5.0.0,>=4.1.2\n  Downloading gensim-4.3.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.5 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 26.5/26.5 MB 71.8 MB/s eta 0:00:0000:0100:01\nCollecting tqdm<5.0.0,>=4.55.1\n  Downloading tqdm-4.65.0-py3-none-any.whl (77 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 77.1/77.1 kB 18.6 MB/s eta 0:00:00\nCollecting networkx<3.0,>=2.5\n  Downloading networkx-2.8.8-py3-none-any.whl (2.0 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.0/2.0 MB 90.4 MB/s eta 0:00:00\nRequirement already satisfied: numpy<2.0.0,>=1.19.5 in /home/coco/anaconda3/envs/py38/lib/python3.8/site-packages (from node2vec) (1.24.2)\nRequirement already satisfied: scipy>=1.7.0 in /home/coco/anaconda3/envs/py38/lib/python3.8/site-packages (from gensim<5.0.0,>=4.1.2->node2vec) (1.10.1)\nCollecting smart-open>=1.8.1\n  Downloading smart_open-6.3.0-py3-none-any.whl (56 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 56.8/56.8 kB 13.6 MB/s eta 0:00:00\nInstalling collected packages: tqdm, smart-open, networkx, gensim, node2vec\n  Attempting uninstall: networkx\n    Found existing installation: networkx 3.0\n    Uninstalling networkx-3.0:\n      Successfully uninstalled networkx-3.0\nSuccessfully installed gensim-4.3.1 networkx-2.8.8 node2vec-0.4.6 smart-open-6.3.0 tqdm-4.65.0\nNote: you may need to restart the kernel to use updated packages.\n\n\n\nfrom node2vec import Node2Vec\nfrom node2vec.edges import HadamardEmbedder, AverageEmbedder, WeightedL1Embedder, WeightedL2Embedder\n\nnode2vec_train = Node2Vec(train_graph, weight_key='weight')\nmodel_train = node2vec_train.fit(window=10)\n\n\n\n\nGenerating walks (CPU: 1): 100%|██████████| 10/10 [00:03<00:00,  2.62it/s]\n\n\n\nNode2Vec 알고리즘 사용해 특징 공간 구축\n\n\nfrom sklearn.ensemble import RandomForestClassifier \nfrom sklearn import metrics \n\nclasses = [HadamardEmbedder, AverageEmbedder, WeightedL1Embedder, WeightedL2Embedder]\nfor cl in classes:\n    embeddings_train = cl(keyed_vectors=model_train.wv) \n    # 벡터스페이스 상에 edge를 투영.. \n\n    train_embeddings = [embeddings_train[str(edgs[x][0]), str(edgs[x][1])] for x in train_edges]\n    test_embeddings = [embeddings_train[str(edgs[x][0]), str(edgs[x][1])] for x in test_edges]\n    \n    rf = RandomForestClassifier(n_estimators=1000, random_state=42) \n    rf.fit(train_embeddings, train_labels); \n\n    y_pred = rf.predict(test_embeddings)\n    print(cl)\n    print('Precision:', metrics.precision_score(test_labels, y_pred)) \n    print('Recall:', metrics.recall_score(test_labels, y_pred)) \n    print('F1-Score:', metrics.f1_score(test_labels, y_pred)) \n\n<class 'node2vec.edges.HadamardEmbedder'>\nPrecision: 0.7451737451737451\nRecall: 0.16538131962296487\nF1-Score: 0.270687237026648\n<class 'node2vec.edges.AverageEmbedder'>\nPrecision: 0.691699604743083\nRecall: 0.7497857754927164\nF1-Score: 0.7195723684210527\n<class 'node2vec.edges.WeightedL1Embedder'>\nPrecision: 0.7142857142857143\nRecall: 0.029991431019708654\nF1-Score: 0.0575657894736842\n<class 'node2vec.edges.WeightedL2Embedder'>\nPrecision: 0.64\nRecall: 0.027420736932305057\nF1-Score: 0.052588331963845526\n\n\n\nNode2Vec 알고리즘 사용해 각 Edge2Vec 알고리즘으로 특징 공간 생성\nsklearn 파이썬 라이브러리의 RandomForestClassifier은 이전 단계에서 생성한 특징에 대해 학습\n검증 테스트 위해 정밀도, 재현율, F1-score 성능 지표 측정\n\n\n\n비지도학습\n\nk-means 알고리즘 사용\n지도학습과의 차이점은 특징 공간이 학습-검증 분할을 안함.\n\n\nnod2vec_unsup = Node2Vec(G_down, weight_key='weight')\nunsup_vals = nod2vec_unsup.fit(window=10)\n\n\n\n\nGenerating walks (CPU: 1): 100%|██████████| 10/10 [00:04<00:00,  2.32it/s]\n\n\n\n다운샘플링 절차에 전체 그래프 알고리즘 계산\n\n\nfrom sklearn.cluster import KMeans\n\nclasses = [HadamardEmbedder, AverageEmbedder, WeightedL1Embedder, WeightedL2Embedder]\ntrue_labels = [x for x in nx.get_edge_attributes(G_down, \"label\").values()]\n\nfor cl in classes:\n    embedding_edge = cl(keyed_vectors=unsup_vals.wv) \n\n    embedding = [embedding_edge[str(x[0]), str(x[1])] for x in G_down.edges()]\n    kmeans = KMeans(2, random_state=42).fit(embedding)\n    \n    \n    nmi = metrics.adjusted_mutual_info_score(true_labels, kmeans.labels_)\n    ho = metrics.homogeneity_score(true_labels, kmeans.labels_)\n    co = metrics.completeness_score(true_labels, kmeans.labels_)\n    vmeasure = metrics.v_measure_score(true_labels, kmeans.labels_)\n    \n    print(cl)\n    print('NMI:', nmi)\n    print('Homogeneity:', ho)\n    print('Completeness:', co)\n    print('V-Measure:', vmeasure)\n\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n\n\n<class 'node2vec.edges.HadamardEmbedder'>\nNMI: 0.04336246478827236\nHomogeneity: 0.0383178531539123\nCompleteness: 0.05011351404941123\nV-Measure: 0.043428985282038965\n\n\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n\n\n<class 'node2vec.edges.AverageEmbedder'>\nNMI: 0.11206093720015026\nHomogeneity: 0.10817496918905492\nCompleteness: 0.11635805522328385\nV-Measure: 0.11211739628609738\n\n\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n\n\n<class 'node2vec.edges.WeightedL1Embedder'>\nNMI: 0.16558117117175825\nHomogeneity: 0.16557714823761863\nCompleteness: 0.16568764408717976\nV-Measure: 0.16563237773404058\n\n\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n\n\n<class 'node2vec.edges.WeightedL2Embedder'>\nNMI: 0.1349652677966787\nHomogeneity: 0.1337881599748603\nCompleteness: 0.1362723387302234\nV-Measure: 0.13501882386803338\n\n\n- NMI(Normalized Mutual Information)\n\n두 개의 군집 결과 비교\n0~1이며 1에 가까울수록 높은 성능\n\n- Homogeneity\n\n하나의 실제 군집 내에서 같은 군집에 속한 샘플들이 군집화 결과에서 같은 군집에 속할 비율\n1에 가까울수록 높은 성능\n\n- Completeness\n\n하나의 예측 군집 내에서 같은 실제 군집에 속한 샘플들이 군집화 결과에서 같은 군집에 속할 비율\n0~1이며 1에 가까울수록 높은 성능\n\n- V-measure\n\nHomogeneity와 Completeness의 조화 평균\n0~1이며 1에 가까울수록 높은 성능\n비지도 학습에 이상치 탐지 방법\nk-means/LOF/One-class SVM 등이 있다.. 한번 같이 해보자.\n조금씩 다 커졌넹..\n\n- 지도학습에서 정상거래에서 다운샘플링을 했는데\n만약, 사기거래에서 업샘플링을 하게되면 어떻게 될까?"
  },
  {
    "objectID": "posts/plot_comparison_over_sampling.html",
    "href": "posts/plot_comparison_over_sampling.html",
    "title": "Scribbling",
    "section": "",
    "text": "%matplotlib inline"
  },
  {
    "objectID": "posts/plot_comparison_over_sampling.html#illustration-of-the-influence-of-the-balancing-ratio",
    "href": "posts/plot_comparison_over_sampling.html#illustration-of-the-influence-of-the-balancing-ratio",
    "title": "Scribbling",
    "section": "Illustration of the influence of the balancing ratio",
    "text": "Illustration of the influence of the balancing ratio\nWe will first illustrate the influence of the balancing ratio on some toy data using a logistic regression classifier which is a linear model.\n\nfrom sklearn.linear_model import LogisticRegression\n\nclf = LogisticRegression()\n\nWe will fit and show the decision boundary model to illustrate the impact of dealing with imbalanced classes.\n\nfig, axs = plt.subplots(nrows=2, ncols=2, figsize=(15, 12))\n\nweights_arr = (\n    (0.01, 0.01, 0.98),\n    (0.01, 0.05, 0.94),\n    (0.2, 0.1, 0.7),\n    (0.33, 0.33, 0.33),\n)\nfor ax, weights in zip(axs.ravel(), weights_arr):\n    X, y = create_dataset(n_samples=300, weights=weights)\n    clf.fit(X, y)\n    plot_decision_function(X, y, clf, ax, title=f\"weight={weights}\")\n    fig.suptitle(f\"Decision function of {clf.__class__.__name__}\")\nfig.tight_layout()\n\n\n\n\nGreater is the difference between the number of samples in each class, poorer are the classification results."
  },
  {
    "objectID": "posts/plot_comparison_over_sampling.html#random-over-sampling-to-balance-the-data-set",
    "href": "posts/plot_comparison_over_sampling.html#random-over-sampling-to-balance-the-data-set",
    "title": "Scribbling",
    "section": "Random over-sampling to balance the data set",
    "text": "Random over-sampling to balance the data set\nRandom over-sampling can be used to repeat some samples and balance the number of samples between the dataset. It can be seen that with this trivial approach the boundary decision is already less biased toward the majority class. The class :class:~imblearn.over_sampling.RandomOverSampler implements such of a strategy.\n\nfrom imblearn.over_sampling import RandomOverSampler\n\n\nfrom imblearn.pipeline import make_pipeline\n\nX, y = create_dataset(n_samples=100, weights=(0.05, 0.25, 0.7))\n\nfig, axs = plt.subplots(nrows=1, ncols=2, figsize=(15, 7))\n\nclf.fit(X, y)\nplot_decision_function(X, y, clf, axs[0], title=\"Without resampling\")\n\nsampler = RandomOverSampler(random_state=0)\nmodel = make_pipeline(sampler, clf).fit(X, y)\nplot_decision_function(X, y, model, axs[1], f\"Using {model[0].__class__.__name__}\")\n\nfig.suptitle(f\"Decision function of {clf.__class__.__name__}\")\nfig.tight_layout()\n\n\n\n\nBy default, random over-sampling generates a bootstrap. The parameter shrinkage allows adding a small perturbation to the generated data to generate a smoothed bootstrap instead. The plot below shows the difference between the two data generation strategies.\n\nfig, axs = plt.subplots(nrows=1, ncols=2, figsize=(15, 7))\n\nsampler.set_params(shrinkage=None)\nplot_resampling(X, y, sampler, ax=axs[0], title=\"Normal bootstrap\")\n\nsampler.set_params(shrinkage=0.3)\nplot_resampling(X, y, sampler, ax=axs[1], title=\"Smoothed bootstrap\")\n\nfig.suptitle(f\"Resampling with {sampler.__class__.__name__}\")\nfig.tight_layout()\n\n\n\n\nIt looks like more samples are generated with smoothed bootstrap. This is due to the fact that the samples generated are not superimposing with the original samples."
  },
  {
    "objectID": "posts/plot_comparison_over_sampling.html#more-advanced-over-sampling-using-adasyn-and-smote",
    "href": "posts/plot_comparison_over_sampling.html#more-advanced-over-sampling-using-adasyn-and-smote",
    "title": "Scribbling",
    "section": "More advanced over-sampling using ADASYN and SMOTE",
    "text": "More advanced over-sampling using ADASYN and SMOTE\nInstead of repeating the same samples when over-sampling or perturbating the generated bootstrap samples, one can use some specific heuristic instead. :class:~imblearn.over_sampling.ADASYN and :class:~imblearn.over_sampling.SMOTE can be used in this case.\n\nfrom imblearn import FunctionSampler  # to use a idendity sampler\nfrom imblearn.over_sampling import ADASYN, SMOTE\n\nX, y = create_dataset(n_samples=150, weights=(0.1, 0.2, 0.7))\n\nfig, axs = plt.subplots(nrows=2, ncols=2, figsize=(15, 15))\n\nsamplers = [\n    FunctionSampler(),\n    RandomOverSampler(random_state=0),\n    SMOTE(random_state=0),\n    ADASYN(random_state=0),\n]\n\nfor ax, sampler in zip(axs.ravel(), samplers):\n    title = \"Original dataset\" if isinstance(sampler, FunctionSampler) else None\n    plot_resampling(X, y, sampler, ax, title=title)\nfig.tight_layout()\n\n\n\n\nThe following plot illustrates the difference between :class:~imblearn.over_sampling.ADASYN and :class:~imblearn.over_sampling.SMOTE. :class:~imblearn.over_sampling.ADASYN will focus on the samples which are difficult to classify with a nearest-neighbors rule while regular :class:~imblearn.over_sampling.SMOTE will not make any distinction. Therefore, the decision function depending of the algorithm.\n\nX, y = create_dataset(n_samples=150, weights=(0.05, 0.25, 0.7))\n\nfig, axs = plt.subplots(nrows=1, ncols=3, figsize=(20, 6))\n\nmodels = {\n    \"Without sampler\": clf,\n    \"ADASYN sampler\": make_pipeline(ADASYN(random_state=0), clf),\n    \"SMOTE sampler\": make_pipeline(SMOTE(random_state=0), clf),\n}\n\nfor ax, (title, model) in zip(axs, models.items()):\n    model.fit(X, y)\n    plot_decision_function(X, y, model, ax=ax, title=title)\n\nfig.suptitle(f\"Decision function using a {clf.__class__.__name__}\")\nfig.tight_layout()\n\n\n\n\nDue to those sampling particularities, it can give rise to some specific issues as illustrated below.\n\nX, y = create_dataset(n_samples=5000, weights=(0.01, 0.05, 0.94), class_sep=0.8)\n\nsamplers = [SMOTE(random_state=0), ADASYN(random_state=0)]\n\nfig, axs = plt.subplots(nrows=2, ncols=2, figsize=(15, 15))\nfor ax, sampler in zip(axs, samplers):\n    model = make_pipeline(sampler, clf).fit(X, y)\n    plot_decision_function(\n        X, y, clf, ax[0], title=f\"Decision function with {sampler.__class__.__name__}\"\n    )\n    plot_resampling(X, y, sampler, ax[1])\n\nfig.suptitle(\"Particularities of over-sampling with SMOTE and ADASYN\")\nfig.tight_layout()\n\n\n\n\nSMOTE proposes several variants by identifying specific samples to consider during the resampling. The borderline version (:class:~imblearn.over_sampling.BorderlineSMOTE) will detect which point to select which are in the border between two classes. The SVM version (:class:~imblearn.over_sampling.SVMSMOTE) will use the support vectors found using an SVM algorithm to create new sample while the KMeans version (:class:~imblearn.over_sampling.KMeansSMOTE) will make a clustering before to generate samples in each cluster independently depending each cluster density.\n\nfrom sklearn.cluster import MiniBatchKMeans\n\nfrom imblearn.over_sampling import SVMSMOTE, BorderlineSMOTE, KMeansSMOTE\n\nX, y = create_dataset(n_samples=5000, weights=(0.01, 0.05, 0.94), class_sep=0.8)\n\nfig, axs = plt.subplots(5, 2, figsize=(15, 30))\n\nsamplers = [\n    SMOTE(random_state=0),\n    BorderlineSMOTE(random_state=0, kind=\"borderline-1\"),\n    BorderlineSMOTE(random_state=0, kind=\"borderline-2\"),\n    KMeansSMOTE(\n        kmeans_estimator=MiniBatchKMeans(n_init=1, random_state=0), random_state=0\n    ),\n    SVMSMOTE(random_state=0),\n]\n\nfor ax, sampler in zip(axs, samplers):\n    model = make_pipeline(sampler, clf).fit(X, y)\n    plot_decision_function(\n        X, y, clf, ax[0], title=f\"Decision function for {sampler.__class__.__name__}\"\n    )\n    plot_resampling(X, y, sampler, ax[1])\n\nfig.suptitle(\"Decision function and resampling using SMOTE variants\")\nfig.tight_layout()\n\n\n\n\nWhen dealing with a mixed of continuous and categorical features, :class:~imblearn.over_sampling.SMOTENC is the only method which can handle this case.\n\nfrom collections import Counter\n\nfrom imblearn.over_sampling import SMOTENC\n\nrng = np.random.RandomState(42)\nn_samples = 50\n# Create a dataset of a mix of numerical and categorical data\nX = np.empty((n_samples, 3), dtype=object)\nX[:, 0] = rng.choice([\"A\", \"B\", \"C\"], size=n_samples).astype(object)\nX[:, 1] = rng.randn(n_samples)\nX[:, 2] = rng.randint(3, size=n_samples)\ny = np.array([0] * 20 + [1] * 30)\n\nprint(\"The original imbalanced dataset\")\nprint(sorted(Counter(y).items()))\nprint()\nprint(\"The first and last columns are containing categorical features:\")\nprint(X[:5])\nprint()\n\nsmote_nc = SMOTENC(categorical_features=[0, 2], random_state=0)\nX_resampled, y_resampled = smote_nc.fit_resample(X, y)\nprint(\"Dataset after resampling:\")\nprint(sorted(Counter(y_resampled).items()))\nprint()\nprint(\"SMOTE-NC will generate categories for the categorical features:\")\nprint(X_resampled[-5:])\nprint()\n\nThe original imbalanced dataset\n[(0, 20), (1, 30)]\n\nThe first and last columns are containing categorical features:\n[['C' -0.14021849735700803 2]\n ['A' -0.033193400066544886 2]\n ['C' -0.7490765234433554 1]\n ['C' -0.7783820070908942 2]\n ['A' 0.948842857719016 2]]\n\nDataset after resampling:\n[(0, 30), (1, 30)]\n\nSMOTE-NC will generate categories for the categorical features:\n[['A' 0.5246469549655818 2]\n ['B' -0.3657680728116921 2]\n ['B' 0.9344237230779993 2]\n ['B' 0.3710891618824609 2]\n ['B' 0.3327240726719727 2]]\n\n\n\nHowever, if the dataset is composed of only categorical features then one should use :class:~imblearn.over_sampling.SMOTEN.\n\nfrom imblearn.over_sampling import SMOTEN\n\n# Generate only categorical data\nX = np.array([\"A\"] * 10 + [\"B\"] * 20 + [\"C\"] * 30, dtype=object).reshape(-1, 1)\ny = np.array([0] * 20 + [1] * 40, dtype=np.int32)\n\nprint(f\"Original class counts: {Counter(y)}\")\nprint()\nprint(X[:5])\nprint()\n\nsampler = SMOTEN(random_state=0)\nX_res, y_res = sampler.fit_resample(X, y)\nprint(f\"Class counts after resampling {Counter(y_res)}\")\nprint()\nprint(X_res[-5:])\nprint()\n\nOriginal class counts: Counter({1: 40, 0: 20})\n\n[['A']\n ['A']\n ['A']\n ['A']\n ['A']]\n\nClass counts after resampling Counter({0: 40, 1: 40})\n\n[['B']\n ['B']\n ['A']\n ['B']\n ['A']]"
  },
  {
    "objectID": "posts/graph8(frac=0.3).html",
    "href": "posts/graph8(frac=0.3).html",
    "title": "CH8. 신용카드 거래에 대한 그래프 분석(frac=0.3)",
    "section": "",
    "text": "그래프 머신러닝\ngithub\nCredit Card Transactions Fraud Detection Dataset\n컬리이미지\nnetworkx"
  },
  {
    "objectID": "posts/graph8(frac=0.3).html#네트워크-토폴로지",
    "href": "posts/graph8(frac=0.3).html#네트워크-토폴로지",
    "title": "CH8. 신용카드 거래에 대한 그래프 분석(frac=0.3)",
    "section": "네트워크 토폴로지",
    "text": "네트워크 토폴로지\n\n각 그래프별 차수 분포 살펴보기\n\n\nfor G in [G_bu, G_tu]:\n    plt.figure(figsize=(10,10))\n    degrees = pd.Series({k:v for k, v in nx.degree(G)})\n    degrees.plot.hist()\n    plt.yscale(\"log\")\n\n\n\n\n\n\n\n\nx축: 노드의 연결도\ny축: 로그 스케일(연결도가 큰 노드의 수가 매우 적으므로)\n\n- 각 그래프 간선 가중치 분포\n\nfor G in [G_bu, G_tu]:\n    allEdgeWeights = pd.Series({\n        (d[0],d[1]):d[2][\"weight\"]  #d[0],d[1]을 key로 d[2]를 weight로\n        #d는 G.edges(data=True)로 (u,v,data)형태의 튜플을 반복하는 반복문\n        for d in G.edges(data=True)})\n    np.quantile(allEdgeWeights.values,\n               [0.10, 0.50, 0.70, 0.9])\n    \n\n\nnp.quantile(allEdgeWeights.values,[0.10, 0.50, 0.70, 0.9])\n\narray([  4.15,  48.01,  75.75, 141.91])\n\n\n- 매게 중심성 측정 지표\n\nfor G in [G_bu, G_tu]:\n    plt.figure(figsize=(10,10))\n    bc_distr = pd.Series(nx.betweenness_centrality(G))\n    bc_distr.plot.hist()\n    plt.yscale(\"log\")\n\nKeyboardInterrupt: \n\n\n<Figure size 1000x1000 with 0 Axes>\n\n\n\n그래프 내에서 노드가 얼마나 중심적인 역할을 하는지 나타내는 지표\n해당 노드가 얼마나 많은 최단경로에 포함되는지 살피기\n노드가 많은 최단경로를 포함하면 해당노드의 매개중심성은 커진다.\n\n- 상관계수\n\nfor G in [G_bu, G_tu]:\n    print(nx.degree_pearson_correlation_coefficient(G))\n\n-0.12467174727090688\n-0.8051895351325623\n\n\n\n음의 동류성(서로 다른 속성을 가진 노드들끼리 연결되어 있다.)\n0~ -1 사이의 값을 가짐\n-1에 가까울수록 서로 다른 속성을 가진 노드들끼리 강한 음의 상관관계\n0에 가까울수록 노드들이 연결될 때 서로 다른 속성을 가진 노드들끼리 큰 차이가 없음 =>\n연결도 높은 개인이 연골도 낮은 개인과 연관돼 있다.\n이분그래프: 낮은 차수의 고객은 들어오는 트랜잭션 수가 많은 높은 차수의 판매자와만 연결되어 상관계수가 낮다.\n삼분그래프:동류성이 훨씬 더 낮다. 트랜잭션 노드가 있기 댸문에?"
  },
  {
    "objectID": "posts/graph8(frac=0.3).html#커뮤니티-감지",
    "href": "posts/graph8(frac=0.3).html#커뮤니티-감지",
    "title": "CH8. 신용카드 거래에 대한 그래프 분석(frac=0.3)",
    "section": "커뮤니티 감지",
    "text": "커뮤니티 감지\n\n# pip install python-louvain\n\n\nimport networkx as nx\nimport community\n\n\nimport community\nfor G in [G_bu, G_tu]:\n    parts = community.best_partition(G, random_state=42, weight='weight')\n\n\ncommunities = pd.Series(parts)\n\n\ncommunities\n\n255288    72\n204367    72\n65143     92\n10004     23\n194072     3\n          ..\n286119    78\n194740    88\n53644     57\n300283     9\n313041    66\nLength: 320413, dtype: int64\n\n\n\nprint(communities.value_counts().sort_values(ascending=False))\n\n4      9426\n94     6025\n6      5835\n42     5636\n50     5016\n       ... \n112    1341\n91     1307\n18     1104\n62     1057\n85      585\nLength: 113, dtype: int64\n\n\n\n커뮤니티 종류가 늘었따. 96>>113개로\n커뮤니티 감지를 통해 특정 사기 패턴 식별\n커뮤니티 추출 후 포함된 노드 수에 따라 정렬\n\n\ncommunities.value_counts().plot.hist(bins=20)\n\n<Axes: ylabel='Frequency'>\n\n\n\n\n\n\n9426개 이상한거 하나있고.. 약간 2000~3000사이에 집중되어 보인다.\n\n\ngraphs = [] # 부분그래프 저장\nd = {}  # 부정 거래 비율 저장 \nfor x in communities.unique():\n    tmp = nx.subgraph(G, communities[communities==x].index)\n    fraud_edges = sum(nx.get_edge_attributes(tmp, \"label\").values())\n    ratio = 0 if fraud_edges == 0 else (fraud_edges/tmp.number_of_edges())*100\n    d[x] = ratio\n    graphs += [tmp]\n\npd.Series(d).sort_values(ascending=False)\n\n56     5.281326\n59     4.709632\n111    4.399142\n77     4.149798\n15     3.975843\n         ...   \n90     0.409650\n112    0.297398\n110    0.292826\n67     0.277008\n18     0.180180\nLength: 113, dtype: float64\n\n\n\n사기 거래 비율 계산. 사기 거래가 집중된 특정 하위 그래프 식별\n특정 커뮤니티에 포함된 노드를 사용하여 노드 유도 하위 그래프 생성\n하위 그래프: 모든 간선 수에 대한 사기 거래 간선 수의 비율로 사기 거래 백분율 계싼\n\n\ngId = 10\nplt.figure(figsize=(10,10))\nspring_pos = nx.spring_layout(graphs[gId])\nplt.axis(\"off\")\nedge_colors = [\"r\" if x == 1 else \"g\" for x in nx.get_edge_attributes(graphs[gId], 'label').values()]  #r:빨간색, g:녹색\nnx.draw_networkx(graphs[gId], pos=spring_pos, node_color=default_node_color, \n                 edge_color=edge_colors, with_labels=False, node_size=15)\n\n\n\n\n\n커뮤니티 감지 알고리즘에 의해 감지된 노드 유도 하위 그래프 그리기\n특정 커뮤니티 인덱스 gId가 주어지면 해당 커뮤니티에서 사용 가능한 노드로 유도 하위 그래프 추출하고 얻는다.\n\n\ngId = 56\nplt.figure(figsize=(10,10))\nspring_pos = nx.spring_layout(graphs[gId])\nplt.axis(\"off\")\nedge_colors = [\"r\" if x == 1 else \"g\" for x in nx.get_edge_attributes(graphs[gId], 'label').values()]  #r:빨간색, g:녹색\nnx.draw_networkx(graphs[gId], pos=spring_pos, node_color=default_node_color, \n                 edge_color=edge_colors, with_labels=False, node_size=15)\n\n\n\n\n\ngId = 18\nplt.figure(figsize=(10,10))\nspring_pos = nx.spring_layout(graphs[gId])\nplt.axis(\"off\")\nedge_colors = [\"r\" if x == 1 else \"g\" for x in nx.get_edge_attributes(graphs[gId], 'label').values()]  #r:빨간색, g:녹색\nnx.draw_networkx(graphs[gId], pos=spring_pos, node_color=default_node_color, \n                 edge_color=edge_colors, with_labels=False, node_size=15)\n\n\n\n\n\npd.Series(d).plot.hist(bins=20)\n\n<Axes: ylabel='Frequency'>"
  },
  {
    "objectID": "posts/graph8(frac=0.3).html#사기-탐지를-위한-지도-및-비지도-임베딩",
    "href": "posts/graph8(frac=0.3).html#사기-탐지를-위한-지도-및-비지도-임베딩",
    "title": "CH8. 신용카드 거래에 대한 그래프 분석(frac=0.3)",
    "section": "사기 탐지를 위한 지도 및 비지도 임베딩",
    "text": "사기 탐지를 위한 지도 및 비지도 임베딩\n\n트랜잭션 간선으로 표기\n각 간선을 올바른 클래스(사기 또는 정상)으로 분류\n\n\n지도학습\n\nfrom sklearn.utils import resample\n\ndf_majority = df[df.is_fraud==0]\ndf_minority = df[df.is_fraud==1]\n\ndf_maj_dowsampled = resample(df_majority,\n                             n_samples=len(df_minority),\n                             random_state=42)\n\ndf_downsampled = pd.concat([df_minority, df_maj_dowsampled])\n\nprint(df_downsampled.is_fraud.value_counts())\nG_down = build_graph_bipartite(df_downsampled)\n\n1    6006\n0    6006\nName: is_fraud, dtype: int64\n\n\n\n무작위 언더샘플링 사용\n소수 클래스(사기거래)이 샘플 수 와 일치시키려고 다수 클래스(정상거래)의 하위 샘플을 가져옴\n데이터 불균형을 처리하기 위해서\n\n\nfrom sklearn.model_selection import train_test_split\n\n\ntrain_edges, test_edges, train_labels, test_labels = train_test_split(list(range(len(G_down.edges))), \n                                                                      list(nx.get_edge_attributes(G_down, \"label\").values()), \n                                                                      test_size=0.20, \n                                                                      random_state=42)\n\n\nedgs = list(G_down.edges)\ntrain_graph = G_down.edge_subgraph([edgs[x] for x in train_edges]).copy()\ntrain_graph.add_nodes_from(list(set(G_down.nodes) - set(train_graph.nodes)))\n\n\n데이터 8:2 비율로 학습 검증\n\n\npip install node2vec\n\nCollecting node2vec\n  Downloading node2vec-0.4.6-py3-none-any.whl (7.0 kB)\nRequirement already satisfied: joblib<2.0.0,>=1.1.0 in /home/coco/anaconda3/envs/py38/lib/python3.8/site-packages (from node2vec) (1.2.0)\nCollecting gensim<5.0.0,>=4.1.2\n  Downloading gensim-4.3.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.5 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 26.5/26.5 MB 71.8 MB/s eta 0:00:0000:0100:01\nCollecting tqdm<5.0.0,>=4.55.1\n  Downloading tqdm-4.65.0-py3-none-any.whl (77 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 77.1/77.1 kB 18.6 MB/s eta 0:00:00\nCollecting networkx<3.0,>=2.5\n  Downloading networkx-2.8.8-py3-none-any.whl (2.0 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.0/2.0 MB 90.4 MB/s eta 0:00:00\nRequirement already satisfied: numpy<2.0.0,>=1.19.5 in /home/coco/anaconda3/envs/py38/lib/python3.8/site-packages (from node2vec) (1.24.2)\nRequirement already satisfied: scipy>=1.7.0 in /home/coco/anaconda3/envs/py38/lib/python3.8/site-packages (from gensim<5.0.0,>=4.1.2->node2vec) (1.10.1)\nCollecting smart-open>=1.8.1\n  Downloading smart_open-6.3.0-py3-none-any.whl (56 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 56.8/56.8 kB 13.6 MB/s eta 0:00:00\nInstalling collected packages: tqdm, smart-open, networkx, gensim, node2vec\n  Attempting uninstall: networkx\n    Found existing installation: networkx 3.0\n    Uninstalling networkx-3.0:\n      Successfully uninstalled networkx-3.0\nSuccessfully installed gensim-4.3.1 networkx-2.8.8 node2vec-0.4.6 smart-open-6.3.0 tqdm-4.65.0\nNote: you may need to restart the kernel to use updated packages.\n\n\n\nfrom node2vec import Node2Vec\nfrom node2vec.edges import HadamardEmbedder, AverageEmbedder, WeightedL1Embedder, WeightedL2Embedder\n\nnode2vec_train = Node2Vec(train_graph, weight_key='weight')\nmodel_train = node2vec_train.fit(window=10)\n\n\n\n\nGenerating walks (CPU: 1): 100%|██████████| 10/10 [00:04<00:00,  2.47it/s]\n\n\n\nNode2Vec 알고리즘 사용해 특징 공간 구축\n\n\nfrom sklearn.ensemble import RandomForestClassifier \nfrom sklearn import metrics \n\nclasses = [HadamardEmbedder, AverageEmbedder, WeightedL1Embedder, WeightedL2Embedder]\nfor cl in classes:\n    embeddings_train = cl(keyed_vectors=model_train.wv) \n    # 벡터스페이스 상에 edge를 투영.. \n\n    train_embeddings = [embeddings_train[str(edgs[x][0]), str(edgs[x][1])] for x in train_edges]\n    test_embeddings = [embeddings_train[str(edgs[x][0]), str(edgs[x][1])] for x in test_edges]\n    \n    rf = RandomForestClassifier(n_estimators=1000, random_state=42) \n    rf.fit(train_embeddings, train_labels); \n\n    y_pred = rf.predict(test_embeddings)\n    print(cl)\n    print('Precision:', metrics.precision_score(test_labels, y_pred)) \n    print('Recall:', metrics.recall_score(test_labels, y_pred)) \n    print('F1-Score:', metrics.f1_score(test_labels, y_pred)) \n\n<class 'node2vec.edges.HadamardEmbedder'>\nPrecision: 0.7349397590361446\nRecall: 0.15996503496503497\nF1-Score: 0.26274228284278534\n<class 'node2vec.edges.AverageEmbedder'>\nPrecision: 0.6856264411990777\nRecall: 0.7797202797202797\nF1-Score: 0.7296523517382413\n<class 'node2vec.edges.WeightedL1Embedder'>\nPrecision: 0.5737704918032787\nRecall: 0.030594405594405596\nF1-Score: 0.05809128630705394\n<class 'node2vec.edges.WeightedL2Embedder'>\nPrecision: 0.609375\nRecall: 0.03409090909090909\nF1-Score: 0.06456953642384106\n\n\n\nNode2Vec 알고리즘 사용해 각 Edge2Vec 알고리즘으로 특징 공간 생성\nsklearn 파이썬 라이브러리의 RandomForestClassifier은 이전 단계에서 생성한 특징에 대해 학습\n검증 테스트 위해 정밀도, 재현율, F1-score 성능 지표 측정\n\n\n\n비지도학습\n\nk-means 알고리즘 사용\n지도학습과의 차이점은 특징 공간이 학습-검증 분할을 안함.\n\n\nnod2vec_unsup = Node2Vec(G_down, weight_key='weight')\nunsup_vals = nod2vec_unsup.fit(window=10)\n\n\n\n\nGenerating walks (CPU: 1): 100%|██████████| 10/10 [00:04<00:00,  2.30it/s]\n\n\n\n다운샘플링 절차에 전체 그래프 알고리즘 계산\n\n\nfrom sklearn.cluster import KMeans\n\nclasses = [HadamardEmbedder, AverageEmbedder, WeightedL1Embedder, WeightedL2Embedder]\ntrue_labels = [x for x in nx.get_edge_attributes(G_down, \"label\").values()]\n\nfor cl in classes:\n    embedding_edge = cl(keyed_vectors=unsup_vals.wv) \n\n    embedding = [embedding_edge[str(x[0]), str(x[1])] for x in G_down.edges()]\n    kmeans = KMeans(2, random_state=42).fit(embedding)\n    \n    \n    nmi = metrics.adjusted_mutual_info_score(true_labels, kmeans.labels_)\n    ho = metrics.homogeneity_score(true_labels, kmeans.labels_)\n    co = metrics.completeness_score(true_labels, kmeans.labels_)\n    vmeasure = metrics.v_measure_score(true_labels, kmeans.labels_)\n    \n    print(cl)\n    print('NMI:', nmi)\n    print('Homogeneity:', ho)\n    print('Completeness:', co)\n    print('V-Measure:', vmeasure)\n\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n\n\n<class 'node2vec.edges.HadamardEmbedder'>\nNMI: 0.04418691434534317\nHomogeneity: 0.0392170155918133\nCompleteness: 0.05077340984619601\nV-Measure: 0.044253187956299615\n\n\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n\n\n<class 'node2vec.edges.AverageEmbedder'>\nNMI: 0.10945180042668563\nHomogeneity: 0.10590886334115046\nCompleteness: 0.11336117407653773\nV-Measure: 0.10950837820667877\n\n\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n\n\n<class 'node2vec.edges.WeightedL1Embedder'>\nNMI: 0.17575054988974667\nHomogeneity: 0.1757509360433583\nCompleteness: 0.17585150874409544\nV-Measure: 0.17580120800977098\n\n\n/home/coco/anaconda3/envs/py38/lib/python3.8/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n\n\n<class 'node2vec.edges.WeightedL2Embedder'>\nNMI: 0.13740583375677415\nHomogeneity: 0.13628828058562012\nCompleteness: 0.1386505946822449\nV-Measure: 0.13745928896382234\n\n\n- NMI(Normalized Mutual Information)\n\n두 개의 군집 결과 비교\n0~1이며 1에 가까울수록 높은 성능\n\n- Homogeneity\n\n하나의 실제 군집 내에서 같은 군집에 속한 샘플들이 군집화 결과에서 같은 군집에 속할 비율\n1에 가까울수록 높은 성능\n\n- Completeness\n\n하나의 예측 군집 내에서 같은 실제 군집에 속한 샘플들이 군집화 결과에서 같은 군집에 속할 비율\n0~1이며 1에 가까울수록 높은 성능\n\n- V-measure\n\nHomogeneity와 Completeness의 조화 평균\n0~1이며 1에 가까울수록 높은 성능\n비지도 학습에 이상치 탐지 방법\nk-means/LOF/One-class SVM 등이 있다.. 한번 같이 해보자.\n조금씩 다 커졌넹..\n\n- 지도학습에서 정상거래에서 다운샘플링을 했는데\n만약, 사기거래에서 업샘플링을 하게되면 어떻게 될까?"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "끄적끄적 coco 올리기 전 아무거나 막 쓰는 용도"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Scribbling",
    "section": "",
    "text": "Compare under-sampling samplers\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n\n\n2023년 학회 발표 준비\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n\n\nCompare over-sampling samplers\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n\n\nCH4. 지도 그래프 학습(그래프 정규화 방법)\n\n\n\n\n\n\n\ngraph\n\n\n\n\n\n\n\n\n\n\n\nApr 6, 2023\n\n\n김보람\n\n\n\n\n\n\n\n\nCH8. 신용카드 거래에 대한 그래프 분석(frac=0.4)\n\n\n\n\n\n\n\ngraph\n\n\n\n\n\n\n\n\n\n\n\nApr 4, 2023\n\n\n김보람\n\n\n\n\n\n\n\n\nCH8. 신용카드 거래에 대한 그래프 분석(frac=0.3)\n\n\n\n\n\n\n\ngraph\n\n\n\n\n\n\n\n\n\n\n\nApr 4, 2023\n\n\n김보람\n\n\n\n\n\n\n\n\nVAE(-ing)\n\n\n\n\n\n\n\nVAE\n\n\nAuto-Encoding Variational Bayes\n\n\n\n\n\n\n\n\n\n\n\nMar 14, 2023\n\n\n김보람\n\n\n\n\n\n\n\n\n콰트로 블로그\n\n\n\n\n\n\n\n콰트로 블로그\n\n\n\n\n\n\n\n\n\n\n\nMar 13, 2023\n\n\n김보람\n\n\n\n\n\n\nNo matching items"
  }
]