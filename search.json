[
  {
    "objectID": "posts/VAE.html",
    "href": "posts/VAE.html",
    "title": "VAE(-ing)",
    "section": "",
    "text": "https://arxiv.org/pdf/1312.6114.pdf"
  },
  {
    "objectID": "posts/VAE.html#오토인코더",
    "href": "posts/VAE.html#오토인코더",
    "title": "VAE(-ing)",
    "section": "오토인코더",
    "text": "오토인코더\n\nEncoder, Decoder 네트워크로 구성된 모델\n학습 데이터-> encoder에 입력값"
  },
  {
    "objectID": "posts/graph4-3.html",
    "href": "posts/graph4-3.html",
    "title": "CH4. 지도 그래프 학습(그래프 정규화 방법)",
    "section": "",
    "text": "그래프 머신러닝\ngithub"
  },
  {
    "objectID": "posts/graph4-3.html#load-dataset",
    "href": "posts/graph4-3.html#load-dataset",
    "title": "CH4. 지도 그래프 학습(그래프 정규화 방법)",
    "section": "Load Dataset",
    "text": "Load Dataset\n- 데이터셋: Cora\n\n7개의 클래스로 라벨링돼 있는 2,708개의 컴퓨터 사이언스 논문\n각 논문은 인용을 기반으로 다른 노드와 연결된 노드\n총 5,429개의 간선\n\n\nfrom stellargraph import datasets\n\n2023-04-06 21:44:50.486139: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n\n\n\ndataset = datasets.Cora()\n\n\n%config Completer.use_jedi = False\n\n\ndataset.download()\n\n\nlabel_index = {\n      'Case_Based': 0,\n      'Genetic_Algorithms': 1,\n      'Neural_Networks': 2,\n      'Probabilistic_Methods': 3,\n      'Reinforcement_Learning': 4,\n      'Rule_Learning': 5,\n      'Theory': 6,\n  }\n\n\nG, labels = dataset.load()\n\n\nG: 네트워크 노드, 간선, BOW표현 설명\nlabea : 논문id와 클래스 중 하나 사이의 매핑\n훈련 샘플: 이웃과 관련된 정보가 포함 -> 훈련을 정규화 하는데 사용\n검증 샘플: 이웃과 관련된 정보 불포함 , 예측된 라벨은 노드 특증, bow표현에만 의존\n\n\nimport numpy as np\nfrom sklearn import preprocessing, feature_extraction, model_selection\n\n\nimport tensorflow as tf\nfrom tensorflow.train import Example, Features, Feature, Int64List, BytesList, FloatList\n\n\nGRAPH_PREFIX=\"NL_nbr\"\n\n\ndef _int64_feature(*value):\n    \"\"\"Returns int64 tf.train.Feature from a bool / enum / int / uint.\"\"\"\n    return Feature(int64_list=Int64List(value=list(value)))\n\ndef _bytes_feature(value):\n    \"\"\"Returns bytes tf.train.Feature from a string.\"\"\"\n    return Feature(\n        bytes_list=BytesList(value=[value.encode('utf-8')])\n    )\n\ndef _float_feature(*value):\n    return Feature(float_list=FloatList(value=list(value)))\n\n\n_int64_feature 함수는 bool, enum, int, uint 데이터 타입을 입력 받아 int64_list 타입의 tf.train.Feature 객체를 반환\n_bytes_feature 함수는 문자열 값을 입력 받아 utf-8로 인코딩하여 bytes_list 타입의 tf.train.Feature 객체를 반환\n_float_feature 함수는 float 데이터 타입을 입력 받아 float_list 타입의 tf.train.Feature 객체를 반환\n\n- 반지도 학습 데이터 셋 만드는 함수 정의\n\nfrom functools import reduce\nfrom typing import List, Tuple\nimport pandas as pd\nimport six\n\ndef addFeatures(x, y):\n    res = Features()\n    res.CopyFrom(x)\n    res.MergeFrom(y)\n    return res\n\ndef neighborFeatures(features: Features, weight: float, prefix: str):  # 객체, 가중치, 접두어 입력으로 받음\n    data = {f\"{prefix}_weight\": _float_feature(weight)}\n    for name, feature in six.iteritems(features.feature):\n        data[f\"{prefix}_{name}\"] = feature \n    return Features(feature=data)\n\ndef neighborsFeatures(neighbors: List[Tuple[Features, float]]):\n    return reduce(\n        addFeatures, \n        [neighborFeatures(sample, weight, f\"{GRAPH_PREFIX}_{ith}\") for ith, (sample, weight) in enumerate(neighbors)],\n        Features()\n    )\n\ndef getNeighbors(idx, adjMatrix, topn=5): #인덱스와 인접행렬 이용하여 이웃 데이터셋 추출 \n    weights = adjMatrix.loc[idx]\n    return weights[weights>0].sort_values(ascending=False).head(topn).to_dict()\n    \n\ndef semisupervisedDataset(G, labels, ratio=0.2, topn=5):  #라벨이 있는 데이터와 없는 데이터 추출\n     #ratio:라벨 유무 비율 설정\n     #topn: 함수에서 추출할 이웃 데이터셋 크기 설정\n    n = int(np.round(len(labels)*ratio)) \n    \n    labelled, unlabelled = model_selection.train_test_split(\n        labels, train_size=n, test_size=None, stratify=labels\n    )\n\n\n1. 노드 특징 df로 구성하고 그래프 인접행렬로 저장\n\nadjMatrix = pd.DataFrame.sparse.from_spmatrix(G.to_adjacency_matrix(), index=G.nodes(), columns=G.nodes())\n    \nfeatures = pd.DataFrame(G.node_features(), index=G.nodes())\n\n\n\n2. adjMatrix사용해 노드ID와 간선 가중치 반환하여 노드의 가장 가까운 TOPN이웃 검색하는 도우미 함수 구현\n\ndef getNeighbors(idx, adjMatrix, topn=5): #인덱스와 인접행렬 이용하여 이웃 데이터셋 추출 \n    weights = adjMatrix.loc[idx]\n    neighbors = weights[weights>0]\\\n        .sort_values(ascending=False)\\\n        .head(topn)\n    return [(k,v) for k, v in neighbors.iteritems()]\n    \n\n\n3. 정보를 단일 df로 병합\n\ndataset = {\n        index: Features(feature = {\n            #\"id\": _bytes_feature(str(index)), \n            \"id\": _int64_feature(index),\n            \"words\": _float_feature(*[float(x) for x in features.loc[index].values]), \n            \"label\": _int64_feature(label_index[label])\n        })\n        for index, label in pd.concat([labelled, unlabelled]).items()\n    }\n\nNameError: name 'labelled' is not defined\n\n\n\nfrom functools import reduce\nfrom typing import List, Tuple\nimport pandas as pd\nimport six\n\ndef addFeatures(x, y):\n    res = Features()\n    res.CopyFrom(x)\n    res.MergeFrom(y)\n    return res\n\ndef neighborFeatures(features: Features, weight: float, prefix: str):\n    data = {f\"{prefix}_weight\": _float_feature(weight)}\n    for name, feature in six.iteritems(features.feature):\n        data[f\"{prefix}_{name}\"] = feature \n    return Features(feature=data)\n\ndef neighborsFeatures(neighbors: List[Tuple[Features, float]]):\n    return reduce(\n        addFeatures, \n        [neighborFeatures(sample, weight, f\"{GRAPH_PREFIX}_{ith}\") for ith, (sample, weight) in enumerate(neighbors)],\n        Features()\n    )\n\ndef getNeighbors(idx, adjMatrix, topn=5):\n    weights = adjMatrix.loc[idx]\n    return weights[weights>0].sort_values(ascending=False).head(topn).to_dict()\n    \n\ndef semisupervisedDataset(G, labels, ratio=0.2, topn=5):\n    n = int(np.round(len(labels)*ratio))\n    \n    labelled, unlabelled = model_selection.train_test_split(\n        labels, train_size=n, test_size=None, stratify=labels\n    )\n    \n    adjMatrix = pd.DataFrame.sparse.from_spmatrix(G.to_adjacency_matrix(), index=G.nodes(), columns=G.nodes())\n    \n    features = pd.DataFrame(G.node_features(), index=G.nodes())\n    \n    dataset = {\n        index: Features(feature = {\n            #\"id\": _bytes_feature(str(index)), \n            \"id\": _int64_feature(index),\n            \"words\": _float_feature(*[float(x) for x in features.loc[index].values]), \n            \"label\": _int64_feature(label_index[label])\n        })\n        for index, label in pd.concat([labelled, unlabelled]).items()\n    }\n    \n    trainingSet = [\n        Example(features=addFeatures(\n            dataset[exampleId], \n            neighborsFeatures(\n                [(dataset[nodeId], weight) for nodeId, weight in getNeighbors(exampleId, adjMatrix, topn).items()]\n            )\n        ))\n        for exampleId in labelled.index\n    ]\n    \n    testSet = [Example(features=dataset[exampleId]) for exampleId in unlabelled.index]\n\n    serializer = lambda _list: [e.SerializeToString() for e in _list]\n    \n    return serializer(trainingSet), serializer(testSet)"
  },
  {
    "objectID": "posts/학회.html",
    "href": "posts/학회.html",
    "title": "Scribbling",
    "section": "",
    "text": "일정:23. 6.29(목) ~ 7.1 (토)\n장소: 부경대학교(부산)\n발표신청 및 초록제출: 3.20.(월) ~ 4.20.(목)\n발표요약본제출(석사과정) : ~4.20.(목)\n포스터파일제출: ~ 5.19.(금)\n\n\n\n\n\n하계: 2023. 7.6.(목) ~ 7.7.(금)\n고려대학교\n발표신청 및 사전등록: 23.5.29.(월)\n초록 또는 논문제출: ~5.31.(수)\n\n\n\n\n\n일정: 6.23.(금) ~ 6.24.(토)\n장소: 강릉원주대학교\n발표논문 초록 제출: ~5.5.(금)\n발표 논문 제출: ~6.16.(금)\n학생논문: ~6.9.(금)"
  },
  {
    "objectID": "posts/ml with python 8.html",
    "href": "posts/ml with python 8.html",
    "title": "지도 학습",
    "section": "",
    "text": "ref\n\n선형대수와 통계학으로 배우는 머신러닝 with 파이썬\ngithub\n\n\n\nk-최근법 이웃 알고리즘\n\n\n# 데이터 불러오기\nfrom sklearn import datasets\nraw_iris = datasets.load_iris()\n\n# 피쳐/타겟\nX = raw_iris.data\ny = raw_iris.target\n\n# 트레이닝/테스트 데이터 분할\nfrom sklearn.model_selection import train_test_split\nX_tn, X_te, y_tn, y_te=train_test_split(X,y,random_state=0)\n\n#데이터 표준화\nfrom sklearn.preprocessing import StandardScaler\nstd_scale = StandardScaler()\nstd_scale.fit(X_tn)\nX_tn_std = std_scale.transform(X_tn)\nX_te_std  = std_scale.transform(X_te)\n\n\n# 학습\nfrom sklearn.neighbors import KNeighborsClassifier\nclf_knn =  KNeighborsClassifier(n_neighbors=2)\nclf_knn.fit(X_tn_std, y_tn)\n\nKNeighborsClassifier(n_neighbors=2)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.KNeighborsClassifierKNeighborsClassifier(n_neighbors=2)\n\n\n\n# 예측\nknn_pred = clf_knn.predict(X_te_std)\nprint(knn_pred)\n\n[2 1 0 2 0 2 0 1 1 1 1 1 1 1 1 0 1 1 0 0 2 1 0 0 2 0 0 1 1 0 2 1 0 2 2 1 0\n 2]\n\n\n\n# 정확도\nfrom sklearn.metrics import accuracy_score\naccuracy = accuracy_score(y_te, knn_pred)\nprint(accuracy)\n\n0.9473684210526315\n\n\n\n# confusion matrix 확인 \nfrom sklearn.metrics import confusion_matrix\nconf_matrix = confusion_matrix(y_te, knn_pred)\nprint(conf_matrix)\n\n[[13  0  0]\n [ 0 15  1]\n [ 0  1  8]]\n\n\n\n# 분류 레포트 확인\nfrom sklearn.metrics import classification_report\nclass_report = classification_report(y_te, knn_pred)\nprint(class_report)\n\n              precision    recall  f1-score   support\n\n           0       1.00      1.00      1.00        13\n           1       0.94      0.94      0.94        16\n           2       0.89      0.89      0.89         9\n\n    accuracy                           0.95        38\n   macro avg       0.94      0.94      0.94        38\nweighted avg       0.95      0.95      0.95        38\n\n\n\n\n\n선형 회귀 분석\nfrom sklearn import datasets\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.linear_model import Ridge\nfrom sklearn.linear_model import Lasso\nfrom sklearn.linear_model import ElasticNet\nfrom sklearn.linear_model import LinearRegression \n\nfrom sklearn.metrics import r2_score\nfrom sklearn.metrics import mean_squared_error\n\n\n# 데이터 불러오기\nraw_boston = datasets.load_boston()\n\n# 피쳐, 타겟 데이터 지정\nX = raw_boston.data\ny = raw_boston.target\n\n# 트레이닝/테스트 데이터 분할\nX_tn, X_te, y_tn, y_te = train_test_split(X,y,random_state=1)\n\n\n# 데이터 표준화\nstd_scale = StandardScaler()\nstd_scale.fit(X_tn)\nX_tn_std = std_scale.transform(X_tn)\nX_te_std  = std_scale.transform(X_te)\n\n# 선형 회귀분석 학습\nclf_lr =  LinearRegression()\nclf_lr.fit(X_tn_std, y_tn)\n\n# 선형 회귀분석 모형 추정 계수 확인\nprint(clf_lr.coef_)\nprint(clf_lr.intercept_)\n\n# 릿지 회귀분석(L2 제약식 적용)\nclf_ridge = Ridge(alpha=1)\nclf_ridge.fit(X_tn_std, y_tn)\n\n# 릿지 회귀분석 모형 추정 계수 확인\nprint(clf_ridge.coef_)\nprint(clf_ridge.intercept_)\n\n# 라쏘 회귀분석(L1 제약식 적용)\nclf_lasso = Lasso(alpha=0.01)\nclf_lasso.fit(X_tn_std, y_tn)\n\n# 라쏘 회귀분석 모형 추정 계수 확인\nprint(clf_lasso.coef_)\nprint(clf_lasso.intercept_)\n\n# 엘라스틱넷\nclf_elastic = ElasticNet(alpha=0.01, l1_ratio=0.01)\nclf_elastic.fit(X_tn_std, y_tn)\n\n# 엘라스틱넷 모형 추정 계수 확인\nprint(clf_elastic.coef_)\nprint(clf_elastic.intercept_)\n\n# 예측\npred_lr = clf_lr.predict(X_te_std)\npred_ridge = clf_ridge.predict(X_te_std)\npred_lasso = clf_lasso.predict(X_te_std)\npred_elastic = clf_elastic.predict(X_te_std)\n\n# 모형 평가-R제곱값\nprint(r2_score(y_te, pred_lr))\nprint(r2_score(y_te, pred_ridge))\nprint(r2_score(y_te, pred_lasso))\nprint(r2_score(y_te, pred_elastic))\n\n# 모형 평가-MSE\nprint(mean_squared_error(y_te, pred_lr))\nprint(mean_squared_error(y_te, pred_ridge))\nprint(mean_squared_error(y_te, pred_lasso))\nprint(mean_squared_error(y_te, pred_elastic))\n- 회귀분석\n\\[\\hat w = (X^TX)^{-1}X^Ty\\]\n- 릿지 회귀 분석(L2제약식)\n\\[\\hat w^{ridge} = (X^TX+ \\lambda I_p)^{-1}X^Ty\\]\n\n\\(\\lambda\\) 계수의 사이즈 조절, 정규식의 크기 조절, 0에 가까울수록 최소 제곱 추정량에 가까워지며 무한대에 가까워질수록 릿지 해는 0에 가까워짐\n편향(bias)가 존재\n\n- 라쏘 회귀 분석(L1제약식)\n\\[\\hat w^{lasso}=argmin_w \\{(y-Xw)^T(y-Xw)+\\lambda(|w|-t) \\}\\]\n\n\n로지스틱 회귀 분석\n\n# 데이터 불러오기\nfrom sklearn import datasets\nraw_cancer = datasets.load_breast_cancer()\n\n# 피쳐, 타겟 데이터 지정\nX = raw_cancer.data\ny = raw_cancer.target\n\n# 트레이닝/테스트 데이터 분할\nfrom sklearn.model_selection import train_test_split\nX_tn, X_te, y_tn, y_te=train_test_split(X,y,random_state=0)\n\n\n\n\n#데이터 표준화\nfrom sklearn.preprocessing import StandardScaler\nstd_scale = StandardScaler()\nstd_scale.fit(X_tn)\nX_tn_std = std_scale.transform(X_tn)\nX_te_std  = std_scale.transform(X_te)\n\n\n# 로지스틱 회귀분석(L2 제약식 적용)\nfrom sklearn.linear_model import LogisticRegression\nclf_logi_l2 =  LogisticRegression(penalty='l2')\nclf_logi_l2.fit(X_tn_std, y_tn)\n\nLogisticRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LogisticRegressionLogisticRegression()\n\n\n\n# 로지스틱 회귀분석 모형(L2 제약식 적용) 추정 계수\nprint(clf_logi_l2.coef_) # 추정 계수\nprint(clf_logi_l2.intercept_) # 상수항\n\n[[-0.29792942 -0.58056355 -0.3109406  -0.377129   -0.11984232  0.42855478\n  -0.71131106 -0.85371164 -0.46688191  0.11762548 -1.38262136  0.0899184\n  -0.94778563 -0.94686238  0.18575731  0.99305313  0.11090349 -0.3458275\n   0.20290919  0.80470317 -0.91626377 -0.91726667 -0.8159834  -0.86539197\n  -0.45539191  0.10347391 -0.83009341 -0.98445173 -0.5920036  -0.61086989]]\n[0.02713751]\n\n\n\n# 예측\npred_logistic = clf_logi_l2.predict(X_te_std)\nprint(pred_logistic)\n\n[0 1 1 1 1 1 1 1 1 1 1 1 1 0 1 0 1 0 0 0 0 0 1 1 0 1 1 0 1 0 1 0 1 0 1 0 1\n 0 1 0 0 1 0 1 1 0 1 1 1 0 0 0 0 1 1 1 1 1 1 0 0 0 1 1 0 1 0 0 0 1 1 0 1 0\n 0 1 1 1 1 1 0 0 0 1 0 1 1 1 0 0 1 0 0 0 1 1 0 1 1 1 1 1 1 1 0 1 0 1 1 1 1\n 0 0 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 0 1 1 1 1 1 0 0 0 1 1 1 0]\n\n\n\n# 확률값으로 예측\npred_proba = clf_logi_l2.predict_proba(X_te_std)\nprint(pred_proba)\n\n[[9.98638613e-01 1.36138656e-03]\n [3.95544804e-02 9.60445520e-01]\n [1.30896362e-03 9.98691036e-01]\n [1.24473354e-02 9.87552665e-01]\n [2.44132101e-04 9.99755868e-01]\n [4.50491513e-03 9.95495085e-01]\n [1.13985968e-04 9.99886014e-01]\n [1.82475894e-03 9.98175241e-01]\n [9.67965506e-05 9.99903203e-01]\n [1.75222878e-06 9.99998248e-01]\n [1.76572612e-01 8.23427388e-01]\n [8.24119135e-02 9.17588087e-01]\n [9.66067493e-06 9.99990339e-01]\n [5.39343196e-01 4.60656804e-01]\n [3.98187854e-01 6.01812146e-01]\n [9.95762760e-01 4.23724017e-03]\n [2.75612083e-03 9.97243879e-01]\n [9.99997097e-01 2.90271401e-06]\n [9.99926506e-01 7.34935682e-05]\n [9.99999997e-01 2.78313939e-09]\n [9.98738365e-01 1.26163489e-03]\n [9.81405399e-01 1.85946008e-02]\n [1.77902039e-02 9.82209796e-01]\n [9.65876713e-04 9.99034123e-01]\n [9.99464578e-01 5.35421808e-04]\n [6.73385015e-04 9.99326615e-01]\n [5.50833875e-05 9.99944917e-01]\n [9.69828919e-01 3.01710813e-02]\n [1.62119075e-03 9.98378809e-01]\n [9.99997821e-01 2.17867101e-06]\n [6.00571253e-05 9.99939943e-01]\n [9.99954808e-01 4.51921300e-05]\n [1.09252006e-01 8.90747994e-01]\n [9.97255978e-01 2.74402243e-03]\n [4.51047979e-06 9.99995490e-01]\n [9.97449456e-01 2.55054412e-03]\n [1.97830173e-02 9.80216983e-01]\n [9.99571529e-01 4.28470822e-04]\n [8.45566258e-03 9.91544337e-01]\n [9.99487912e-01 5.12087502e-04]\n [9.42409583e-01 5.75904174e-02]\n [8.34700429e-05 9.99916530e-01]\n [9.32505814e-01 6.74941855e-02]\n [8.11944408e-05 9.99918806e-01]\n [6.08911689e-02 9.39108831e-01]\n [9.99999999e-01 1.17373572e-09]\n [1.00967748e-06 9.99998990e-01]\n [1.48182234e-02 9.85181777e-01]\n [6.33630458e-04 9.99366370e-01]\n [9.99927519e-01 7.24813084e-05]\n [9.99989528e-01 1.04724511e-05]\n [8.04262948e-01 1.95737052e-01]\n [9.99965014e-01 3.49860375e-05]\n [1.36691079e-03 9.98633089e-01]\n [1.95330244e-03 9.98046698e-01]\n [5.74609838e-04 9.99425390e-01]\n [1.05063052e-03 9.98949369e-01]\n [7.96089471e-03 9.92039105e-01]\n [1.00288029e-02 9.89971197e-01]\n [9.99999999e-01 1.44073341e-09]\n [9.97609027e-01 2.39097260e-03]\n [9.99257870e-01 7.42129950e-04]\n [3.14309030e-05 9.99968569e-01]\n [4.40044150e-03 9.95599559e-01]\n [9.99897373e-01 1.02627439e-04]\n [1.52976144e-01 8.47023856e-01]\n [1.00000000e+00 2.39185116e-13]\n [9.99998777e-01 1.22317020e-06]\n [9.99999046e-01 9.53579837e-07]\n [7.96239235e-04 9.99203761e-01]\n [3.87033734e-01 6.12966266e-01]\n [9.99993469e-01 6.53125942e-06]\n [2.97085842e-03 9.97029142e-01]\n [8.09412134e-01 1.90587866e-01]\n [9.99996998e-01 3.00240009e-06]\n [1.75950117e-02 9.82404988e-01]\n [4.94325863e-05 9.99950567e-01]\n [3.51047770e-02 9.64895223e-01]\n [4.25841119e-04 9.99574159e-01]\n [2.09232609e-05 9.99979077e-01]\n [9.82374564e-01 1.76254356e-02]\n [1.00000000e+00 3.57855006e-10]\n [9.99988747e-01 1.12526453e-05]\n [5.94724730e-05 9.99940528e-01]\n [9.62731634e-01 3.72683662e-02]\n [1.69452548e-03 9.98305475e-01]\n [6.14966533e-05 9.99938503e-01]\n [6.36886875e-06 9.99993631e-01]\n [9.99902779e-01 9.72205364e-05]\n [1.00000000e+00 8.14423797e-11]\n [3.47458432e-05 9.99965254e-01]\n [5.53589378e-01 4.46410622e-01]\n [6.91462937e-01 3.08537063e-01]\n [9.99996851e-01 3.14924112e-06]\n [2.01951834e-03 9.97980482e-01]\n [2.39759190e-03 9.97602408e-01]\n [9.99999992e-01 7.92006333e-09]\n [1.03400237e-02 9.89659976e-01]\n [9.23218910e-03 9.90767811e-01]\n [9.80048490e-04 9.99019952e-01]\n [5.45753731e-09 9.99999995e-01]\n [3.09034901e-03 9.96909651e-01]\n [6.22819445e-03 9.93771806e-01]\n [1.49494565e-01 8.50505435e-01]\n [9.99994787e-01 5.21292981e-06]\n [6.02188244e-04 9.99397812e-01]\n [9.99995658e-01 4.34219020e-06]\n [9.49795077e-02 9.05020492e-01]\n [3.27428663e-01 6.72571337e-01]\n [1.72350019e-02 9.82764998e-01]\n [3.75686888e-02 9.62431311e-01]\n [9.99975711e-01 2.42887910e-05]\n [9.99911399e-01 8.86014791e-05]\n [8.65663331e-02 9.13433667e-01]\n [8.21398481e-04 9.99178602e-01]\n [2.45946373e-02 9.75405363e-01]\n [1.43898490e-01 8.56101510e-01]\n [1.58128486e-03 9.98418715e-01]\n [1.79682971e-02 9.82031703e-01]\n [1.18803803e-03 9.98811962e-01]\n [1.55728346e-02 9.84427165e-01]\n [1.43822197e-03 9.98561778e-01]\n [3.86829219e-01 6.13170781e-01]\n [2.65232841e-02 9.73476716e-01]\n [9.99999918e-01 8.17382381e-08]\n [1.28424726e-01 8.71575274e-01]\n [4.67709202e-01 5.32290798e-01]\n [2.58725940e-04 9.99741274e-01]\n [3.25269018e-05 9.99967473e-01]\n [4.00075207e-05 9.99959992e-01]\n [9.99901036e-01 9.89636008e-05]\n [1.27248974e-04 9.99872751e-01]\n [2.66411581e-04 9.99733588e-01]\n [2.13163719e-01 7.86836281e-01]\n [2.92511631e-02 9.70748837e-01]\n [2.37309476e-05 9.99976269e-01]\n [5.09465728e-01 4.90534272e-01]\n [6.17881971e-01 3.82118029e-01]\n [1.00000000e+00 1.46648090e-12]\n [8.41453252e-05 9.99915855e-01]\n [1.58701592e-03 9.98412984e-01]\n [1.26424968e-03 9.98735750e-01]\n [9.99999994e-01 5.81805301e-09]]\n\n\n\n# 정밀도\nfrom sklearn.metrics import precision_score\nprecision = precision_score(y_te, pred_logistic)\nprint(precision)\n\n0.9666666666666667\n\n\n\n# confusion matrix 확인 \nfrom sklearn.metrics import confusion_matrix\nconf_matrix = confusion_matrix(y_te, pred_logistic)\nprint(conf_matrix)\n\n[[50  3]\n [ 3 87]]\n\n\n\n# 분류 레포트 확인\nfrom sklearn.metrics import classification_report\nclass_report = classification_report(y_te, pred_logistic)\nprint(class_report)\n\n              precision    recall  f1-score   support\n\n           0       0.94      0.94      0.94        53\n           1       0.97      0.97      0.97        90\n\n    accuracy                           0.96       143\n   macro avg       0.96      0.96      0.96       143\nweighted avg       0.96      0.96      0.96       143\n\n\n\n\n\n나이브 베이즈(추후 다시)\nfrom sklearn import datasets\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.naive_bayes import GaussianNB\n\nfrom sklearn.metrics import recall_score\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report\n\n\n# 데이터 불러오기\nraw_wine = datasets.load_wine()\n\n# 피쳐, 타겟 데이터 지정\nX = raw_wine.data\ny = raw_wine.target\n\n# 트레이닝/테스트 데이터 분할\nX_tn, X_te, y_tn, y_te=train_test_split(X,y,random_state=0)\n\n# 데이터 표준화\nstd_scale = StandardScaler()\nstd_scale.fit(X_tn)\nX_tn_std = std_scale.transform(X_tn)\nX_te_std  = std_scale.transform(X_te)\n\n# 나이브 베이즈 학습\nclf_gnb = GaussianNB()\nclf_gnb.fit(X_tn_std, y_tn)\n\n# 예측\npred_gnb = clf_gnb.predict(X_te_std)\nprint(pred_gnb)\n\n# 리콜\nrecall = recall_score(y_te, pred_gnb, average='macro')\nprint(recall)\n\n# confusion matrix 확인 \nconf_matrix = confusion_matrix(y_te, pred_gnb)\nprint(conf_matrix)\n\n# 분류 레포트 확인\nclass_report = classification_report(y_te, pred_gnb)\nprint(class_report)\n\n\n의사결정나무(추후 다시)\n\n테스트 성능 평가는 엔트로피 이용\n엔트로피는 불순도(노드에 서로 다른 데이터가 얼마나 섞여 있는지) 정도를 측정하며 낮을수록 좋다.\n\n\\[Entropy(d) = - \\sum p(x) log P(x)\\]\n\\[= - \\sum_{i=1}^k p(i|d)log_2(p(i|d))\\]\n\n# 데이터 불러오기\nfrom sklearn import datasets\nraw_wine = datasets.load_wine()\n\n# 피쳐, 타겟 데이터 지정\nX = raw_wine.data\ny = raw_wine.target\n\n# 트레이닝/테스트 데이터 분할\nfrom sklearn.model_selection import train_test_split\nX_tn, X_te, y_tn, y_te=train_test_split(X,y,random_state=0)\n\n# 데이터 표준화\nfrom sklearn.preprocessing import StandardScaler\nstd_scale = StandardScaler()\nstd_scale.fit(X_tn)\nX_tn_std = std_scale.transform(X_tn)\nX_te_std  = std_scale.transform(X_te)\n\n\n# 의사결정나무 학습\nfrom sklearn import tree \nclf_tree = tree.DecisionTreeClassifier(random_state=0)\nclf_tree.fit(X_tn_std, y_tn)\n\n\nDecisionTreeClassifier(random_state=0)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.DecisionTreeClassifierDecisionTreeClassifier(random_state=0)\n\n\n\n# 예측\npred_tree = clf_tree.predict(X_te_std)\nprint(pred_tree)\n\n\n[0 2 1 0 1 1 0 2 1 1 2 2 0 1 2 1 0 0 2 0 1 0 1 1 1 1 1 1 1 2 0 0 1 0 0 0 2\n 1 1 2 1 0 1 1 1]\n\n\n\n# f1 score\nfrom sklearn.metrics import f1_score\nf1 = f1_score(y_te, pred_tree, average='macro')\nprint(f1)\n\n0.9349141206870346\n\n\n\n# confusion matrix 확인 \nfrom sklearn.metrics import confusion_matrix\nconf_matrix = confusion_matrix(y_te, pred_tree)\nprint(conf_matrix)\n\n[[14  2  0]\n [ 0 20  1]\n [ 0  0  8]]\n\n\n\n# 분류 레포트 확인\nfrom sklearn.metrics import classification_report\nclass_report = classification_report(y_te, pred_tree)\nprint(class_report)\n\n              precision    recall  f1-score   support\n\n           0       1.00      0.88      0.93        16\n           1       0.91      0.95      0.93        21\n           2       0.89      1.00      0.94         8\n\n    accuracy                           0.93        45\n   macro avg       0.93      0.94      0.93        45\nweighted avg       0.94      0.93      0.93        45\n\n\n\n\n\n서포트 벡터 머신(추후 다시)\n\n\n크로스 밸리데이션(추후 다시)"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "끄적끄적 coco 올리기 전 아무거나 막 쓰는 용도"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Scribbling",
    "section": "",
    "text": "2023년 학회 발표 준비\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n\n\n지도 학습\n\n\n\n\n\n\n\n지도 학습\n\n\n\n\n\n\n\n\n\n\n\nMay 11, 2023\n\n\n김보람\n\n\n\n\n\n\n\n\nCH4. 지도 그래프 학습(그래프 정규화 방법)\n\n\n\n\n\n\n\ngraph\n\n\n\n\n\n\n\n\n\n\n\nApr 6, 2023\n\n\n김보람\n\n\n\n\n\n\n\n\nVAE(-ing)\n\n\n\n\n\n\n\nVAE\n\n\nAuto-Encoding Variational Bayes\n\n\n\n\n\n\n\n\n\n\n\nMar 14, 2023\n\n\n김보람\n\n\n\n\n\n\nNo matching items"
  }
]